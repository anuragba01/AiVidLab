{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.912Z",
     "iopub.execute_input": "2025-10-23T12:33:33.293887Z",
     "iopub.status.busy": "2025-10-23T12:33:33.293644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Master Setup & Imports (Phase 0) - System Dependencies First\n",
    "# --------------------------------------------------------------------\n",
    "# REVISED: To remove Stable Diffusion XL dependencies and ensure Gemini ones.\n",
    "\n",
    "# --- STEP 0: ENSURE CRITICAL SYSTEM DEPENDENCIES ARE INSTALLED FIRST ---\\n\n",
    "# This is crucial so that shutil.which() can find them for Python-level configuration.\n",
    "print(\"--- Phase 0.0: Installing critical system dependencies (ImageMagick, FFmpeg, eSpeak-NG) ---\")\n",
    "# Run apt-get update once\n",
    "!apt-get update -qq\n",
    "# Install all system dependencies in one go\n",
    "!apt-get install -y imagemagick ghostscript ffmpeg espeak-ng -qq\n",
    "print(\"--- System dependency installation attempt complete. Verifying installations... ---\")\n",
    "\n",
    "# Optional: Verify installations immediately after apt-get\n",
    "import subprocess # subprocess needed for these checks\n",
    "\n",
    "try:\n",
    "    subprocess.run(['convert', '-version'], check=True, capture_output=True, text=True, encoding='utf-8')\n",
    "    print(\"  ImageMagick 'convert' verified via command line.\")\n",
    "except Exception as e:\n",
    "    print(f\"  WARNING: ImageMagick 'convert' verification failed after apt-get: {e}\")\n",
    "\n",
    "try:\n",
    "    subprocess.run(['ffmpeg', '-version'], check=True, capture_output=True, text=True, encoding='utf-8')\n",
    "    print(\"  FFmpeg verified via command line.\")\n",
    "except Exception as e:\n",
    "    print(f\"  WARNING: FFmpeg verification failed after apt-get: {e}\")\n",
    "\n",
    "try:\n",
    "    subprocess.run(['espeak-ng', '--version'], check=True, capture_output=True, text=True, encoding='utf-8')\n",
    "    print(\"  eSpeak-NG verified via command line.\")\n",
    "except Exception as e:\n",
    "    print(f\"  WARNING: eSpeak-NG verification failed after apt-get: {e}\")\n",
    "print(\"--- End of system dependency verification ---\\\\n\")\n",
    "\n",
    "\n",
    "# --- STEP 1: PYTHON IMPORTS & CONFIGURE MOVIEPY BINARY PATHS ---\\n\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import gc\n",
    "# subprocess was imported above for verification\n",
    "\n",
    "# This import must happen AFTER system dependencies are installed and paths are potentially available\n",
    "import moviepy.config as mpy_config\n",
    "\n",
    "print(\"--- Phase 0.A: Configuring Paths for FFmpeg, FFprobe, and ImageMagick for MoviePy ---\")\n",
    "\n",
    "# A. Set Environment Variables (Often the most robust way)\n",
    "system_ffmpeg_path = shutil.which('ffmpeg')\n",
    "if system_ffmpeg_path:\n",
    "    os.environ[\"FFMPEG_BINARY\"] = system_ffmpeg_path\n",
    "    print(f\"  ENVIRONMENT VARIABLE: FFMPEG_BINARY set to: {system_ffmpeg_path}\")\n",
    "else:\n",
    "    print(\"  CRITICAL ERROR: System FFmpeg NOT FOUND by shutil.which() even after apt-get. MoviePy will likely fail.\")\n",
    "\n",
    "system_ffprobe_path = shutil.which('ffprobe')\n",
    "if system_ffprobe_path:\n",
    "    os.environ[\"FFPROBE_BINARY\"] = system_ffprobe_path\n",
    "    print(f\"  ENVIRONMENT VARIABLE: FFPROBE_BINARY set to: {system_ffprobe_path}\")\n",
    "else:\n",
    "    print(\"  WARNING: System FFprobe NOT FOUND by shutil.which(). Audio metadata tasks might be affected if MoviePy can't infer it.\")\n",
    "\n",
    "system_convert_path = shutil.which('convert')\n",
    "if system_convert_path:\n",
    "    os.environ[\"IMAGEMAGICK_BINARY\"] = system_convert_path\n",
    "    print(f\"  ENVIRONMENT VARIABLE: IMAGEMAGICK_BINARY set to: {system_convert_path}\")\n",
    "else:\n",
    "    print(\"  CRITICAL ERROR: System ImageMagick 'convert' NOT FOUND by shutil.which() even after apt-get. TextClip will fail.\")\n",
    "\n",
    "# B. Verify/Explicitly Set MoviePy Config Settings\n",
    "if system_ffmpeg_path:\n",
    "    try:\n",
    "        current_ffmpeg_setting = mpy_config.get_setting('FFMPEG_BINARY')\n",
    "        if current_ffmpeg_setting == system_ffmpeg_path:\n",
    "            print(f\"  VERIFIED: MoviePy FFMPEG_BINARY correctly using (likely from env var): {current_ffmpeg_setting}\")\n",
    "        else:\n",
    "            print(f\"  INFO: MoviePy FFMPEG_BINARY is '{current_ffmpeg_setting}'. Attempting override with change_settings.\")\n",
    "            mpy_config.change_settings({\"FFMPEG_BINARY\": system_ffmpeg_path})\n",
    "            if mpy_config.get_setting('FFMPEG_BINARY') == system_ffmpeg_path:\n",
    "                print(f\"    SUCCESS: MoviePy FFMPEG_BINARY now set by change_settings to: {system_ffmpeg_path}\")\n",
    "            else:\n",
    "                print(f\"    FAILURE: MoviePy FFMPEG_BINARY still not '{system_ffmpeg_path}' after change_settings.\")\n",
    "    except Exception as e_ffmpeg_cfg:\n",
    "        print(f\"  ERROR during FFMPEG_BINARY configuration/verification with MoviePy: {e_ffmpeg_cfg}\")\n",
    "\n",
    "if system_convert_path:\n",
    "    try:\n",
    "        print(f\"  Attempting to set MoviePy IMAGEMAGICK_BINARY via change_settings to: {system_convert_path}\")\n",
    "        mpy_config.change_settings({\"IMAGEMAGICK_BINARY\": system_convert_path})\n",
    "        new_im_path = mpy_config.get_setting('IMAGEMAGICK_BINARY')\n",
    "        if new_im_path == system_convert_path:\n",
    "            print(f\"    SUCCESS: MoviePy IMAGEMAGICK_BINARY explicitly set to: {new_im_path}\")\n",
    "        else:\n",
    "            print(f\"    CRITICAL WARNING: MoviePy IMAGEMAGICK_BINARY after change_settings is '{new_im_path}', expected '{system_convert_path}'. TextClip may still fail.\")\n",
    "    except Exception as e_set_im: # Broader exception catch\n",
    "        print(f\"  ERROR trying to set MoviePy IMAGEMAGICK_BINARY via config: {e_set_im}\")\n",
    "\n",
    "print(\"--- Finished configuring MoviePy Binary Paths ---\\\\n\")\n",
    "\n",
    "\n",
    "# --- STEP 2: Install Core Python AI/ML Libraries ---\\n\n",
    "print(\"Phase 0.B: Installing Core AI/ML Python Libraries (Revised for Gemini focus)...\")\n",
    "original_tokenizer_parallelism = os.environ.get(\"TOKENIZERS_PARALLELISM\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --upgrade --quiet\n",
    "import torch\n",
    "print(f\"  Successfully imported PyTorch. Version: {torch.__version__}\")\n",
    "if hasattr(torch, 'version') and hasattr(torch.version, 'cuda') and torch.version.cuda:\n",
    "    print(f\"  PyTorch CUDA Toolkit Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"  PyTorch CUDA Toolkit Version not found or PyTorch not compiled with CUDA.\")\n",
    "import torchaudio\n",
    "print(f\"  Successfully imported Torchaudio. Version: {torchaudio.__version__}\")\n",
    "\n",
    "# REVISED: Removing accelerate, bitsandbytes, diffusers, transformers (if only for SDXL)\n",
    "# sentencepiece is often a dependency for tokenizer models, keep for now.\n",
    "# google-generativeai is installed in STEP 3.\n",
    "# !pip install -q --upgrade accelerate bitsandbytes diffusers sentencepiece --quiet # OLD\n",
    "# !pip install transformers==4.51.3 --quiet # OLD, assuming not needed by Whisper or other components\n",
    "# !pip install -q diffusers>=0.24.0 --quiet # OLD\n",
    "\n",
    "!pip install -q --upgrade sentencepiece --quiet # Keep sentencepiece\n",
    "print(\"Core AI/ML libraries (PyTorch, SentencePiece) installation attempt complete.\")\n",
    "\n",
    "# --- STEP 3: Install Other Python Libraries (Audio/Video, Whisper, Gemini) ---\\n\n",
    "print(\"\\\\nPhase 0.C: Installing Other Python Libraries...\")\n",
    "# REVISED: Removed TTS (Coqui) from this line. moviepy is re-installed here.\n",
    "!pip install -q pydub librosa moviepy --quiet\n",
    "print(\"Pydub, Librosa, MoviePy installation attempt complete.\")\n",
    "\n",
    "!pip install -q git+https://github.com/linto-ai/whisper-timestamped.git --quiet\n",
    "print(\"Whisper (timestamped fork) installation attempt complete.\")\n",
    "\n",
    "# REVISED: Ensure google-generativeai is installed here.\n",
    "# Removed accelerate, bitsandbytes, diffusers. Kept sentencepiece (also in STEP 2, pip handles duplicates).\n",
    "!pip install -q --upgrade google-generativeai sentencepiece --quiet\n",
    "print(\"Google Generative AI and supporting libraries installation attempt complete.\")\n",
    "\n",
    "\n",
    "if original_tokenizer_parallelism is not None:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = original_tokenizer_parallelism\n",
    "else:\n",
    "    os.environ.pop(\"TOKENIZERS_PARALLELISM\", None)\n",
    "\n",
    "\n",
    "# --- STEP 4: Import All Other Python Libraries ---\\n\n",
    "print(\"\\\\nPhase 0.D: Importing Main Python Libraries...\")\n",
    "try:\n",
    "    import time\n",
    "    import math\n",
    "    import datetime\n",
    "    import json\n",
    "    import glob\n",
    "    import re\n",
    "    import builtins\n",
    "    from io import BytesIO # Added for Gemini Image Gen\n",
    "\n",
    "    # Removed: from TTS.api import TTS\n",
    "\n",
    "    from pydub import AudioSegment\n",
    "    from pydub.silence import split_on_silence\n",
    "    import numpy as np\n",
    "\n",
    "    # Removed: Transformers (hf_pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig)\n",
    "    # Assuming these were primarily for a Hugging Face based SDXL or local LLM not being used.\n",
    "    # If any of these are needed for other non-SDXL, non-Gemini-LLM tasks, they should be kept.\n",
    "\n",
    "    import whisper_timestamped as whisper\n",
    "\n",
    "    # Removed: from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
    "    from PIL import Image as PILImageMod # Keep PIL.Image, aliased as PILImageMod\n",
    "\n",
    "    from moviepy.editor import (\n",
    "        ImageClip, AudioFileClip, VideoFileClip, TextClip, CompositeVideoClip, ColorClip,\n",
    "        concatenate_videoclips, CompositeAudioClip,\n",
    "        vfx, concatenate_audioclips\n",
    "    )\n",
    "    import moviepy.video.fx.all as vfx_all\n",
    "    # 'moviepy.config as mpy_config' was already imported\n",
    "\n",
    "    # Gemini Imports (essential for TTS and Image Gen)\n",
    "    import google.generativeai as genai\n",
    "    from google.generativeai import types as genai_types # Using alias for clarity\n",
    "\n",
    "    print(\"All required Python libraries imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Failed to import a required library: {e}\")\n",
    "    if \"Stopping notebook\" not in str(e):\n",
    "        sys.exit(f\"Stopping notebook: Essential library import failed: {e}\")\n",
    "\n",
    "\n",
    "# --- STEP 5: Helper Function for GPU Memory Management & Device Config ---\\n\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU memory cleared (if CUDA available) and Python garbage collected.\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\\\nUsing device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"  CUDA (GPU) not available. Operations will run on CPU.\")\n",
    "\n",
    "# --- STEP 6. PyTorch Safe Loading Fix (for Coqui TTS) ---\n",
    "# THIS ENTIRE SECTION IS REMOVED as Coqui TTS is removed.\n",
    "print(\"INFO: Coqui TTS specific PyTorch safe loading fix section removed as Coqui TTS is no longer used.\")\n",
    "\n",
    "# Ensure genai was imported (it is in STEP 4 now)\n",
    "# import google.generativeai as genai # Redundant if already imported correctly above\n",
    "\n",
    "print(\"\\\\n-------------------------------------------\\n\")\n",
    "print(\"Phase 0: All Setup and Imports Complete.\")\n",
    "print(\"-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: User Configuration & Global Parameters (REVISED FOR FULL GEMINI INTEGRATION)\n",
    "# ------------------------------------------------------------------------------------\n",
    "import os # For os.makedirs and os.path.join\n",
    "import time # For unique filenames\n",
    "\n",
    "print(\"Phase 1 (Config): Defining User Configurations and Global Parameters (Full Gemini Suite)...\")\n",
    "\n",
    "# --- General Project Paths ---\\n\n",
    "KAGGLE_WORKING_DIR = \"/kaggle/working/\"\n",
    "KAGGLE_INPUT_DIR = \"/kaggle/input/\"\n",
    "os.makedirs(KAGGLE_WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# --- Universal Timestamp for this Run ---\\n\n",
    "timestamp_final = str(int(time.time()))\n",
    "print(f\"Using timestamp_final for this run: {timestamp_final}\")\n",
    "\n",
    "# --- API Key for ALL Gemini Services (TTS, LLM, Image Generation) ---\\n\n",
    "# IMPORTANT: Replace with your actual key and manage securely (e.g., Kaggle Secrets).\n",
    "# This single key will be used by all Gemini API calls in subsequent cells.\n",
    "GEMINI_API_KEY = \" \" # User provided\n",
    "\n",
    "\n",
    "# --- I. Input Text ---\\n\n",
    "TEXT_TO_SPEAK = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" # Truncated for brevity\n",
    "\n",
    "# --- II. Text-to-Speech (TTS) Configuration (USING GEMINI TTS) ---\\n\n",
    "USE_GEMINI_FOR_TTS = True # Master switch\n",
    "\n",
    "if USE_GEMINI_FOR_TTS:\n",
    "    print(\"INFO: Configuring for Gemini TTS.\")\n",
    "    GEMINI_TTS_MODEL_NAME = \"gemini-2.5-flash-preview-tts\"\n",
    "    GEMINI_TTS_VOICE_NAME = \"Charon\"\n",
    "    GEMINI_TTS_VOICE_VARIANT = \"default\"\n",
    "    GEMINI_TTS_INSTRUCTION_PREFIX = \" \"\n",
    "    GEMINI_TTS_OUTPUT_AUDIO_RATE = 24000 # Hz\n",
    "    GEMINI_TTS_OUTPUT_CHANNELS = 1       # Mono\n",
    "    GEMINI_TTS_OUTPUT_SAMPLE_WIDTH = 2   # 2 bytes = 16-bit PCM\n",
    "    GEMINI_TTS_API_TIMEOUT_S = 300       # Timeout for API call\n",
    "else:\n",
    "    print(\"INFO: Gemini TTS is disabled. No fallback TTS configured in this version.\")\n",
    "\n",
    "GENERATED_TTS_AUDIO_PATH = os.path.join(KAGGLE_WORKING_DIR, f\"1_master_tts_audio_{timestamp_final}.wav\")\n",
    "\n",
    "# --- III. Audio Chunking Configuration (for Image Pacing) ---\\n\n",
    "MIN_CHUNK_DURATION_S_PACING = 15\n",
    "MAX_CHUNK_DURATION_S_PACING = 30\n",
    "FLEXIBILITY_S_PACING = 3\n",
    "SILENCE_THRESH_DB_OFFSET_PACING = -26\n",
    "MIN_SILENCE_LEN_MS_PACING = 300\n",
    "KEEP_SILENCE_MS_PACING = True\n",
    "MIN_ACCEPTABLE_CHUNK_MS_PACING = 500\n",
    "\n",
    "# --- IV. Speech-to-Text (STT) Configuration ---\\n\n",
    "STT_MODEL_ID_WHISPER = \"small\"\n",
    "\n",
    "# --- V. Language Model (LM) API for Prompt Generation Configuration (USING GEMINI LLM) ---\\n\n",
    "USE_GEMINI_API_FOR_LM = True # Uses GEMINI_API_KEY defined at the top\n",
    "\n",
    "if USE_GEMINI_API_FOR_LM:\n",
    "    print(\"INFO: Configuring for Gemini LLM for image prompt generation.\")\n",
    "    GEMINI_LLM_MODEL_NAME = \"models/gemini-2.0-flash-lite\" # Or your chosen LLM model\n",
    "    LLM_API_RPM_LIMIT = 30 # RPM for the LLM (distinct from image gen RPM)\n",
    "    LLM_API_REQUEST_DELAY_S = 60.0 / LLM_API_RPM_LIMIT if LLM_API_RPM_LIMIT > 0 else 2.0\n",
    "else:\n",
    "    print(\"INFO: Gemini LLM for image prompts is disabled.\")\n",
    "\n",
    "CREATIVE_BRIEF_FOR_LM = \"\"\"\n",
    "Create visuals that maintain audience attention and prevent feelings of isolation or boredom. \n",
    "Incorporate multiple people in the scene to foster a sense and engagement.\n",
    "\n",
    "Style:\n",
    "- Inspired by ancient philosophy â€” visually deep and reflective.\n",
    "- Should resonate with the present-day context while preserving a timeless, philosophical tone.\n",
    "\n",
    "Avoid:\n",
    "- Explicit religious symbols\n",
    "- Sexualized depictions of women\n",
    "- Violence or nudity\n",
    "- Real identifiable persons or deities\n",
    "- Neon colors\n",
    "\"\"\"\n",
    "\n",
    "LM_MAX_INPUT_TOKENS = 4096\n",
    "\n",
    "# --- VI. Image Generation Configuration (USING GEMINI IMAGE GENERATION) ---\\n\n",
    "USE_GEMINI_FOR_IMAGE_GENERATION = True # Master switch\n",
    "\n",
    "if USE_GEMINI_FOR_IMAGE_GENERATION:\n",
    "    print(\"INFO: Configuring for Gemini Image Generation.\")\n",
    "    GEMINI_IMAGE_GEN_MODEL_NAME = \"gemini-2.0-flash-preview-image-generation\"\n",
    "    GEMINI_IMAGE_GEN_RPM_LIMIT = 10  # Requests Per Minute for image generation\n",
    "    GEMINI_IMAGE_GEN_REQUEST_DELAY_S = 60.0 / GEMINI_IMAGE_GEN_RPM_LIMIT if GEMINI_IMAGE_GEN_RPM_LIMIT > 0 else 6.0\n",
    "    GEMINI_IMAGE_GEN_API_TIMEOUT_S = 180 # Timeout for image gen API call (e.g., 3 minutes)\n",
    "\n",
    "    # Target dimensions for images *after* generation and optional resizing by us.\n",
    "    # Gemini might have its own output sizes; these are for our pipeline's consistency.\n",
    "    TARGET_IMAGE_WIDTH = 1024\n",
    "    TARGET_IMAGE_HEIGHT = 576\n",
    "\n",
    "    # Suffix to append to prompts sent to Gemini Image Gen. Adjust as needed.\n",
    "    GEMINI_PROMPT_UNIVERSAL_SUFFIX = \", soft lighting,cinematic style, high detail, emotionally immersive\"\n",
    "\n",
    "    # Negative prompts might be handled by adding \"AVOID: ...\" to the main prompt for Gemini.\n",
    "    # This variable is kept for reference or if a different strategy is used.\n",
    "    NEGATIVE_PROMPT_TERMS = \"low quality, poorly drawn face, poorly drawn hands, extra limbs, out of frame, distorted body, neon colors, distracting backgrounds, unrealistic lighting, game art, watermark, signature, noisy, jpeg artifacts, lowres, childish, blur face, women, nudity, naked, distorted body part\"\n",
    "else:\n",
    "    print(\"INFO: Gemini Image Generation is disabled. No fallback image generator configured.\")\n",
    "    TARGET_IMAGE_WIDTH = 1920 # Default if Gemini not used, for downstream consistency\n",
    "    TARGET_IMAGE_HEIGHT = 1080\n",
    "    GEMINI_PROMPT_UNIVERSAL_SUFFIX = \"\"\n",
    "    NEGATIVE_PROMPT_TERMS = \"\"\n",
    "\n",
    "\n",
    "IMAGE_OUTPUT_DIR = os.path.join(KAGGLE_WORKING_DIR, \"2_generated_images/\")\n",
    "os.makedirs(IMAGE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- VII. Video Assembly Configuration ---\\n\n",
    "VIDEO_TARGET_WIDTH = 1920 # Final video output dimensions\n",
    "VIDEO_TARGET_HEIGHT = 1080\n",
    "VIDEO_FPS = 18\n",
    "VISUAL_TRANSITION_DURATION_S = 1.5\n",
    "\n",
    "# Conceptual paths, not necessarily written if pipeline is fully in-memory for these stages\n",
    "SILENT_VIDEO_PATH = os.path.join(KAGGLE_WORKING_DIR, f\"INTERMEDIATE_silent_video_{timestamp_final}.mp4\")\n",
    "VIDEO_WITH_TTS_AUDIO_PATH = os.path.join(KAGGLE_WORKING_DIR, f\"INTERMEDIATE_video_with_audio_{timestamp_final}.mp4\")\n",
    "\n",
    "# --- VIII. Subtitle (ASS) Configuration ---\\n\n",
    "ASS_SUBTITLE_PATH = os.path.join(KAGGLE_WORKING_DIR, f\"5_subtitles_{timestamp_final}.ass\")\n",
    "MAX_WORDS_PER_LINE_ASS = 7\n",
    "MAX_LINE_DURATION_SEC_ASS = 12.0\n",
    "GAP_THRESHOLD_SEC_ASS = 0.4\n",
    "ASS_FONT_NAME = \"Times New Roman\"\n",
    "ASS_FONT_SIZE = max(72, int(VIDEO_TARGET_HEIGHT / 22))\n",
    "ASS_PRIMARY_COLOUR = \"&H00FFFFFF\"\n",
    "ASS_SECONDARY_COLOUR_ASS = \"&H000000FF\"\n",
    "ASS_OUTLINE_COLOUR = \"&H00000000\"\n",
    "ASS_BACK_COLOUR = \"&H99000000\"\n",
    "ASS_BOLD = -1\n",
    "ASS_ALIGNMENT = 5\n",
    "ASS_MARGIN_L = 20\n",
    "ASS_MARGIN_R = 20\n",
    "ASS_MARGIN_V = max(15, int(VIDEO_TARGET_HEIGHT * 0.10))\n",
    "ASS_FADE_EFFECT_MS = 0\n",
    "\n",
    "# --- XIV. ASS Heading Style Configuration ---\\n\n",
    "ENABLE_HEADING_SUBTITLE_STYLE = False\n",
    "ASS_HEADING_FONT_NAME = \"Arial\"\n",
    "ASS_HEADING_FONT_SIZE = int(ASS_FONT_SIZE * 1.2) # Relies on ASS_FONT_SIZE being defined\n",
    "ASS_HEADING_PRIMARY_COLOUR = \"&H00FFFF00\"\n",
    "ASS_HEADING_OUTLINE_COLOUR = \"&H00000000\"\n",
    "ASS_HEADING_BACK_COLOUR = \"&H60000000\"\n",
    "ASS_HEADING_BOLD = -1\n",
    "ASS_HEADING_ITALIC = 0\n",
    "ASS_HEADING_ALIGNMENT = 5\n",
    "ASS_HEADING_MARGIN_L = 30\n",
    "ASS_HEADING_MARGIN_R = 30\n",
    "ASS_HEADING_MARGIN_V = 50\n",
    "RAW_IDENTIFIED_HEADINGS_LIST_PATH = os.path.join(KAGGLE_WORKING_DIR, f\"temp_raw_headings_from_lm_{timestamp_final}.json\")\n",
    "\n",
    "# --- X. Post-Processing Configuration (Intro/Outro, Fades, Zoom) ---\\n\n",
    "ADD_INTRO_OUTRO_SCREENS = True\n",
    "INTRO_SCREEN_DURATION_S = 3.0\n",
    "OUTRO_SCREEN_DURATION_S = 2.0\n",
    "ENABLE_INTRO_TEXT = True\n",
    "INTRO_TEXT_CONTENT = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "INTRO_TEXT_FONT = \"Arial-Bold\"\n",
    "INTRO_TEXT_FONTSIZE = 35\n",
    "INTRO_TEXT_COLOR = \"white\"\n",
    "INTRO_TEXT_ALIGNMENT = \"center\"\n",
    "INTRO_TEXT_POSITION = ('center', 'center')\n",
    "\n",
    "\n",
    "ENABLE_OUTRO_TEXT = False\n",
    "OUTRO_TEXT_CONTENT = \" \"\n",
    "OUTRO_TEXT_FONT = \"Arial-Bold\"\n",
    "OUTRO_TEXT_FONTSIZE = 30\n",
    "OUTRO_TEXT_COLOR = \"yellow\"\n",
    "OUTRO_TEXT_ALIGNMENT = \"center\"\n",
    "OUTRO_TEXT_POSITION = ('center', 'center')\n",
    "MAIN_CONTENT_FADE_IN_DURATION_S = 1.0\n",
    "MAIN_CONTENT_FADE_OUT_DURATION_S = 1.0\n",
    "\n",
    "# --- XI. Optional Background Music Configuration ---\\n\n",
    "ADD_BACKGROUND_MUSIC = True\n",
    "BACKGROUND_MUSIC_VOLUME = 0.04\n",
    "dataset_music_foldername = \"musica1\"\n",
    "filename_background_music = \"freepik-final-lap.wav\"\n",
    "BACKGROUND_MUSIC_PATH = os.path.join(KAGGLE_INPUT_DIR, dataset_music_foldername, filename_background_music)\n",
    "\n",
    "# --- NEW SECTION: FFmpeg Filter Parameters for Advanced Visual Effects ---\n",
    "print(\"INFO: Configuring FFmpeg filter parameters for advanced visual effects.\")\n",
    "\n",
    "# For Transitions (xfade filter)\n",
    "FFMPEG_XFADE_TRANSITION_TYPE = \"fade\"  # Examples: \"fade\", \"wipeleft\", \"slideright\", \"circleopen\", \"rectcrop\", etc.\n",
    "                                    # Refer to FFmpeg xfade filter documentation for all options.\n",
    "FFMPEG_XFADE_DURATION_S = globals().get('VISUAL_TRANSITION_DURATION_S', 1.5) # Reuse existing transition duration\n",
    "\n",
    "# For Calm Zoom (zoompan filter - this will be complex to implement dynamically)\n",
    "# These are templates or base values; the actual expressions will be generated per clip.\n",
    "FFMPEG_ZOOMPAN_MAX_SCALE = globals().get('CALM_ZOOM_MAX_SCALE', 1.10)\n",
    "FFMPEG_ZOOMPAN_CYCLES_PER_CLIP = globals().get('CALM_ZOOM_CYCLES_PER_CLIP', 1) # How many in-out cycles for the 'multi_cycle_in_out' logic\n",
    "FFMPEG_ZOOMPAN_TARGET_FPS = globals().get('VIDEO_FPS', 24) # FPS for zoompan calculations\n",
    "\n",
    "# For Intro/Outro Text Rendering with FFmpeg's drawtext filter\n",
    "# You MUST ensure this font file is available in your Kaggle environment.\n",
    "# Common paths: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\n",
    "#               /usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\n",
    "# If it's not found, drawtext will fail.\n",
    "FFMPEG_DRAWTEXT_FONTFILE = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
    "FFMPEG_DRAWTEXT_INTRO_COLOR = \"white\"\n",
    "FFMPEG_DRAWTEXT_OUTRO_COLOR = \"yellow\"\n",
    "# INTRO_TEXT_CONTENT, OUTRO_TEXT_CONTENT, INTRO_TEXT_FONTSIZE, OUTRO_TEXT_FONTSIZE\n",
    "# INTRO_TEXT_ALIGNMENT, OUTRO_TEXT_ALIGNMENT are already defined in Section X and will be reused.\n",
    "# Note: FFmpeg drawtext alignment is different from MoviePy. 'x' and 'y' expressions will be needed.\n",
    "#       e.g., x=(w-text_w)/2, y=(h-text_h)/2 for center.\n",
    "\n",
    "# --- XII. Post-Processing Configuration: Calm Zoom Effect ---\\n\n",
    "CALM_ZOOM_EFFECT_ENABLED = True\n",
    "CALM_ZOOM_MAX_SCALE = 1.05\n",
    "CALM_ZOOM_CYCLES_PER_CLIP = 0.5\n",
    "CALM_ZOOM_TYPE = \"multi_cycle_in_out\"\n",
    "\n",
    "# --- XIII. FINAL OUTPUT VIDEO PATH ---\\n\n",
    "ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD = os.path.join(KAGGLE_WORKING_DIR, f\"ULTIMATE_FINAL_VIDEO_OUTPUT_{timestamp_final}.mp4\")\n",
    "\n",
    "# --- FFmpeg parameters for Phase D ---\\n\n",
    "FFMPEG_VCODEC = 'h264_nvenc'\n",
    "FFMPEG_NVENC_PRESET = 'p6'\n",
    "FFMPEG_NVENC_TUNE = 'hq'\n",
    "FFMPEG_NVENC_RC = 'vbr'\n",
    "FFMPEG_NVENC_CQ = '23'\n",
    "FFMPEG_NVENC_BITRATE = '0'\n",
    "FFMPEG_PIX_FMT = 'yuv420p'\n",
    "FFMPEG_ACODEC_FINAL = 'aac' # Changed from 'copy' as Gemini TTS outputs PCM WAV. AAC is a good target.\n",
    "FFMPEG_AUDIO_CHANNELS_FINAL = 2\n",
    "\n",
    "\n",
    "print(\"\\\\nAll configurations set.\")\n",
    "\n",
    "# --- Sanity Checks (Revised for Full Gemini Suite) ---\\n\n",
    "# API Key Check (applies to all Gemini services)\n",
    "if not GEMINI_API_KEY or \"YOUR_GOOGLE_AI_STUDIO_API_KEY\" in GEMINI_API_KEY or \"AIzaSyB7SlRM-GpPeproTvWq-uSP8Xpgm2sdYEY\" in GEMINI_API_KEY: # Example key check\n",
    "    print(f\"WARNING (Sanity Check): GEMINI_API_KEY ('{GEMINI_API_KEY}') looks like a placeholder or the public example. Ensure it's your private, valid API key for all Gemini services.\")\n",
    "else:\n",
    "    print(f\"INFO (Sanity Check): GEMINI_API_KEY is set.\")\n",
    "\n",
    "# Gemini TTS Sanity Check\n",
    "if USE_GEMINI_FOR_TTS:\n",
    "    print(f\"INFO (Sanity Check): Gemini TTS is ENABLED. Model: '{GEMINI_TTS_MODEL_NAME}', Voice: '{GEMINI_TTS_VOICE_NAME}'.\")\n",
    "    if not GENERATED_TTS_AUDIO_PATH:\n",
    "        print(\"CRITICAL WARNING (Sanity Check): GENERATED_TTS_AUDIO_PATH for Gemini TTS output is not set.\")\n",
    "else:\n",
    "    print(\"INFO (Sanity Check): Gemini TTS is DISABLED.\")\n",
    "\n",
    "# Gemini LLM (for prompts) Sanity Check\n",
    "if USE_GEMINI_API_FOR_LM:\n",
    "    print(f\"INFO (Sanity Check): Gemini LLM for image prompts is ENABLED. Model: '{GEMINI_LLM_MODEL_NAME}'.\")\n",
    "else:\n",
    "    print(\"INFO (Sanity Check): Gemini LLM for image prompts is DISABLED.\")\n",
    "\n",
    "# Gemini Image Generation Sanity Check\n",
    "if USE_GEMINI_FOR_IMAGE_GENERATION:\n",
    "    print(f\"INFO (Sanity Check): Gemini Image Generation is ENABLED. Model: '{GEMINI_IMAGE_GEN_MODEL_NAME}'. RPM: {GEMINI_IMAGE_GEN_RPM_LIMIT}.\")\n",
    "    if not all([TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT]):\n",
    "        print(\"WARNING (Sanity Check): TARGET_IMAGE_WIDTH or TARGET_IMAGE_HEIGHT for Gemini Image Gen not set.\")\n",
    "else:\n",
    "    print(\"INFO (Sanity Check): Gemini Image Generation is DISABLED.\")\n",
    "\n",
    "# BGM path\n",
    "if ADD_BACKGROUND_MUSIC:\n",
    "    if not (BACKGROUND_MUSIC_PATH and os.path.exists(BACKGROUND_MUSIC_PATH)):\n",
    "        print(f\"WARNING (Sanity Check): ADD_BACKGROUND_MUSIC is True, but BGM file not found: '{BACKGROUND_MUSIC_PATH}'.\")\n",
    "    else:\n",
    "        print(f\"INFO (Sanity Check): Background music track found: '{BACKGROUND_MUSIC_PATH}'\")\n",
    "\n",
    "print(\"\\\\n--------------------------------------------------------------------------\")\n",
    "print(\"Phase 1 (Config): Configuration Setup Complete.\")\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Phase 2 - Text-to-Speech (TTS) with Gemini API (Chunking Implemented)\n",
    "# ------------------------------------------------------------------------------------\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import wave\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.exceptions import CouldntDecodeError\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import types as genai_types\n",
    "\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Phase 2 (TTS with Gemini API - Chunking): Starting Text-to-Speech Synthesis...\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "# --- 0. Retrieve Configurations from Cell 2 ---\n",
    "USE_GEMINI_FOR_TTS = globals().get('USE_GEMINI_FOR_TTS', False)\n",
    "GEMINI_API_KEY = globals().get('GEMINI_API_KEY')\n",
    "TEXT_TO_SPEAK_ORIGINAL = globals().get('TEXT_TO_SPEAK', \"\") # From Cell 2\n",
    "GENERATED_TTS_AUDIO_PATH = globals().get('GENERATED_TTS_AUDIO_PATH') # From Cell 2\n",
    "KAGGLE_WORKING_DIR = globals().get(\"KAGGLE_WORKING_DIR\", \"/kaggle/working/\")\n",
    "\n",
    "GEMINI_TTS_MODEL_NAME = globals().get('GEMINI_TTS_MODEL_NAME', \"gemini-2.5-flash-preview-tts\")\n",
    "GEMINI_TTS_VOICE_NAME = globals().get('GEMINI_TTS_VOICE_NAME', \"Charon\") \n",
    "GEMINI_TTS_INSTRUCTION_PREFIX = globals().get('GEMINI_TTS_INSTRUCTION_PREFIX', \"\")\n",
    "\n",
    "GEMINI_TTS_OUTPUT_AUDIO_RATE = globals().get('GEMINI_TTS_OUTPUT_AUDIO_RATE', 24000)\n",
    "GEMINI_TTS_OUTPUT_CHANNELS = globals().get('GEMINI_TTS_OUTPUT_CHANNELS', 1)\n",
    "GEMINI_TTS_OUTPUT_SAMPLE_WIDTH = globals().get('GEMINI_TTS_OUTPUT_SAMPLE_WIDTH', 2)\n",
    "GEMINI_TTS_API_TIMEOUT_S = globals().get('GEMINI_TTS_API_TIMEOUT_S', 300)\n",
    "\n",
    "MAX_CHARS_PER_TTS_CHUNK = 3800 # Based on your findings\n",
    "# --- END Configuration ---\n",
    "\n",
    "synthesis_successful_flag = False\n",
    "temp_audio_files_list = [] # Store paths of temporary chunk audio files\n",
    "\n",
    "def save_wave_file_gemini(filename, pcm_data, channels, rate, sample_width):\n",
    "   # ... (same save_wave_file_gemini function as in the isolated test)\n",
    "   if not filename: print(\"ERROR (save_wave_file_gemini): Filename missing.\"); return False\n",
    "   try:\n",
    "       with wave.open(filename, \"wb\") as wf:\n",
    "          wf.setnchannels(channels); wf.setsampwidth(sample_width); wf.setframerate(rate); wf.writeframes(pcm_data)\n",
    "       print(f\"  Successfully saved audio chunk to: {os.path.basename(filename)}\")\n",
    "       return True\n",
    "   except Exception as e: print(f\"  ERROR saving wave file {filename}: {e}\"); traceback.print_exc(); return False\n",
    "\n",
    "\n",
    "def split_text_for_tts_api(long_text, max_chunk_length):\n",
    "    # ... (same split_text_for_tts_api function as in the isolated test) ...\n",
    "    chunks = []; current_chunk_start = 0\n",
    "    if not long_text or not long_text.strip(): return chunks\n",
    "    while current_chunk_start < len(long_text):\n",
    "        provisional_end = min(current_chunk_start + max_chunk_length, len(long_text))\n",
    "        actual_end = provisional_end\n",
    "        if provisional_end < len(long_text):\n",
    "            best_break_point = -1\n",
    "            for delimiter_set in [(\"\\n\\n\", 2), (\". \", 1), (\"? \", 1), (\"! \", 1), (\", \", 1), (\" \", 1)]:\n",
    "                delimiter, cut_after_len = delimiter_set\n",
    "                break_candidate = long_text.rfind(delimiter, current_chunk_start, provisional_end)\n",
    "                if break_candidate != -1 and break_candidate > current_chunk_start : best_break_point = break_candidate + cut_after_len; break \n",
    "            if best_break_point != -1: actual_end = best_break_point\n",
    "        chunk_text = long_text[current_chunk_start:actual_end].strip()\n",
    "        if chunk_text: chunks.append(chunk_text)\n",
    "        current_chunk_start = actual_end\n",
    "    final_chunks = []\n",
    "    for chunk in chunks: \n",
    "        if len(chunk) > max_chunk_length:\n",
    "            for i in range(0, len(chunk), max_chunk_length): final_chunks.append(chunk[i:i+max_chunk_length])\n",
    "        elif chunk: final_chunks.append(chunk)\n",
    "    return final_chunks\n",
    "\n",
    "# --- Main TTS Processing Logic ---\n",
    "if not USE_GEMINI_FOR_TTS:\n",
    "    print(\"INFO: USE_GEMINI_FOR_TTS is False in Cell 2. Skipping Gemini TTS synthesis.\")\n",
    "elif not GEMINI_API_KEY or \"YOUR_API_KEY\" in str(GEMINI_API_KEY) or (\"AIzaSy\" not in str(GEMINI_API_KEY) and len(str(GEMINI_API_KEY)) < 30) : # More robust placeholder check\n",
    "    print(\"ERROR: GEMINI_API_KEY is missing, a placeholder, or too short. Please set a valid API key in Cell 2.\")\n",
    "elif not TEXT_TO_SPEAK_ORIGINAL.strip():\n",
    "    print(\"ERROR: TEXT_TO_SPEAK (from Cell 2) is empty. Nothing to synthesize.\")\n",
    "elif not GENERATED_TTS_AUDIO_PATH:\n",
    "    print(\"ERROR: GENERATED_TTS_AUDIO_PATH is not defined in Cell 2.\")\n",
    "else:\n",
    "    print(f\"Using Gemini model: {GEMINI_TTS_MODEL_NAME}, Voice: {GEMINI_TTS_VOICE_NAME}\")\n",
    "    if os.path.exists(GENERATED_TTS_AUDIO_PATH):\n",
    "        try: os.remove(GENERATED_TTS_AUDIO_PATH); print(f\"DEBUG: Deleted existing main TTS output file: {GENERATED_TTS_AUDIO_PATH}\")\n",
    "        except Exception as e_del: print(f\"DEBUG: Could not delete existing TTS output file: {e_del}\")\n",
    "\n",
    "    text_to_process = TEXT_TO_SPEAK_ORIGINAL\n",
    "    if GEMINI_TTS_INSTRUCTION_PREFIX and GEMINI_TTS_INSTRUCTION_PREFIX.strip():\n",
    "        text_to_process = GEMINI_TTS_INSTRUCTION_PREFIX.strip() + \"\\n\" + TEXT_TO_SPEAK_ORIGINAL\n",
    "        print(f\"  Prepended TTS instruction prefix.\")\n",
    "\n",
    "    text_chunks = split_text_for_tts_api(text_to_process, MAX_CHARS_PER_TTS_CHUNK)\n",
    "    print(f\"Text split into {len(text_chunks)} chunks for API calls (max chars per chunk: ~{MAX_CHARS_PER_TTS_CHUNK}).\")\n",
    "\n",
    "    tts_model_gemini = None\n",
    "    all_chunks_synthesized_successfully = True if text_chunks else False # Start true only if there are chunks\n",
    "\n",
    "    if not text_chunks:\n",
    "        print(\"No text chunks to process after splitting.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Ensure API key is configured for the genai library\n",
    "            # This might have been done in Cell 1 already. Calling it again is usually safe.\n",
    "            genai.configure(api_key=GEMINI_API_KEY)\n",
    "            print(\"Gemini API key configured for this session.\")\n",
    "            \n",
    "            safety_settings_tts = [\n",
    "                {\"category\": c, \"threshold\": genai_types.HarmBlockThreshold.BLOCK_NONE}\n",
    "                for c in genai_types.HarmCategory if c != genai_types.HarmCategory.HARM_CATEGORY_UNSPECIFIED\n",
    "            ]\n",
    "            tts_model_gemini = genai.GenerativeModel(GEMINI_TTS_MODEL_NAME, safety_settings=safety_settings_tts)\n",
    "            print(f\"Gemini GenerativeModel for TTS ('{GEMINI_TTS_MODEL_NAME}') initialized.\")\n",
    "\n",
    "            for i, chunk_str in enumerate(text_chunks):\n",
    "                if not chunk_str.strip(): print(f\"  Skipping empty chunk {i+1}.\"); continue\n",
    "                \n",
    "                print(f\"  Synthesizing audio for chunk {i+1}/{len(text_chunks)} (length: {len(chunk_str)} chars)...\")\n",
    "                temp_wav_path_chunk = os.path.join(KAGGLE_WORKING_DIR, f\"temp_pipeline_tts_chunk_{i}.wav\") # Changed temp filename slightly\n",
    "                \n",
    "                try:\n",
    "                    start_time_api_chunk = time.time()\n",
    "                    generation_config_payload = {\n",
    "                        \"response_modalities\": [\"AUDIO\"],\n",
    "                        \"speech_config\": {\n",
    "                            \"voice_config\": {\n",
    "                                \"prebuilt_voice_config\": {\"voice_name\": GEMINI_TTS_VOICE_NAME}\n",
    "                            }\n",
    "                        },\n",
    "                        \"candidate_count\": 1\n",
    "                    }\n",
    "                    # print(f\"    DEBUG: Generation Config Payload: {generation_config_payload}\") # Optional debug\n",
    "\n",
    "                    response = tts_model_gemini.generate_content(\n",
    "                       contents=chunk_str,\n",
    "                       generation_config=generation_config_payload,\n",
    "                       request_options={\"timeout\": GEMINI_TTS_API_TIMEOUT_S}\n",
    "                    )\n",
    "                    api_call_duration_chunk = time.time() - start_time_api_chunk\n",
    "                    print(f\"    Gemini TTS API call for chunk {i+1} completed in {api_call_duration_chunk:.2f} seconds.\")\n",
    "\n",
    "                    if response.candidates and response.candidates[0].content and \\\n",
    "                       response.candidates[0].content.parts and \\\n",
    "                       response.candidates[0].content.parts[0].inline_data and \\\n",
    "                       hasattr(response.candidates[0].content.parts[0].inline_data, 'data') and \\\n",
    "                       response.candidates[0].content.parts[0].inline_data.data:\n",
    "                        \n",
    "                        audio_data_bytes = response.candidates[0].content.parts[0].inline_data.data\n",
    "                        print(f\"    Received audio data for chunk {i+1} (Size: {len(audio_data_bytes)} bytes).\")\n",
    "                        \n",
    "                        if save_wave_file_gemini(\n",
    "                            temp_wav_path_chunk, audio_data_bytes,\n",
    "                            channels=GEMINI_TTS_OUTPUT_CHANNELS,\n",
    "                            rate=GEMINI_TTS_OUTPUT_AUDIO_RATE,\n",
    "                            sample_width=GEMINI_TTS_OUTPUT_SAMPLE_WIDTH\n",
    "                        ):\n",
    "                            if os.path.exists(temp_wav_path_chunk) and os.path.getsize(temp_wav_path_chunk) > 0:\n",
    "                                temp_audio_files_list.append(temp_wav_path_chunk)\n",
    "                            else: print(f\"    ERROR: Saved audio chunk {i+1} is empty/invalid.\"); all_chunks_synthesized_successfully = False; break\n",
    "                        else: print(f\"    ERROR: Failed to save audio for chunk {i+1}.\"); all_chunks_synthesized_successfully = False; break\n",
    "                    else:\n",
    "                        print(f\"    ERROR: Gemini TTS API response for chunk {i+1} did not contain valid audio data.\")\n",
    "                        if hasattr(response, 'prompt_feedback') and response.prompt_feedback and \\\n",
    "                           hasattr(response.prompt_feedback, 'block_reason') and response.prompt_feedback.block_reason:\n",
    "                            print(f\"      API Block Reason: {response.prompt_feedback.block_reason}\")\n",
    "                        all_chunks_synthesized_successfully = False; break\n",
    "                except Exception as e_synth_chunk:\n",
    "                    print(f\"    ERROR during Gemini TTS API call or processing for chunk {i+1}: {e_synth_chunk}\")\n",
    "                    traceback.print_exc(); all_chunks_synthesized_successfully = False; break\n",
    "        except Exception as e_outer_setup:\n",
    "            print(f\"ERROR during Gemini TTS setup (key config or model init): {e_outer_setup}\")\n",
    "            traceback.print_exc(); all_chunks_synthesized_successfully = False\n",
    "\n",
    "    if all_chunks_synthesized_successfully and temp_audio_files_list:\n",
    "        print(f\"\\nCombining {len(temp_audio_files_list)} audio chunks...\")\n",
    "        try:\n",
    "            combined_audio = AudioSegment.empty()\n",
    "            for f_path in temp_audio_files_list:\n",
    "                try: segment = AudioSegment.from_file(f_path, format=\"wav\"); combined_audio += segment\n",
    "                except CouldntDecodeError: print(f\"  ERROR: Could not decode {f_path}. Skipping.\"); all_chunks_synthesized_successfully = False; break\n",
    "                except Exception as e_load_segment: print(f\"  ERROR loading segment {f_path}: {e_load_segment}\"); all_chunks_synthesized_successfully = False; break\n",
    "            \n",
    "            if all_chunks_synthesized_successfully and len(combined_audio) > 0: # Re-check flag\n",
    "                combined_audio.export(GENERATED_TTS_AUDIO_PATH, format=\"wav\")\n",
    "                print(f\"Full TTS audio (from combined chunks) saved to: {GENERATED_TTS_AUDIO_PATH}\"); synthesis_successful_flag = True\n",
    "            elif not all_chunks_synthesized_successfully:\n",
    "                print(\"Concatenation skipped due to errors during chunk processing or loading.\")\n",
    "            else: print(\"  ERROR: Combined audio is empty after attempting concatenation.\")\n",
    "        except Exception as e_combine_audio: print(f\"  Error combining audio chunks: {e_combine_audio}\"); traceback.print_exc()\n",
    "    elif text_chunks: print(\"Synthesis of one or more chunks failed or no chunks were successfully processed to combine.\")\n",
    "\n",
    "# --- Final Cleanup and Verification ---\n",
    "if temp_audio_files_list:\n",
    "    print(\"\\nCleaning up temporary TTS audio chunk files...\")\n",
    "    for f_path in temp_audio_files_list:\n",
    "        if os.path.exists(f_path):\n",
    "            try: os.remove(f_path)\n",
    "            except Exception as e_remove: print(f\"  Could not remove temp file {f_path}: {e_remove}\")\n",
    "\n",
    "if not synthesis_successful_flag and GENERATED_TTS_AUDIO_PATH and os.path.exists(GENERATED_TTS_AUDIO_PATH):\n",
    "    try: os.remove(GENERATED_TTS_AUDIO_PATH); print(f\"Attempted to remove incomplete main TTS output: {GENERATED_TTS_AUDIO_PATH}\")\n",
    "    except: pass\n",
    "\n",
    "final_audio_exists = GENERATED_TTS_AUDIO_PATH and os.path.exists(GENERATED_TTS_AUDIO_PATH) and os.path.getsize(GENERATED_TTS_AUDIO_PATH) > 0\n",
    "if final_audio_exists and synthesis_successful_flag:\n",
    "    print(f\"\\nSUCCESS: Full TTS audio (from Gemini API) is available at: {GENERATED_TTS_AUDIO_PATH}\")\n",
    "    print(f\"  File size: {os.path.getsize(GENERATED_TTS_AUDIO_PATH) / (1024*1024):.2f} MB\")\n",
    "    try: final_segment = AudioSegment.from_wav(GENERATED_TTS_AUDIO_PATH); print(f\"  Audio Duration (from Pydub): {final_segment.duration_seconds:.3f}s\")\n",
    "    except: print(\"  Could not read final audio file with Pydub to get duration.\")\n",
    "    print(f\"  Saved assuming Specs: Rate={GEMINI_TTS_OUTPUT_AUDIO_RATE}Hz, Channels={GEMINI_TTS_OUTPUT_CHANNELS}, SampleWidth={GEMINI_TTS_OUTPUT_SAMPLE_WIDTH*8}-bit PCM\")\n",
    "else:\n",
    "    print(\"\\nERROR: Gemini TTS audio generation FAILED or final output file not found/empty.\")\n",
    "    if GENERATED_TTS_AUDIO_PATH: print(f\"  Expected path: {GENERATED_TTS_AUDIO_PATH}, Exists: {os.path.exists(GENERATED_TTS_AUDIO_PATH)}, Size (if exists): {os.path.getsize(GENERATED_TTS_AUDIO_PATH) if os.path.exists(GENERATED_TTS_AUDIO_PATH) else 'N/A'}\")\n",
    "\n",
    "print(\"\\n--------------------------------------------------------------------------\")\n",
    "if USE_GEMINI_FOR_TTS: print(\"Phase 2 (TTS with Gemini API - Chunking): Synthesis Attempt Complete.\")\n",
    "else: print(\"Phase 2 (TTS with Gemini API - Chunking): Skipped as per configuration.\")\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell PhaseB: Prepare Audio Components (LOAD WITH PYDUB, then MoviePy AudioClip from Array) - Vectorized make_frame\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Phase B: Preparing Audio Components (Load with Pydub, then MoviePy AudioClip from Array)...\")\n",
    "print(\"           (Using vectorized make_frame functions)\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "from moviepy.editor import AudioClip\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# --- Ensure timestamp_final exists (for fallback naming if needed) ---\n",
    "if 'timestamp_final' not in globals():\n",
    "    timestamp_final = str(int(time.time()))\n",
    "    print(f\"WARNING (Phase B): 'timestamp_final' was not globally defined. Using new timestamp: {timestamp_final}\")\n",
    "\n",
    "# --- Output variables for this phase (initialize to None) ---\n",
    "narration_audioclip_from_array = None\n",
    "bg_music_audioclip_from_array = None\n",
    "narration_duration_from_phase_b = 0.0\n",
    "bgm_duration_from_phase_b = 0.0\n",
    "\n",
    "# --- Helper to load audio using Pydub and convert to NumPy array ---\n",
    "# (This function remains the same as your last successful version)\n",
    "def load_audio_with_pydub_to_numpy(filepath, display_name=\"Audio\", target_sr=44100, expected_channels=None):\n",
    "    print(f\"  Attempting to load '{display_name}' from '{os.path.basename(filepath)}' with Pydub...\")\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"    ERROR: File not found: {filepath}\")\n",
    "        return None, 0, 0\n",
    "    try:\n",
    "        segment = AudioSegment.from_file(filepath)\n",
    "        print(f\"    Pydub loaded '{display_name}'. Original SR: {segment.frame_rate}Hz, Channels: {segment.channels}, Duration: {segment.duration_seconds:.2f}s\")\n",
    "        if segment.frame_rate != target_sr:\n",
    "            print(f\"      Resampling '{display_name}' from {segment.frame_rate}Hz to {target_sr}Hz...\")\n",
    "            segment = segment.set_frame_rate(target_sr)\n",
    "        if expected_channels is not None and segment.channels != expected_channels:\n",
    "            print(f\"      Converting '{display_name}' channels from {segment.channels} to {expected_channels}...\")\n",
    "            segment = segment.set_channels(expected_channels)\n",
    "        samples_int = np.array(segment.get_array_of_samples())\n",
    "        if samples_int.dtype == np.int16:\n",
    "            sound_array_float32 = samples_int.astype(np.float32) / (2**15)\n",
    "        elif samples_int.dtype == np.int32:\n",
    "            sound_array_float32 = samples_int.astype(np.float32) / (2**31)\n",
    "        elif samples_int.dtype == np.uint8:\n",
    "             sound_array_float32 = (samples_int.astype(np.float32) - 128) / 128\n",
    "        else:\n",
    "            print(f\"    WARNING: Unexpected sample dtype from Pydub for '{display_name}': {samples_int.dtype}. Attempting normalization.\")\n",
    "            if np.issubdtype(samples_int.dtype, np.floating):\n",
    "                 sound_array_float32 = samples_int.astype(np.float32)\n",
    "            else:\n",
    "                 max_val = np.iinfo(samples_int.dtype).max\n",
    "                 sound_array_float32 = samples_int.astype(np.float32) / max_val if max_val > 0 else samples_int.astype(np.float32)\n",
    "        if segment.channels == 2:\n",
    "            sound_array_reshaped = sound_array_float32.reshape((-1, 2))\n",
    "        elif segment.channels == 1:\n",
    "            sound_array_reshaped = sound_array_float32.reshape((-1, 1))\n",
    "        else:\n",
    "            print(f\"    ERROR: Unexpected number of channels ({segment.channels}) for '{display_name}'.\")\n",
    "            return None, segment.frame_rate, segment.duration_seconds\n",
    "        final_duration_seconds = len(sound_array_reshaped) / segment.frame_rate\n",
    "        print(f\"    Pydub processing complete for '{display_name}'. Array shape: {sound_array_reshaped.shape}, SR: {segment.frame_rate}Hz, Actual Duration: {final_duration_seconds:.2f}s\")\n",
    "        return sound_array_reshaped, segment.frame_rate, final_duration_seconds\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR loading or processing '{display_name}' with Pydub: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, 0, 0\n",
    "\n",
    "# --- Configurable: Target sample rate and channels for audio consistency ---\n",
    "TARGET_AUDIO_SR = 44100\n",
    "TARGET_NARRATION_CHANNELS = int(globals().get('FFMPEG_AUDIO_CHANNELS_FINAL', 2))\n",
    "TARGET_BGM_CHANNELS = int(globals().get('FFMPEG_AUDIO_CHANNELS_FINAL', 2))\n",
    "\n",
    "# --- 1. Load and Materialize Narration Audio ---\n",
    "nar_input_path = globals().get('narration_normalized_path')\n",
    "if not (nar_input_path and os.path.exists(nar_input_path)):\n",
    "    print(\"INFO (Phase B): Normalized narration path not found. Trying original GENERATED_TTS_AUDIO_PATH.\")\n",
    "    nar_input_path = globals().get('GENERATED_TTS_AUDIO_PATH')\n",
    "\n",
    "nar_array_global_ref = None # To hold the array for the lambda closure\n",
    "\n",
    "if nar_input_path and os.path.exists(nar_input_path):\n",
    "    nar_array, nar_fps, nar_duration = load_audio_with_pydub_to_numpy(\n",
    "        nar_input_path,\n",
    "        display_name=\"Narration\",\n",
    "        target_sr=TARGET_AUDIO_SR,\n",
    "        expected_channels=TARGET_NARRATION_CHANNELS\n",
    "    )\n",
    "    if nar_array is not None and nar_duration > 0.01 and nar_fps > 0:\n",
    "        nar_array_global_ref = nar_array # Assign to a variable accessible by the lambda\n",
    "        num_channels_nar_ref = nar_array_global_ref.shape[1] if nar_array_global_ref.ndim > 1 else 1\n",
    "        try:\n",
    "            # VECTORIZED make_frame function for Narration\n",
    "            def nar_make_frame_vectorized(t):\n",
    "                # t can be a scalar or a numpy array\n",
    "                t_np_array = np.asarray(t) # Ensure t is a NumPy array\n",
    "                indices = (t_np_array * nar_fps).astype(int)\n",
    "\n",
    "                # Determine output shape based on whether t was scalar or array\n",
    "                if t_np_array.ndim == 0: # t was scalar\n",
    "                    out_shape = (num_channels_nar_ref,)\n",
    "                else: # t was an array\n",
    "                    out_shape = (len(t_np_array), num_channels_nar_ref)\n",
    "                \n",
    "                output_samples = np.zeros(out_shape, dtype=nar_array_global_ref.dtype)\n",
    "\n",
    "                if t_np_array.ndim == 0: # Scalar t\n",
    "                    if 0 <= indices < len(nar_array_global_ref):\n",
    "                        output_samples = nar_array_global_ref[indices]\n",
    "                else: # Array t\n",
    "                    # Create a mask for valid indices\n",
    "                    valid_mask = (indices >= 0) & (indices < len(nar_array_global_ref))\n",
    "                    # Get valid indices to read from source array\n",
    "                    valid_read_indices = indices[valid_mask]\n",
    "                    \n",
    "                    if len(valid_read_indices) > 0:\n",
    "                        # Place valid samples into the output array at positions indicated by valid_mask\n",
    "                        output_samples[valid_mask] = nar_array_global_ref[valid_read_indices]\n",
    "                return output_samples\n",
    "\n",
    "            narration_audioclip_from_array = AudioClip(nar_make_frame_vectorized, duration=nar_duration, fps=nar_fps)\n",
    "            narration_duration_from_phase_b = nar_duration\n",
    "            print(f\"    SUCCESS: Narration loaded via Pydub & wrapped in MoviePy AudioClip (vectorized).\")\n",
    "            print(f\"      Clip Duration: {narration_audioclip_from_array.duration:.2f}s, FPS: {narration_audioclip_from_array.fps}, Channels (from array): {num_channels_nar_ref}\")\n",
    "        except Exception as e_create_nar_clip:\n",
    "            print(f\"    ERROR creating MoviePy AudioClip for Narration from array: {e_create_nar_clip}\")\n",
    "            traceback.print_exc()\n",
    "            narration_audioclip_from_array = None\n",
    "    else:\n",
    "        print(f\"    ERROR: Failed to get valid audio data from Pydub for Narration (path: {nar_input_path}).\")\n",
    "        narration_audioclip_from_array = None\n",
    "else:\n",
    "    print(f\"ERROR (Phase B): Narration input file path ('{nar_input_path or 'Not Defined'}') not found or invalid.\")\n",
    "\n",
    "# --- 2. Load, Process, and Materialize Background Music (if enabled) ---\n",
    "bgm_array_global_ref = None # To hold the array for the lambda closure\n",
    "\n",
    "if globals().get('ADD_BACKGROUND_MUSIC', False):\n",
    "    bgm_input_path = globals().get('bgm_normalized_path')\n",
    "    if not (bgm_input_path and os.path.exists(bgm_input_path)):\n",
    "        print(\"INFO (Phase B): Normalized BGM path not found. Trying original BACKGROUND_MUSIC_PATH.\")\n",
    "        bgm_input_path = globals().get('BACKGROUND_MUSIC_PATH')\n",
    "\n",
    "    if bgm_input_path and os.path.exists(bgm_input_path):\n",
    "        bgm_array, bgm_fps, bgm_duration_actual = load_audio_with_pydub_to_numpy(\n",
    "            bgm_input_path,\n",
    "            display_name=\"Background Music\",\n",
    "            target_sr=TARGET_AUDIO_SR,\n",
    "            expected_channels=TARGET_BGM_CHANNELS\n",
    "        )\n",
    "        if bgm_array is not None and bgm_duration_actual > 0.01 and bgm_fps > 0:\n",
    "            bgm_array_global_ref = bgm_array # Assign to a variable accessible by the lambda\n",
    "            num_channels_bgm_ref = bgm_array_global_ref.shape[1] if bgm_array_global_ref.ndim > 1 else 1\n",
    "            temp_bgm_audioclip_from_array = None\n",
    "            try:\n",
    "                # VECTORIZED make_frame function for BGM\n",
    "                def bgm_make_frame_vectorized(t):\n",
    "                    t_np_array = np.asarray(t)\n",
    "                    indices = (t_np_array * bgm_fps).astype(int)\n",
    "\n",
    "                    if t_np_array.ndim == 0:\n",
    "                        out_shape = (num_channels_bgm_ref,)\n",
    "                    else:\n",
    "                        out_shape = (len(t_np_array), num_channels_bgm_ref)\n",
    "                    \n",
    "                    output_samples = np.zeros(out_shape, dtype=bgm_array_global_ref.dtype)\n",
    "\n",
    "                    if t_np_array.ndim == 0:\n",
    "                        if 0 <= indices < len(bgm_array_global_ref):\n",
    "                            output_samples = bgm_array_global_ref[indices]\n",
    "                    else:\n",
    "                        valid_mask = (indices >= 0) & (indices < len(bgm_array_global_ref))\n",
    "                        valid_read_indices = indices[valid_mask]\n",
    "                        if len(valid_read_indices) > 0:\n",
    "                            output_samples[valid_mask] = bgm_array_global_ref[valid_read_indices]\n",
    "                    return output_samples\n",
    "\n",
    "                temp_bgm_audioclip_from_array = AudioClip(bgm_make_frame_vectorized, duration=bgm_duration_actual, fps=bgm_fps)\n",
    "                print(f\"    SUCCESS: BGM loaded via Pydub & wrapped in temp MoviePy AudioClip (vectorized).\")\n",
    "                print(f\"      Clip Duration: {temp_bgm_audioclip_from_array.duration:.2f}s, FPS: {temp_bgm_audioclip_from_array.fps}, Channels (from array): {num_channels_bgm_ref}\")\n",
    "\n",
    "                bg_volume = float(globals().get('BACKGROUND_MUSIC_VOLUME', 0.1))\n",
    "                print(f\"    Adjusting BGM volume to: {bg_volume*100:.1f}%...\")\n",
    "                bg_music_audioclip_from_array = temp_bgm_audioclip_from_array.volumex(bg_volume)\n",
    "                bgm_duration_from_phase_b = bg_music_audioclip_from_array.duration\n",
    "                print(f\"    BGM volume adjusted. Final BGM Clip Duration: {bg_music_audioclip_from_array.duration:.2f}s\")\n",
    "\n",
    "            except Exception as e_create_bgm_clip:\n",
    "                print(f\"    ERROR creating/processing MoviePy AudioClip for BGM from array: {e_create_bgm_clip}\")\n",
    "                traceback.print_exc()\n",
    "                bg_music_audioclip_from_array = None\n",
    "            finally:\n",
    "                if temp_bgm_audioclip_from_array and \\\n",
    "                   bg_music_audioclip_from_array is not temp_bgm_audioclip_from_array and \\\n",
    "                   hasattr(temp_bgm_audioclip_from_array, 'close'):\n",
    "                    try:\n",
    "                        temp_bgm_audioclip_from_array.close()\n",
    "                        print(\"    Closed intermediate BGM AudioClip (before volume adjustment).\")\n",
    "                    except Exception as e_close_temp_bgm:\n",
    "                        print(f\"    Warning: Could not close intermediate BGM audioclip: {e_close_temp_bgm}\")\n",
    "        else:\n",
    "            print(f\"    ERROR: Failed to get valid audio data from Pydub for BGM (path: {bgm_input_path}).\")\n",
    "            bg_music_audioclip_from_array = None\n",
    "    else:\n",
    "        print(f\"WARNING (Phase B): ADD_BACKGROUND_MUSIC is True, but BGM input file path ('{bgm_input_path or 'Not Defined'}') not found or invalid.\")\n",
    "else:\n",
    "    print(\"INFO (Phase B): Skipping background music processing as ADD_BACKGROUND_MUSIC is False.\")\n",
    "\n",
    "# --- Phase B Verification & Cleanup ---\n",
    "print(\"\\n--- Phase B Summary ---\")\n",
    "if narration_audioclip_from_array:\n",
    "    print(f\"SUCCESS: Narration AudioClip (from array) PREPARED. Duration: {narration_audioclip_from_array.duration:.2f}s, FPS: {getattr(narration_audioclip_from_array, 'fps', 'N/A')}\")\n",
    "else:\n",
    "    print(\"ERROR: Narration AudioClip (from array) FAILED to prepare.\")\n",
    "\n",
    "if globals().get('ADD_BACKGROUND_MUSIC', False):\n",
    "    if bg_music_audioclip_from_array:\n",
    "        print(f\"SUCCESS: BGM AudioClip (from array, volume adjusted) PREPARED. Duration: {bg_music_audioclip_from_array.duration:.2f}s, FPS: {getattr(bg_music_audioclip_from_array, 'fps', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"ERROR: BGM AudioClip (from array) FAILED to prepare (though ADD_BACKGROUND_MUSIC was True).\")\n",
    "else:\n",
    "    print(\"INFO: Background music was not added (ADD_BACKGROUND_MUSIC is False).\")\n",
    "\n",
    "# Forcing a clear of the large numpy arrays from memory once the AudioClips are created.\n",
    "# The lambdas will hold references, but the original large named variables can be deleted.\n",
    "if 'nar_array' in locals():\n",
    "    del nar_array\n",
    "    print(\"INFO: Deleted 'nar_array' from local scope.\")\n",
    "if 'bgm_array' in locals():\n",
    "    del bgm_array\n",
    "    print(\"INFO: Deleted 'bgm_array' from local scope.\")\n",
    "# The nar_array_global_ref and bgm_array_global_ref are still captured by lambdas.\n",
    "\n",
    "gc.collect()\n",
    "print(\"--- Phase B: Audio Component Preparation Complete ---\")\n",
    "print(\"#######################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Phase 3a - Audio Chunking (for Image Pacing & Initial LM Text)\n",
    "# -----------------------------------------------------------------------\n",
    "# Global variables from Cell 2 needed:\n",
    "# GENERATED_TTS_AUDIO_PATH (master audio from Cell 3)\n",
    "# MIN_CHUNK_DURATION_S_PACING, MAX_CHUNK_DURATION_S_PACING, FLEXIBILITY_S_PACING,\n",
    "# SILENCE_THRESH_DB_OFFSET_PACING, MIN_SILENCE_LEN_MS_PACING, KEEP_SILENCE_MS_PACING,\n",
    "# MIN_ACCEPTABLE_CHUNK_MS_PACING\n",
    "# KAGGLE_WORKING_DIR (for saving debug chunks if needed)\n",
    "\n",
    "# Ensure AudioSegment, split_on_silence, os, math are imported from Cell 1\n",
    "\n",
    "print(\"Phase 3a (Audio Chunking for Pacing): Starting to chunk the master TTS audio...\")\n",
    "\n",
    "# --- 1. Define the smart_chunk_audio function (if not already globally defined or imported) ---\n",
    "# Assuming 'smart_chunk_audio' function is the same as the one from our previous Cell 4.\n",
    "# If you defined it inside the previous Cell 4, you'll need to redefine it here or make it global.\n",
    "# For clarity, I'll include its definition here.\n",
    "\n",
    "def smart_chunk_audio(audio_path,\n",
    "                      min_target_ms,\n",
    "                      max_target_ms,\n",
    "                      flexible_max_ms,\n",
    "                      silence_db_offset,\n",
    "                      min_silence_ms,\n",
    "                      keep_silence_setting,\n",
    "                      min_acceptable_chunk_ms,\n",
    "                      verbose=False):\n",
    "    \"\"\"\n",
    "    Chunks audio based on silence, aiming for target durations.\n",
    "    `keep_silence_setting` can be a boolean or milliseconds.\n",
    "    Returns list of (AudioSegment_object, duration_ms)\n",
    "    \"\"\"\n",
    "    if verbose: print(f\"--- smart_chunk_audio VERBOSE MODE (Pacing) ---\")\n",
    "    if verbose: print(f\"  Parameters: min_target={min_target_ms/1000:.1f}s, max_target={max_target_ms/1000:.1f}s, flex_max={flexible_max_ms/1000:.1f}s\")\n",
    "    # ... (rest of the smart_chunk_audio function from the previous working Cell 4) ...\n",
    "    # Make sure it returns: final_chunks_with_durations, total_discarded_ms\n",
    "    # where final_chunks_with_durations is a list of (AudioSegment_object, duration_ms)\n",
    "\n",
    "    # --- [PASTING THE BODY of smart_chunk_audio from previous Cell 4 for completeness] ---\n",
    "    if not audio_path or not os.path.exists(audio_path):\n",
    "        print(f\"Audio file not found at {audio_path}\")\n",
    "        return [], 0.0\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        if verbose: print(f\"Loaded audio: {os.path.basename(audio_path)}, Duration: {len(audio) / 1000:.2f}s, Avg dBFS: {audio.dBFS:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        return [], 0.0\n",
    "\n",
    "    total_discarded_ms = 0\n",
    "    effective_silence_thresh = audio.dBFS + silence_db_offset\n",
    "    if verbose: print(f\"Using effective silence threshold: {effective_silence_thresh:.2f} dBFS\")\n",
    "    \n",
    "    silence_based_segments = split_on_silence(\n",
    "        audio, min_silence_len=min_silence_ms, silence_thresh=effective_silence_thresh, keep_silence=keep_silence_setting\n",
    "    )\n",
    "\n",
    "    if not silence_based_segments: # Handle no silences found\n",
    "        if verbose: print(\"No silences found by pydub.split_on_silence.\")\n",
    "        # Simplified fallback: return whole audio if it's not too long, or split naively\n",
    "        if len(audio) <= flexible_max_ms and len(audio) >= min_acceptable_chunk_ms:\n",
    "            return [(audio, len(audio))], total_discarded_ms\n",
    "        elif len(audio) > 0: # Naive split if too long or too short but not zero\n",
    "            chunks = []\n",
    "            for i in range(0, len(audio), max_target_ms):\n",
    "                chunk = audio[i:i + max_target_ms]\n",
    "                if len(chunk) >= min_acceptable_chunk_ms: chunks.append((chunk, len(chunk)))\n",
    "                elif len(chunk) > 0: total_discarded_ms += len(chunk)\n",
    "            return chunks, total_discarded_ms\n",
    "        return [], total_discarded_ms\n",
    "\n",
    "\n",
    "    processed_chunks_step1 = []\n",
    "    current_chunk_buffer = AudioSegment.empty()\n",
    "    for segment_part in silence_based_segments:\n",
    "        if len(current_chunk_buffer) == 0: current_chunk_buffer = segment_part\n",
    "        elif len(current_chunk_buffer) + len(segment_part) > flexible_max_ms and len(current_chunk_buffer) > 0:\n",
    "            if len(current_chunk_buffer) >= min_acceptable_chunk_ms: processed_chunks_step1.append(current_chunk_buffer)\n",
    "            elif len(current_chunk_buffer) > 0: total_discarded_ms += len(current_chunk_buffer)\n",
    "            current_chunk_buffer = segment_part\n",
    "        else: current_chunk_buffer += segment_part\n",
    "    if len(current_chunk_buffer) >= min_acceptable_chunk_ms: processed_chunks_step1.append(current_chunk_buffer)\n",
    "    elif len(current_chunk_buffer) > 0: total_discarded_ms += len(current_chunk_buffer)\n",
    "    if verbose: print(f\"Formed {len(processed_chunks_step1)} primary chunks after initial combination.\")\n",
    "\n",
    "    final_chunks_segments_only = []\n",
    "    temp_merge_buffer = AudioSegment.empty()\n",
    "    for chunk in processed_chunks_step1:\n",
    "        duration = len(chunk)\n",
    "        if duration > max_target_ms:\n",
    "            if len(temp_merge_buffer) >= min_acceptable_chunk_ms: final_chunks_segments_only.append(temp_merge_buffer)\n",
    "            elif len(temp_merge_buffer) > 0: total_discarded_ms += len(temp_merge_buffer)\n",
    "            temp_merge_buffer = AudioSegment.empty()\n",
    "            num_sub_chunks = math.ceil(duration / max_target_ms)\n",
    "            sub_chunk_len_ideal = math.ceil(duration / num_sub_chunks)\n",
    "            for sub_i in range(num_sub_chunks):\n",
    "                sub_chunk = chunk[sub_i * sub_chunk_len_ideal : (sub_i + 1) * sub_chunk_len_ideal]\n",
    "                if len(sub_chunk) >= min_acceptable_chunk_ms: final_chunks_segments_only.append(sub_chunk)\n",
    "                elif len(sub_chunk) > 0: total_discarded_ms += len(sub_chunk)\n",
    "        elif duration < min_target_ms and duration >= min_acceptable_chunk_ms:\n",
    "            if len(temp_merge_buffer) == 0: temp_merge_buffer = chunk\n",
    "            elif len(temp_merge_buffer) + duration <= max_target_ms:\n",
    "                temp_merge_buffer += chunk\n",
    "                if len(temp_merge_buffer) >= min_target_ms:\n",
    "                    final_chunks_segments_only.append(temp_merge_buffer)\n",
    "                    temp_merge_buffer = AudioSegment.empty()\n",
    "            else:\n",
    "                if len(temp_merge_buffer) >= min_acceptable_chunk_ms: final_chunks_segments_only.append(temp_merge_buffer)\n",
    "                elif len(temp_merge_buffer) > 0: total_discarded_ms += len(temp_merge_buffer)\n",
    "                temp_merge_buffer = chunk\n",
    "        elif duration >= min_target_ms:\n",
    "            if len(temp_merge_buffer) >= min_acceptable_chunk_ms: final_chunks_segments_only.append(temp_merge_buffer)\n",
    "            elif len(temp_merge_buffer) > 0: total_discarded_ms += len(temp_merge_buffer)\n",
    "            temp_merge_buffer = AudioSegment.empty()\n",
    "            final_chunks_segments_only.append(chunk)\n",
    "        elif duration > 0: total_discarded_ms += duration\n",
    "    if len(temp_merge_buffer) >= min_acceptable_chunk_ms: final_chunks_segments_only.append(temp_merge_buffer)\n",
    "    elif len(temp_merge_buffer) > 0: total_discarded_ms += len(temp_merge_buffer)\n",
    "    if verbose: print(f\"Formed {len(final_chunks_segments_only)} chunks after Step 3 post-processing.\")\n",
    "\n",
    "    if not final_chunks_segments_only and len(audio) > 0: # Fallback\n",
    "        if verbose: print(\"Fallback: No usable chunks from main logic. Using original audio if it fits, or hard splitting.\")\n",
    "        if len(audio) <= flexible_max_ms and len(audio) >= min_acceptable_chunk_ms:\n",
    "            final_chunks_segments_only.append(audio)\n",
    "        elif len(audio) > flexible_max_ms :\n",
    "            for i_fb in range(0, len(audio), max_target_ms):\n",
    "                chunk_fb = audio[i_fb:i_fb+max_target_ms]\n",
    "                if len(chunk_fb) >= min_acceptable_chunk_ms: final_chunks_segments_only.append(chunk_fb)\n",
    "                elif len(chunk_fb)>0: total_discarded_ms += len(chunk_fb)\n",
    "    \n",
    "    final_chunks_with_durations = [(seg, len(seg)) for seg in final_chunks_segments_only]\n",
    "    return final_chunks_with_durations, total_discarded_ms\n",
    "    # --- [END OF PASTED smart_chunk_audio BODY] ---\n",
    "\n",
    "# --- 2. Perform Chunking for Pacing ---\n",
    "# This list will store tuples of (AudioSegment_object, duration_in_ms)\n",
    "# These chunks are specifically for determining image display times and getting initial text for LM.\n",
    "audio_chunks_for_pacing = [] # New variable name for clarity\n",
    "total_pacing_audio_discarded_ms = 0\n",
    "original_master_audio_duration_s = -1\n",
    "\n",
    "# Check if the master TTS audio file exists\n",
    "print(f\"DEBUG: Cell 4 (Pacing Chunking) is attempting to chunk audio from: {GENERATED_TTS_AUDIO_PATH}\")\n",
    "if GENERATED_TTS_AUDIO_PATH and os.path.exists(GENERATED_TTS_AUDIO_PATH):\n",
    "    try:\n",
    "        # Get original duration for comparison\n",
    "        master_audio_segment_for_check = AudioSegment.from_file(GENERATED_TTS_AUDIO_PATH)\n",
    "        original_master_audio_duration_s = len(master_audio_segment_for_check) / 1000.0\n",
    "        print(f\"Master TTS audio duration (for pacing chunking): {original_master_audio_duration_s:.2f}s\")\n",
    "\n",
    "        # Use the *_PACING parameters from Cell 2\n",
    "        audio_chunks_for_pacing, total_pacing_audio_discarded_ms = smart_chunk_audio(\n",
    "            audio_path=GENERATED_TTS_AUDIO_PATH,\n",
    "            min_target_ms=MIN_CHUNK_DURATION_S_PACING * 1000,\n",
    "            max_target_ms=MAX_CHUNK_DURATION_S_PACING * 1000,\n",
    "            flexible_max_ms=(MAX_CHUNK_DURATION_S_PACING + FLEXIBILITY_S_PACING) * 1000,\n",
    "            silence_db_offset=SILENCE_THRESH_DB_OFFSET_PACING,\n",
    "            min_silence_ms=MIN_SILENCE_LEN_MS_PACING,\n",
    "            keep_silence_setting=KEEP_SILENCE_MS_PACING,\n",
    "            min_acceptable_chunk_ms=MIN_ACCEPTABLE_CHUNK_MS_PACING,\n",
    "            verbose=True # Enable verbose logging for detailed output\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPacing audio chunking complete. Generated {len(audio_chunks_for_pacing)} chunks for pacing.\")\n",
    "        \n",
    "        chunked_total_duration_s_pacing = sum(d/1000.0 for _, d in audio_chunks_for_pacing)\n",
    "        print(f\"Total duration of pacing chunks: {chunked_total_duration_s_pacing:.2f}s\")\n",
    "        print(f\"Total duration explicitly discarded by pacing chunker: {total_pacing_audio_discarded_ms / 1000.0:.2f}s\")\n",
    "        \n",
    "        calculated_lost_duration_s_pacing = original_master_audio_duration_s - chunked_total_duration_s_pacing\n",
    "        print(f\"Calculated implicitly lost duration for pacing chunks (Original - Chunked Sum): {calculated_lost_duration_s_pacing:.2f}s\")\n",
    "\n",
    "        # Warning if significant audio is lost, as this affects video length\n",
    "        if abs(calculated_lost_duration_s_pacing) > 1.0: # Allow up to 1s total loss for rounding/silence effects\n",
    "            print(\"WARNING: Pacing chunking resulted in a total duration significantly different from the original master audio!\")\n",
    "            print(\"         This will affect the final video length relative to the master audio.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during audio chunking for pacing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        audio_chunks_for_pacing = [] # Ensure it's empty on failure\n",
    "else:\n",
    "    print(f\"ERROR: Master TTS audio file ('{GENERATED_TTS_AUDIO_PATH}') not found. Skipping pacing chunking.\")\n",
    "    audio_chunks_for_pacing = []\n",
    "\n",
    "# --- 3. Verify Output & Optional Debug Save ---\n",
    "if audio_chunks_for_pacing:\n",
    "    print(f\"\\nSUCCESS: Audio chunking for pacing produced {len(audio_chunks_for_pacing)} segments.\")\n",
    "    \n",
    "    # Optional: Save first few chunks for manual listening verification\n",
    "    # print(\"\\nDEBUG (Cell 4 - Pacing): Saving first few audio chunks for manual inspection...\")\n",
    "    # for i, (segment, duration) in enumerate(audio_chunks_for_pacing[:2]): # Save first 2\n",
    "    #     chunk_filename = os.path.join(KAGGLE_WORKING_DIR, f\"debug_PacingChunk_C4_num_{i}.wav\")\n",
    "    #     try:\n",
    "    #         segment.export(chunk_filename, format=\"wav\")\n",
    "    #         print(f\"  Saved for debug: {chunk_filename}\")\n",
    "    #     except Exception as e_export_debug:\n",
    "    #         print(f\"  ERROR saving debug pacing chunk {chunk_filename}: {e_export_debug}\")\n",
    "    #     if i >= 1: break \n",
    "else:\n",
    "    print(\"\\nERROR: No audio chunks were generated for pacing. Subsequent steps (LM, Image Gen) will be affected.\")\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------\")\n",
    "print(\"Phase 3a (Audio Chunking for Pacing): Chunking Complete.\")\n",
    "print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Phase 3b - STT with whisper-timestamped (Corrected API Call)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Global variables from Cell 2 needed:\n",
    "# STT_MODEL_ID_WHISPER (e.g., \"base\", \"small\"), DEVICE\n",
    "# 'audio_chunks_for_pacing' (output from Cell 4) should be available.\n",
    "# KAGGLE_WORKING_DIR\n",
    "\n",
    "import whisper_timestamped as whisper # Import the new library\n",
    "# Ensure 'numpy' as 'np', 'os', 'gc', 'AudioSegment' from 'pydub', 'shutil' are imported (Cell 1).\n",
    "import shutil # For rmtree if temp_stt_input_dir is used robustly\n",
    "\n",
    "print(\"Phase 3b (STT with whisper-timestamped): Starting Speech-to-Text...\")\n",
    "\n",
    "print(\"DEBUG: Clearing/Re-initializing STT output lists for this run.\")\n",
    "chunk_text_for_lm_input = []\n",
    "all_word_level_data_for_ass = []\n",
    "\n",
    "stt_model = None\n",
    "# STT_MODEL_ID_WHISPER in Cell 2 should be \"base\", \"small\", etc. (NOT \"openai/whisper-base\")\n",
    "print(f\"Loading whisper-timestamped model: '{STT_MODEL_ID_WHISPER}' onto {DEVICE}...\")\n",
    "try:\n",
    "    stt_model = whisper.load_model(STT_MODEL_ID_WHISPER, device=DEVICE)\n",
    "    print(\"whisper-timestamped model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load whisper-timestamped model '{STT_MODEL_ID_WHISPER}'. Error: {e}\")\n",
    "    import traceback; traceback.print_exc()\n",
    "\n",
    "current_absolute_time_offset_s = 0.0\n",
    "temp_stt_input_dir = os.path.join(KAGGLE_WORKING_DIR, \"temp_stt_input_audio_files\") # More descriptive name\n",
    "os.makedirs(temp_stt_input_dir, exist_ok=True)\n",
    "\n",
    "if stt_model and 'audio_chunks_for_pacing' in locals() and audio_chunks_for_pacing:\n",
    "    print(f\"\\nTranscribing {len(audio_chunks_for_pacing)} pacing chunks using whisper-timestamped...\")\n",
    "    \n",
    "    for i, (chunk_segment, duration_ms) in enumerate(audio_chunks_for_pacing):\n",
    "        print(f\"  Processing chunk {i+1}/{len(audio_chunks_for_pacing)} (Dur: {duration_ms/1000.0:.2f}s, Abs Offset: {current_absolute_time_offset_s:.2f}s)...\")\n",
    "        \n",
    "        raw_text_for_lm = \"[WHISPER_TS_FAILED_CHUNK]\"\n",
    "        temp_wav_path = os.path.join(temp_stt_input_dir, f\"temp_stt_chunk_{i}.wav\") # Slightly changed temp name\n",
    "\n",
    "        try:\n",
    "            # Ensure chunk is 16kHz mono and export to temp file for whisper-timestamped\n",
    "            if chunk_segment.frame_rate != 16000: chunk_segment_16k = chunk_segment.set_frame_rate(16000)\n",
    "            else: chunk_segment_16k = chunk_segment\n",
    "            if chunk_segment_16k.channels != 1: chunk_segment_16k = chunk_segment_16k.set_channels(1)\n",
    "            chunk_segment_16k.export(temp_wav_path, format=\"wav\")\n",
    "\n",
    "            # Use whisper_timestamped's audio loader\n",
    "            audio_for_ts = whisper.load_audio(temp_wav_path) \n",
    "            \n",
    "            # CORRECTED: Use the standard .transcribe() method from whisper_timestamped\n",
    "            result = whisper.transcribe(\n",
    "                stt_model, \n",
    "                audio_for_ts, \n",
    "                language=\"en\"\n",
    "                # Common options you might add if needed:\n",
    "                # beam_size=5,\n",
    "                # best_of=5,\n",
    "                # temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0) # tuple for multiple passes\n",
    "            )\n",
    "\n",
    "            raw_text_for_lm = result.get(\"text\", \"[WHISPER_TS_NO_TEXT]\").strip()\n",
    "            print(f\"    Raw text for LM: \\\"{raw_text_for_lm[:100].replace(chr(10), ' ')}...\\\"\")\n",
    "            \n",
    "            chunk_text_for_lm_input.append({\n",
    "                \"raw_text\": raw_text_for_lm,\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"original_chunk_index\": i\n",
    "            })\n",
    "\n",
    "            if \"segments\" in result and result[\"segments\"]:\n",
    "                for segment_info in result[\"segments\"]:\n",
    "                    for word_info in segment_info.get(\"words\", []): # 'words' should be present per segment\n",
    "                        word_text = word_info.get(\"text\", \"\").strip()\n",
    "                        word_start_s = word_info.get(\"start\")\n",
    "                        word_end_s = word_info.get(\"end\")\n",
    "\n",
    "                        if word_text and isinstance(word_start_s, (float, int)) and isinstance(word_end_s, (float, int)) and word_end_s >= word_start_s:\n",
    "                            all_word_level_data_for_ass.append({\n",
    "                                \"text\": word_text,\n",
    "                                \"start\": float(word_start_s) + current_absolute_time_offset_s,\n",
    "                                \"end\": float(word_end_s) + current_absolute_time_offset_s\n",
    "                            })\n",
    "            else:\n",
    "                print(f\"    WARNING: No 'segments' with 'words' found in whisper-timestamped result for chunk {i+1}.\")\n",
    "\n",
    "        except Exception as e_transcribe_ts:\n",
    "            print(f\"    ERROR transcribing chunk {i+1} with whisper-timestamped: {e_transcribe_ts}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            if not any(d['original_chunk_index'] == i for d in chunk_text_for_lm_input):\n",
    "                chunk_text_for_lm_input.append({\n",
    "                    \"raw_text\": \"[WHISPER_TS_CHUNK_ERROR]\",\n",
    "                    \"duration_ms\": duration_ms,\n",
    "                    \"original_chunk_index\": i\n",
    "                })\n",
    "        finally:\n",
    "            if os.path.exists(temp_wav_path):\n",
    "                try: os.remove(temp_wav_path)\n",
    "                except: pass # Ignore error on temp file removal\n",
    "        \n",
    "        current_absolute_time_offset_s += (duration_ms / 1000.0)\n",
    "    \n",
    "    # Clean up the temporary input directory after processing all chunks\n",
    "    if os.path.exists(temp_stt_input_dir):\n",
    "        try: \n",
    "            shutil.rmtree(temp_stt_input_dir)\n",
    "            print(f\"Cleaned up temporary STT input directory: {temp_stt_input_dir}\")\n",
    "        except Exception as e_rm_dir: \n",
    "            print(f\"Warning: Could not remove temp STT input directory {temp_stt_input_dir}: {e_rm_dir}\")\n",
    "\n",
    "    print(\"\\nTranscription of all chunks (for LM and Subtitles via whisper-timestamped) complete.\")\n",
    "\n",
    "elif not stt_model:\n",
    "    print(\"whisper-timestamped STT model not loaded. Skipping transcription.\")\n",
    "else:\n",
    "    print(\"No pacing audio chunks found from Phase 3a (Cell 4). Skipping transcription.\")\n",
    "\n",
    "# --- 3. Clean up STT Model from Memory ---\n",
    "print(\"\\nCleaning up whisper-timestamped STT model from memory...\")\n",
    "if stt_model is not None: del stt_model; stt_model = None\n",
    "clear_gpu_memory() \n",
    "print(\"whisper-timestamped STT model cleanup attempted.\")\n",
    "\n",
    "# --- 4. Verify Outputs ---\n",
    "if chunk_text_for_lm_input:\n",
    "    print(f\"\\nSUCCESS: Generated {len(chunk_text_for_lm_input)} raw text segments for LM input.\")\n",
    "    if chunk_text_for_lm_input: print(f\"  Sample LM input text (first item): {chunk_text_for_lm_input[0]['raw_text'][:100]}...\")\n",
    "else: print(\"\\nWARNING: No raw text segments generated for LM input.\")\n",
    "\n",
    "if all_word_level_data_for_ass:\n",
    "    print(f\"\\nSUCCESS: Extracted {len(all_word_level_data_for_ass)} words with absolute timestamps (from chunks via whisper-timestamped) for subtitles.\")\n",
    "    if all_word_level_data_for_ass: print(f\"  Sample subtitle word data (first 10 words):\"); [print(f\"    Word {k}: {wi}\") for k, wi in enumerate(all_word_level_data_for_ass[:10])]\n",
    "else: print(\"\\nWARNING: No word-level timestamp data extracted from chunks (whisper-timestamped) for subtitles.\")\n",
    "\n",
    "print(\"\\n--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Phase 3b (STT with whisper-timestamped): STT for LM Text and Subtitle Timestamps Complete.\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5X: Phase 3X - Regex-based Heading Identification (REVISED)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Global variables from Cell 2 needed:\n",
    "# ENABLE_HEADING_SUBTITLE_STYLE, TEXT_TO_SPEAK,\n",
    "# RAW_IDENTIFIED_HEADINGS_LIST_PATH (optional save path)\n",
    "\n",
    "# Imports from Cell 1 (or here for clarity)\n",
    "import re\n",
    "import json # For saving list to file if RAW_IDENTIFIED_HEADINGS_LIST_PATH is used\n",
    "\n",
    "print(\"Phase 3X (Regex Heading ID): Starting Heading Identification using Regular Expressions...\")\n",
    "\n",
    "# --- 0. Initialize output list ---\n",
    "list_of_identified_heading_strings = []\n",
    "\n",
    "if 'ENABLE_HEADING_SUBTITLE_STYLE' in globals() and ENABLE_HEADING_SUBTITLE_STYLE:\n",
    "    if not ('TEXT_TO_SPEAK' in globals() and TEXT_TO_SPEAK and TEXT_TO_SPEAK.strip()):\n",
    "        print(\"WARNING: TEXT_TO_SPEAK is empty or contains only whitespace. Skipping heading identification.\")\n",
    "    else:\n",
    "        print(\"\\nIdentifying headings from the full script using regex pattern ':content::'...\")\n",
    "        try:\n",
    "            # Regex pattern: finds text between a starting ':' and an ending '::'\n",
    "            # (.*?) captures the content in between, non-greedily.\n",
    "            regex_pattern = r\":(.*?)::\"\n",
    "            identified_headings_raw = re.findall(regex_pattern, TEXT_TO_SPEAK)\n",
    "            \n",
    "            if identified_headings_raw:\n",
    "                # Clean whitespace from extracted headings and filter out empty ones\n",
    "                list_of_identified_heading_strings = [h.strip() for h in identified_headings_raw if h.strip()]\n",
    "                \n",
    "                if list_of_identified_heading_strings:\n",
    "                    print(f\"  Successfully identified {len(list_of_identified_heading_strings)} potential headings using regex:\")\n",
    "                    for idx, h_text in enumerate(list_of_identified_heading_strings[:10]): # Print first 10\n",
    "                        print(f\"    H{idx+1}: {h_text}\")\n",
    "                    if len(list_of_identified_heading_strings) > 10: print(\"    ...\")\n",
    "                    \n",
    "                    # Optional: Save to file if the path variable is defined and used\n",
    "                    if 'RAW_IDENTIFIED_HEADINGS_LIST_PATH' in globals() and RAW_IDENTIFIED_HEADINGS_LIST_PATH:\n",
    "                        try:\n",
    "                            with open(RAW_IDENTIFIED_HEADINGS_LIST_PATH, 'w', encoding='utf-8') as f:\n",
    "                                json.dump(list_of_identified_heading_strings, f, indent=2)\n",
    "                            print(f\"  Raw identified headings saved to: {RAW_IDENTIFIED_HEADINGS_LIST_PATH}\")\n",
    "                        except Exception as e_save_h:\n",
    "                            print(f\"  WARNING: Could not save raw headings to file: {e_save_h}\")\n",
    "                else:\n",
    "                    print(\"  Regex found matches for the pattern ':content::', but the content between markers was empty after stripping.\")\n",
    "            else:\n",
    "                print(\"  No headings matching the pattern ':content::' found in TEXT_TO_SPEAK.\")\n",
    "                \n",
    "        except Exception as e_regex_headings:\n",
    "            print(f\"  ERROR during regex-based heading identification: {e_regex_headings}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    if not ('ENABLE_HEADING_SUBTITLE_STYLE' in globals() and ENABLE_HEADING_SUBTITLE_STYLE):\n",
    "        print(\"Skipping heading identification as ENABLE_HEADING_SUBTITLE_STYLE is False or not defined in Cell 2.\")\n",
    "    # The case where TEXT_TO_SPEAK is empty is handled inside the main 'if' block.\n",
    "\n",
    "# Ensure the output variable is defined globally even if skipped or failed\n",
    "if 'list_of_identified_heading_strings' not in globals():\n",
    "    list_of_identified_heading_strings = []\n",
    "\n",
    "print(f\"\\\\nOutput of this cell: `list_of_identified_heading_strings` (contains {len(list_of_identified_heading_strings)} items)\")\n",
    "print(\"Phase 3X (Regex Heading ID): Heading Identification Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10 (was old Cell 9): Phase 6c - Generate ASS Subtitle File with Heading Styles (Offset for Intro)\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import difflib # For fuzzy string matching/sequence matching\n",
    "import re\n",
    "import json # Should be imported in Cell 1, but good to have if this cell is run standalone for testing\n",
    "import datetime # For sorting dialogue lines by time\n",
    "import math # Should be imported in Cell 1\n",
    "import os # Should be imported in Cell 1\n",
    "\n",
    "print(\"#############################################################################################\")\n",
    "print(\"Phase 6c (ASS Generation with Headings & Intro Offset): Generating ASS Subtitle File...\")\n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# --- 1. Helper Functions ---\n",
    "def format_ass_time(seconds: float) -> str:\n",
    "    if not isinstance(seconds, (int, float)) or seconds < 0: seconds = 0.0\n",
    "    milliseconds = round(seconds * 1000.0)\n",
    "    hours = int(milliseconds // 3_600_000)\n",
    "    milliseconds %= 3_600_000\n",
    "    minutes = int(milliseconds // 60_000)\n",
    "    milliseconds %= 60_000\n",
    "    secs = int(milliseconds // 1_000)\n",
    "    milliseconds %= 1_000\n",
    "    centiseconds = int(milliseconds // 10) # ASS uses centiseconds (hundredths of a second)\n",
    "    return f\"{hours:d}:{minutes:02d}:{secs:02d}.{centiseconds:02d}\"\n",
    "\n",
    "def escape_ass_text(text: str) -> str:\n",
    "    if not isinstance(text, str): text = str(text) # Ensure it's a string\n",
    "    # Basic ASS escaping: \\N for newline, others might be needed depending on content\n",
    "    # For dialogue lines, MoviePy's ffmpeg burn-in might handle some things,\n",
    "    # but explicit \\N for newlines intended from whisper is good.\n",
    "    # Whisper typically doesn't output \\n in single word 'text' fields.\n",
    "    # This mainly escapes characters that have special meaning in ASS tags.\n",
    "    text = text.strip().replace('\\\\', '\\\\\\\\').replace('{', '\\\\{').replace('}', '\\\\}')\n",
    "    return text\n",
    "\n",
    "def normalize_text_for_matching(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    text = text.lower()\n",
    "    # Remove punctuation more aggressively for matching, keep spaces and alphanumerics\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # \\w includes underscore\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "# --- Determine Subtitle Time Offset (based on Intro Screen from Cell 2 configs) ---\n",
    "subtitle_time_offset_s = 0.0\n",
    "# These globals should come from Cell 2\n",
    "_add_intro_config = globals().get('ADD_INTRO_OUTRO_SCREENS', False)\n",
    "_intro_duration_config = globals().get('INTRO_SCREEN_DURATION_S', 0.0)\n",
    "\n",
    "if _add_intro_config and _intro_duration_config > 0:\n",
    "    subtitle_time_offset_s = float(_intro_duration_config)\n",
    "    print(f\"INFO (ASS Gen): Applying a subtitle time offset of {subtitle_time_offset_s:.2f}s due to intro screen.\")\n",
    "else:\n",
    "    print(\"INFO (ASS Gen): No subtitle time offset applied (no intro screen, or intro duration is zero).\")\n",
    "\n",
    "\n",
    "# --- 2. Construct ASS Header and Style Definitions ---\n",
    "# Pulling style configurations from globals (expected to be set in Cell 2)\n",
    "_ass_font_name = globals().get('ASS_FONT_NAME', \"Arial\")\n",
    "_ass_font_size = globals().get('ASS_FONT_SIZE', 72)\n",
    "_ass_primary_colour = globals().get('ASS_PRIMARY_COLOUR', \"&H00FFFFFF\") # White\n",
    "_ass_secondary_colour = globals().get('ASS_SECONDARY_COLOUR_ASS', \"&H000000FF\") # Red (often for karaoke)\n",
    "_ass_outline_colour = globals().get('ASS_OUTLINE_COLOUR', \"&H00000000\") # Black\n",
    "_ass_back_colour = globals().get('ASS_BACK_COLOUR', \"&H99000000\") # Semi-transparent black\n",
    "_ass_bold = globals().get('ASS_BOLD', -1) # -1 for True, 0 for False\n",
    "_ass_alignment = globals().get('ASS_ALIGNMENT', 2) # 2 is bottom center\n",
    "_ass_margin_l = globals().get('ASS_MARGIN_L', 10)\n",
    "_ass_margin_r = globals().get('ASS_MARGIN_R', 10)\n",
    "_ass_margin_v = globals().get('ASS_MARGIN_V', 10)\n",
    "\n",
    "_enable_heading_style = globals().get('ENABLE_HEADING_SUBTITLE_STYLE', False)\n",
    "_ass_heading_font_name = globals().get('ASS_HEADING_FONT_NAME', \"Arial\")\n",
    "_ass_heading_font_size = globals().get('ASS_HEADING_FONT_SIZE', 86)\n",
    "_ass_heading_primary_colour = globals().get('ASS_HEADING_PRIMARY_COLOUR', \"&H00FFFF00\") # Yellow\n",
    "_ass_heading_outline_colour = globals().get('ASS_HEADING_OUTLINE_COLOUR', \"&H00000000\")\n",
    "_ass_heading_back_colour = globals().get('ASS_HEADING_BACK_COLOUR', \"&H60000000\")\n",
    "_ass_heading_bold = globals().get('ASS_HEADING_BOLD', -1)\n",
    "_ass_heading_italic = globals().get('ASS_HEADING_ITALIC', 0)\n",
    "_ass_heading_alignment = globals().get('ASS_HEADING_ALIGNMENT', 5) # Middle center\n",
    "_ass_heading_margin_l = globals().get('ASS_HEADING_MARGIN_L', 30)\n",
    "_ass_heading_margin_r = globals().get('ASS_HEADING_MARGIN_R', 30)\n",
    "_ass_heading_margin_v = globals().get('ASS_HEADING_MARGIN_V', 50)\n",
    "\n",
    "_video_target_width = globals().get('VIDEO_TARGET_WIDTH', 1920)\n",
    "_video_target_height = globals().get('VIDEO_TARGET_HEIGHT', 1080)\n",
    "\n",
    "\n",
    "ass_style_default_line = (\n",
    "    f\"Style: Default,{_ass_font_name},{_ass_font_size},\"\n",
    "    f\"{_ass_primary_colour},{_ass_secondary_colour},\"\n",
    "    f\"{_ass_outline_colour},{_ass_back_colour},\"\n",
    "    f\"{_ass_bold},0,0,0,100,100,0,0,1,2,1,\" # Assuming BorderStyle 1, Outline 2, Shadow 1\n",
    "    f\"{_ass_alignment},{_ass_margin_l},{_ass_margin_r},{_ass_margin_v},1\" # Encoding 1 (Unicode)\n",
    ")\n",
    "ass_style_heading_line = \"\"\n",
    "if _enable_heading_style:\n",
    "    ass_style_heading_line = (\n",
    "        f\"Style: HeadingStyle,{_ass_heading_font_name},{_ass_heading_font_size},\"\n",
    "        f\"{_ass_heading_primary_colour},{_ass_secondary_colour},\" # Using default secondary for headings too\n",
    "        f\"{_ass_heading_outline_colour},{_ass_heading_back_colour},\"\n",
    "        f\"{_ass_heading_bold},{_ass_heading_italic},0,0,100,100,0,0,1,2,1,\"\n",
    "        f\"{_ass_heading_alignment},{_ass_heading_margin_l},{_ass_heading_margin_r},{_ass_heading_margin_v},1\"\n",
    "    )\n",
    "\n",
    "ass_header_content = f\"\"\"[Script Info]\n",
    "Title: Auto-Generated Subtitles\n",
    "ScriptType: v4.00+\n",
    "WrapStyle: 0\n",
    "PlayResX: {_video_target_width}\n",
    "PlayResY: {_video_target_height}\n",
    "ScaledBorderAndShadow: yes\n",
    "YCbCr Matrix: None\n",
    "\n",
    "[V4+ Styles]\n",
    "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n",
    "{ass_style_default_line}\n",
    "{ass_style_heading_line if _enable_heading_style else \"\"}\n",
    "\n",
    "[Events]\n",
    "Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n",
    "\"\"\"\n",
    "\n",
    "# --- 3. Align Identified Headings with Word Timestamps ---\n",
    "aligned_headings_data = []\n",
    "stt_words_normalized_with_indices = []\n",
    "_all_word_data = globals().get('all_word_level_data_for_ass', []) # Get from globals\n",
    "\n",
    "if _all_word_data:\n",
    "    for idx, w_data in enumerate(_all_word_data):\n",
    "        stt_words_normalized_with_indices.append(\n",
    "            (normalize_text_for_matching(w_data.get(\"text\",\"\")), idx) # Use .get for safety\n",
    "        )\n",
    "\n",
    "stt_word_indices_used_by_headings = set()\n",
    "_list_of_headings = globals().get('list_of_identified_heading_strings', [])\n",
    "\n",
    "if _enable_heading_style and _list_of_headings and stt_words_normalized_with_indices:\n",
    "    print(f\"Aligning {len(_list_of_headings)} identified headings with STT word timestamps...\")\n",
    "    stt_normalized_word_list = [item[0] for item in stt_words_normalized_with_indices]\n",
    "\n",
    "    for heading_text_raw in _list_of_headings:\n",
    "        normalized_lm_heading = normalize_text_for_matching(heading_text_raw)\n",
    "        if not normalized_lm_heading: continue\n",
    "        lm_heading_words = normalized_lm_heading.split()\n",
    "        if not lm_heading_words: continue\n",
    "\n",
    "        matcher = difflib.SequenceMatcher(None, lm_heading_words, stt_normalized_word_list, autojunk=False)\n",
    "        best_match_info = None # Renamed from best_match to avoid conflict with loop var\n",
    "        highest_ratio_for_this_heading = 0.0\n",
    "        min_similarity_ratio = 0.7\n",
    "        min_match_length_words = max(1, len(lm_heading_words) // 2)\n",
    "\n",
    "        for match_block in matcher.get_matching_blocks(): # Renamed loop var\n",
    "            if match_block.size < min_match_length_words: continue\n",
    "            \n",
    "            start_stt_original_idx = stt_words_normalized_with_indices[match_block.b][1]\n",
    "            end_stt_original_idx = stt_words_normalized_with_indices[match_block.b + match_block.size - 1][1]\n",
    "            current_indices_range = set(range(start_stt_original_idx, end_stt_original_idx + 1))\n",
    "            \n",
    "            if current_indices_range.intersection(stt_word_indices_used_by_headings): continue\n",
    "\n",
    "            current_match_ratio = match_block.size / len(lm_heading_words)\n",
    "            if current_match_ratio >= min_similarity_ratio and current_match_ratio > highest_ratio_for_this_heading:\n",
    "                best_match_info = {\n",
    "                    \"text\": heading_text_raw,\n",
    "                    \"start_s\": _all_word_data[start_stt_original_idx][\"start\"],\n",
    "                    \"end_s\": _all_word_data[end_stt_original_idx][\"end\"],\n",
    "                    \"stt_word_indices\": list(current_indices_range), # Store original indices\n",
    "                    \"match_ratio\": current_match_ratio,\n",
    "                }\n",
    "                highest_ratio_for_this_heading = current_match_ratio\n",
    "        \n",
    "        if best_match_info:\n",
    "            current_best_match_indices_set = set(best_match_info[\"stt_word_indices\"])\n",
    "            if not current_best_match_indices_set.intersection(stt_word_indices_used_by_headings):\n",
    "                aligned_headings_data.append(best_match_info)\n",
    "                stt_word_indices_used_by_headings.update(current_best_match_indices_set)\n",
    "                print(f\"  Aligned heading (ratio {best_match_info['match_ratio']:.2f}): \\\"{best_match_info['text'][:50]}...\\\"\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Best match for \\\"{heading_text_raw[:50]}...\\\" overlaps. Skipping.\")\n",
    "        else:\n",
    "            print(f\"  WARNING: Could not align heading (or low similarity): \\\"{heading_text_raw[:50]}...\\\"\")\n",
    "    if aligned_headings_data:\n",
    "        aligned_headings_data.sort(key=lambda x: x[\"start_s\"])\n",
    "    print(f\"Alignment complete. {len(aligned_headings_data)} headings successfully aligned.\")\n",
    "else:\n",
    "    if _enable_heading_style: print(\"Skipping heading alignment: Heading style enabled but no headings or STT data found.\")\n",
    "\n",
    "# --- 4. Generate ASS Dialogue Lines (Applying offset) ---\n",
    "dialogue_lines_ass = []\n",
    "current_default_line_word_objects = []\n",
    "current_default_line_start_time_unoffset = -1.0 # Unoffseted start time for current line logic\n",
    "\n",
    "# Add aligned headings with offset\n",
    "if _enable_heading_style and aligned_headings_data:\n",
    "    for heading_data in aligned_headings_data:\n",
    "        heading_text_escaped = escape_ass_text(heading_data[\"text\"])\n",
    "        offsetted_start = heading_data['start_s'] + subtitle_time_offset_s\n",
    "        offsetted_end = heading_data['end_s'] + subtitle_time_offset_s\n",
    "        dialogue_entry = (f\"Dialogue: 0,{format_ass_time(offsetted_start)},\"\n",
    "                          f\"{format_ass_time(offsetted_end)},HeadingStyle,,0,0,0,,{heading_text_escaped}\")\n",
    "        dialogue_lines_ass.append(dialogue_entry)\n",
    "\n",
    "# Generate default subtitle lines with offset\n",
    "if _all_word_data:\n",
    "    print(f\"Generating default style subtitle lines (offset: {subtitle_time_offset_s:.2f}s)...\")\n",
    "    last_default_word_data_unoffset = None # Tracks the end of the last word in the previous line (unoffseted)\n",
    "\n",
    "    # Configs for line breaking\n",
    "    _gap_threshold = globals().get('GAP_THRESHOLD_SEC_ASS', 0.4)\n",
    "    _max_words_per_line = globals().get('MAX_WORDS_PER_LINE_ASS', 7)\n",
    "    _max_line_duration = globals().get('MAX_LINE_DURATION_SEC_ASS', 12.0)\n",
    "    _ass_fade_ms = globals().get('ASS_FADE_EFFECT_MS', 0)\n",
    "\n",
    "    for i, word_data in enumerate(_all_word_data):\n",
    "        # Skip words already used in headings\n",
    "        if _enable_heading_style and i in stt_word_indices_used_by_headings:\n",
    "            if current_default_line_word_objects: # Finalize any pending line\n",
    "                start_final = current_default_line_start_time_unoffset + subtitle_time_offset_s\n",
    "                end_final = current_default_line_word_objects[-1][\"end\"] + subtitle_time_offset_s\n",
    "                text_final = \" \".join([escape_ass_text(wd[\"text\"]) for wd in current_default_line_word_objects])\n",
    "                tags = f\"{{\\\\fad({_ass_fade_ms},{_ass_fade_ms})}}\" if _ass_fade_ms > 0 else \"\"\n",
    "                dialogue_lines_ass.append(f\"Dialogue: 0,{format_ass_time(start_final)},{format_ass_time(end_final)},Default,,0,0,0,,{tags}{text_final}\")\n",
    "                current_default_line_word_objects = []\n",
    "                current_default_line_start_time_unoffset = -1.0\n",
    "            last_default_word_data_unoffset = None # Reset after a heading\n",
    "            continue\n",
    "\n",
    "        if not word_data.get(\"text\",\"\").strip(): continue # Skip empty words\n",
    "\n",
    "        start_new_line_flag = False\n",
    "        if not current_default_line_word_objects: # First word of a new line\n",
    "            start_new_line_flag = True\n",
    "        else:\n",
    "            # Line breaking logic (using unoffseted times for relative checks)\n",
    "            gap = word_data[\"start\"] - (last_default_word_data_unoffset[\"end\"] if last_default_word_data_unoffset else word_data[\"start\"])\n",
    "            line_len_words = len(current_default_line_word_objects)\n",
    "            line_duration_unoffset = word_data[\"end\"] - current_default_line_start_time_unoffset\n",
    "\n",
    "            if gap > _gap_threshold: start_new_line_flag = True\n",
    "            elif line_len_words >= _max_words_per_line: start_new_line_flag = True\n",
    "            elif line_duration_unoffset > _max_line_duration: start_new_line_flag = True\n",
    "        \n",
    "        if start_new_line_flag and current_default_line_word_objects: # Finalize current line\n",
    "            start_final = current_default_line_start_time_unoffset + subtitle_time_offset_s\n",
    "            end_final = current_default_line_word_objects[-1][\"end\"] + subtitle_time_offset_s\n",
    "            text_final = \" \".join([escape_ass_text(wd[\"text\"]) for wd in current_default_line_word_objects])\n",
    "            tags = f\"{{\\\\fad({_ass_fade_ms},{_ass_fade_ms})}}\" if _ass_fade_ms > 0 else \"\"\n",
    "            dialogue_lines_ass.append(f\"Dialogue: 0,{format_ass_time(start_final)},{format_ass_time(end_final)},Default,,0,0,0,,{tags}{text_final}\")\n",
    "            current_default_line_word_objects = []\n",
    "        \n",
    "        if not current_default_line_word_objects: # Start a new line\n",
    "            current_default_line_start_time_unoffset = word_data[\"start\"]\n",
    "        \n",
    "        current_default_line_word_objects.append(word_data)\n",
    "        last_default_word_data_unoffset = word_data # This word is now the last one processed\n",
    "\n",
    "    if current_default_line_word_objects: # Add the very last line\n",
    "        start_final = current_default_line_start_time_unoffset + subtitle_time_offset_s\n",
    "        end_final = current_default_line_word_objects[-1][\"end\"] + subtitle_time_offset_s\n",
    "        text_final = \" \".join([escape_ass_text(wd[\"text\"]) for wd in current_default_line_word_objects])\n",
    "        tags = f\"{{\\\\fad({_ass_fade_ms},{_ass_fade_ms})}}\" if _ass_fade_ms > 0 else \"\"\n",
    "        dialogue_lines_ass.append(f\"Dialogue: 0,{format_ass_time(start_final)},{format_ass_time(end_final)},Default,,0,0,0,,{tags}{text_final}\")\n",
    "\n",
    "    # Sort all dialogue lines (headings + default) by their final (offsetted) start times\n",
    "    try:\n",
    "        dialogue_lines_ass.sort(key=lambda line: datetime.datetime.strptime(line.split(',')[1], \"%H:%M:%S.%f\"))\n",
    "    except ValueError as e_sort:\n",
    "        print(f\"Warning: Could not sort dialogue lines, possibly due to time format issue: {e_sort}\")\n",
    "        print(\"  Ensure format_ass_time output is consistent with %H:%M:%S.%f (e.g., 0:00:01.23)\")\n",
    "\n",
    "\n",
    "    print(f\"Generated {len(dialogue_lines_ass)} total ASS dialogue lines.\")\n",
    "else:\n",
    "    print(\"WARNING: No word-level STT data ('all_word_level_data_for_ass'). ASS file will be empty of dialogues.\")\n",
    "\n",
    "# --- 5. Assemble Final ASS Content and Write to File ---\n",
    "_ass_subtitle_path_final = globals().get('ASS_SUBTITLE_PATH', '/kaggle/working/subtitles.ass')\n",
    "ass_full_content = ass_header_content + \"\\n\".join(dialogue_lines_ass)\n",
    "print(f\"\\nWriting ASS subtitle content to: {_ass_subtitle_path_final}...\")\n",
    "try:\n",
    "    with open(_ass_subtitle_path_final, 'w', encoding='utf-8') as f:\n",
    "        f.write(ass_full_content)\n",
    "    print(f\"ASS file written successfully to {_ass_subtitle_path_final}\")\n",
    "except Exception as e_write_ass:\n",
    "    print(f\"ERROR: Failed to write ASS file: {e_write_ass}\")\n",
    "\n",
    "# Verification\n",
    "if os.path.exists(_ass_subtitle_path_final):\n",
    "    if len(dialogue_lines_ass) > 0 :\n",
    "        print(f\"SUCCESS: ASS subtitle file created at: {_ass_subtitle_path_final}\")\n",
    "        print(f\"  File size: {os.path.getsize(_ass_subtitle_path_final)} bytes. Contains {len(dialogue_lines_ass)} dialogue lines.\")\n",
    "    elif not _all_word_data: # No STT data, so empty dialogue section is expected\n",
    "        print(f\"INFO: ASS file created (header only) at {_ass_subtitle_path_final} as no STT data was available.\")\n",
    "    else: # STT data was there, but no lines generated (shouldn't happen with this logic if STT data is valid)\n",
    "        print(f\"WARNING: ASS file created at {_ass_subtitle_path_final}, but it's empty of dialogues despite STT data. Check logic.\")\n",
    "else:\n",
    "    print(f\"ERROR: ASS subtitle file was NOT created at {_ass_subtitle_path_final}.\")\n",
    "\n",
    "print(\"#############################################################################################\")\n",
    "print(\"Phase 6c (ASS Generation): Complete.\")\n",
    "print(\"#############################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10B: Generate YouTube Chapter Timestamps from Aligned Headings\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# This cell should run AFTER Cell 10 (ASS Generation) has successfully\n",
    "# populated 'aligned_headings_data'.\n",
    "\n",
    "# Global variable needed:\n",
    "# 'aligned_headings_data' (output from Cell 10's alignment step)\n",
    "# Optional: VIDEO_FPS (from Cell 2, though not strictly needed for time conversion here)\n",
    "\n",
    "# Imports (datetime if more complex time formatting needed, but simple math is fine)\n",
    "\n",
    "print(\"Generating YouTube Chapter Timestamps...\")\n",
    "\n",
    "youtube_chapters_output = []\n",
    "\n",
    "if 'aligned_headings_data' in globals() and aligned_headings_data:\n",
    "    \n",
    "    # Helper function to format seconds into HH:MM:SS or MM:SS\n",
    "    def format_youtube_timestamp(seconds_float):\n",
    "        seconds_int = int(round(seconds_float)) # Round to nearest second for chapters\n",
    "        \n",
    "        hours = seconds_int // 3600\n",
    "        minutes = (seconds_int % 3600) // 60\n",
    "        seconds = seconds_int % 60\n",
    "        \n",
    "        if hours > 0:\n",
    "            return f\"{hours:01d}:{minutes:02d}:{seconds:02d}\" # HH:MM:SS\n",
    "        else:\n",
    "            return f\"{minutes:01d}:{seconds:02d}\" # MM:SS (or M:SS if minutes < 10)\n",
    "\n",
    "    # Check if the first heading starts significantly after 0:00. If so, add an \"Intro\".\n",
    "    # Threshold for adding an \"Intro\" chapter, e.g., if first heading is after 5 seconds.\n",
    "    intro_threshold_s = 5.0 \n",
    "    if not aligned_headings_data[0][\"start_s\"] <= intro_threshold_s :\n",
    "         # Or if you always want an intro:\n",
    "         # if True: \n",
    "        youtube_chapters_output.append(\"0:00 Intro\")\n",
    "        print(\"  Added '0:00 Intro' as the first chapter.\")\n",
    "    elif aligned_headings_data[0][\"start_s\"] > 0: # First heading doesn't start at 0:00 exactly\n",
    "        # Option: Make the first heading start at 0:00 if it's very close, or keep its actual time.\n",
    "        # For simplicity, we will use its actual time.\n",
    "        # If you wanted the first heading to *be* the 0:00 mark:\n",
    "        # first_heading_data = aligned_headings_data[0]\n",
    "        # chapter_text = escape_ass_text(first_heading_data[\"text\"]) # Reuse escape for consistency\n",
    "        # youtube_chapters_output.append(f\"0:00 {chapter_text}\")\n",
    "        # start_index_for_loop = 1\n",
    "        pass # Let the loop handle the first actual heading\n",
    "\n",
    "\n",
    "    for heading_data in aligned_headings_data:\n",
    "        start_seconds = heading_data[\"start_s\"]\n",
    "        # Ensure the first chapter is at 0:00 if it's the very first item and no intro was added\n",
    "        if not youtube_chapters_output and start_seconds > 0.5 : # If list is empty and first heading not at 0\n",
    "             youtube_chapters_output.append(\"0:00 Start\") # Or use the first heading text at 0:00\n",
    "        elif not youtube_chapters_output and start_seconds <= 0.5: # First heading is at 0:00\n",
    "             start_seconds = 0 # Force to 0:00\n",
    "\n",
    "        formatted_time = format_youtube_timestamp(start_seconds)\n",
    "        \n",
    "        # Use the original heading text (before ASS escaping, but clean it for description)\n",
    "        # Basic cleaning for YouTube description (remove excessive newlines, etc.)\n",
    "        heading_display_text = heading_data[\"text\"].replace('\\n', ' ').strip()\n",
    "        heading_display_text = re.sub(r'\\s+', ' ', heading_display_text) # Normalize whitespace\n",
    "\n",
    "        # Avoid duplicate timestamps if multiple headings start at almost the same time (unlikely with STT)\n",
    "        # This check is simplified; a more robust check would compare 'formatted_time'\n",
    "        is_duplicate_time = False\n",
    "        if youtube_chapters_output:\n",
    "            last_chapter_time = youtube_chapters_output[-1].split(' ')[0]\n",
    "            if last_chapter_time == formatted_time:\n",
    "                is_duplicate_time = True\n",
    "                print(f\"  INFO: Skipping heading '{heading_display_text[:30]}...' due to duplicate timestamp '{formatted_time}' with previous chapter.\")\n",
    "\n",
    "        if not is_duplicate_time:\n",
    "            youtube_chapters_output.append(f\"{formatted_time} {heading_display_text}\")\n",
    "\n",
    "    print(\"\\nSuccessfully generated YouTube Chapter List:\")\n",
    "    for chapter_line in youtube_chapters_output:\n",
    "        print(chapter_line)\n",
    "    \n",
    "    # Optional: Save to a file\n",
    "    chapters_filepath = os.path.join(KAGGLE_WORKING_DIR, \"youtube_chapters.txt\")\n",
    "    try:\n",
    "        with open(chapters_filepath, 'w', encoding='utf-8') as f:\n",
    "            for line in youtube_chapters_output:\n",
    "                f.write(line + \"\\n\")\n",
    "        print(f\"\\nYouTube chapters saved to: {chapters_filepath}\")\n",
    "        # from IPython.display import FileLink, display # Import if not done globally\n",
    "        # display(FileLink(chapters_filepath))\n",
    "    except Exception as e_save_chapters:\n",
    "        print(f\"ERROR saving YouTube chapters file: {e_save_chapters}\")\n",
    "\n",
    "else:\n",
    "    print(\"No aligned heading data found. Cannot generate YouTube chapters.\")\n",
    "\n",
    "print(\"\\n--------------------------------------------------------------------\")\n",
    "print(\"YouTube Chapter Timestamps Generation Complete.\")\n",
    "print(\"--------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6N: Phase 4 - Language Model (Gemini API) - Image Prompt Generation\n",
    "# ---------------------------------------------------------------------------------\n",
    "# This cell uses the Google Gemini API to:\n",
    "# 1. Generate a global summary of the entire script.\n",
    "# 2. Generate a visual prompt for an image AI for each text chunk,\n",
    "#    using the global summary, a creative brief, and the chunk's text.\n",
    "# It respects the configured API rate limits.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time # Ensure time is imported for rate limiting\n",
    "import traceback\n",
    "\n",
    "# Attempt to import Gemini library, assuming it's installed from Cell 1\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except ImportError:\n",
    "    print(\"ERROR: The 'google-generativeai' library is not installed. Please ensure it's in Cell 1's pip install list.\")\n",
    "    genai = None # Set to None so checks below will fail gracefully\n",
    "\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Phase 4 (LM - Gemini API): Starting Image Prompt Generation...\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "# --- 0. DEBUG: Clear/Re-initialize output list ---\n",
    "print(\"DEBUG: Clearing/Re-initializing LM output list 'lm_generated_prompts_data' for this run.\")\n",
    "lm_generated_prompts_data = []\n",
    "global_script_summary_for_lm = \"No global summary generated (LM step skipped or failed).\" # Default fallback\n",
    "\n",
    "# --- 1. Initialize Gemini Client and Retrieve Configurations ---\n",
    "gemini_model_obj = None # Initialize to None for robust error handling\n",
    "\n",
    "# Retrieve configurations from Cell 2\n",
    "# These variables are expected to be defined in Cell 2 from our Phase 1 plan\n",
    "USE_GEMINI_API_FOR_LM = globals().get('USE_GEMINI_API_FOR_LM', False)\n",
    "GEMINI_API_KEY = globals().get('GEMINI_API_KEY', \"YOUR_GOOGLE_AI_STUDIO_API_KEY_NOT_SET\")\n",
    "GEMINI_MODEL_NAME = globals().get('GEMINI_MODEL_NAME', \"models/gemini-2.0-flash-lite\") # Default if not set\n",
    "LLM_API_RPM_LIMIT = globals().get('LLM_API_RPM_LIMIT', 30) # Default if not set\n",
    "# Calculate delay, ensure it's at least a small positive number if RPM is very high or zero\n",
    "LLM_API_REQUEST_DELAY_S = 60.0 / LLM_API_RPM_LIMIT if LLM_API_RPM_LIMIT > 0 else 2.0 \n",
    "\n",
    "# Other needed globals from Cell 2\n",
    "TEXT_TO_SPEAK = globals().get('TEXT_TO_SPEAK', \"\")\n",
    "CREATIVE_BRIEF_FOR_LM = globals().get('CREATIVE_BRIEF_FOR_LM', \"Default creative brief: Visually stunning, cinematic.\")\n",
    "\n",
    "if not genai:\n",
    "    print(\"Critical ERROR: 'google-generativeai' library not available. Cannot proceed with Gemini API.\")\n",
    "elif not USE_GEMINI_API_FOR_LM:\n",
    "    print(\"INFO: USE_GEMINI_API_FOR_LM is False in Cell 2. Skipping Gemini API setup and LM processing.\")\n",
    "elif not GEMINI_API_KEY or GEMINI_API_KEY == \"YOUR_GOOGLE_AI_STUDIO_API_KEY_NOT_SET\" or GEMINI_API_KEY == \"YOUR_GOOGLE_AI_STUDIO_API_KEY\": # Catch common placeholders\n",
    "    print(\"ERROR: GEMINI_API_KEY is not set or is still a placeholder in Cell 2. Please update it.\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"Configuring Gemini client with API key for model: {GEMINI_MODEL_NAME}...\")\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        \n",
    "        # Define safety settings (optional, adjust as needed)\n",
    "        # Example: More permissive, blocks only HIGH harm categories. Adjust based on API behavior.\n",
    "        safety_settings_config = [\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "        ]\n",
    "        # If you prefer stricter defaults, you can omit safety_settings or use \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "\n",
    "        gemini_model_obj = genai.GenerativeModel(\n",
    "            GEMINI_MODEL_NAME,\n",
    "            safety_settings=safety_settings_config\n",
    "        )\n",
    "        print(f\"Gemini client configured successfully for model: {GEMINI_MODEL_NAME}.\")\n",
    "        print(f\"  Rate limit: {LLM_API_RPM_LIMIT} RPM. Min delay between requests: {LLM_API_REQUEST_DELAY_S:.2f} seconds.\")\n",
    "\n",
    "    except Exception as e_gemini_setup:\n",
    "        print(f\"ERROR: Failed to configure Gemini client. Error: {e_gemini_setup}\")\n",
    "        traceback.print_exc()\n",
    "        gemini_model_obj = None # Ensure it's None on failure\n",
    "\n",
    "# --- 2. Phase 4a: Generate Global Script Summary using the LM ---\n",
    "if gemini_model_obj and TEXT_TO_SPEAK and TEXT_TO_SPEAK.strip():\n",
    "    print(f\"\\nPhase 4a: Generating global script summary using Gemini ({GEMINI_MODEL_NAME})...\")\n",
    "    \n",
    "    summarization_prompt_text = f\"\"\"\n",
    "Instruction: Read the following 'Full Script'.\n",
    "Your task is to create a very concise summary (around 100-150 words, or 3-5 key sentences) that captures the main themes, overall narrative arc, and any recurring visual motifs or moods.\n",
    "This summary will be used as global context to guide image generation for different segments of the script.\n",
    "Focus on elements that would be visually translatable or help maintain thematic consistency in a series of images.\n",
    "Do not analyze or critique the script; just provide a factual, condensed summary of its core visual and thematic essence.\n",
    "\n",
    "Full Script:\n",
    "---\n",
    "{TEXT_TO_SPEAK}\n",
    "---\n",
    "\n",
    "Concise Visual & Thematic Summary (100-150 words):\n",
    "\"\"\"\n",
    "    try:\n",
    "        summary_generation_config = genai.types.GenerationConfig(\n",
    "            max_output_tokens=250, # Max tokens for the summary output\n",
    "            temperature=0.3,     # Lower temperature for more factual summary\n",
    "        )\n",
    "\n",
    "        print(f\"  Waiting {LLM_API_REQUEST_DELAY_S:.2f}s before API call (for global summary)...\")\n",
    "        time.sleep(LLM_API_REQUEST_DELAY_S)\n",
    "\n",
    "        response_summary = gemini_model_obj.generate_content(\n",
    "            summarization_prompt_text,\n",
    "            generation_config=summary_generation_config,\n",
    "            request_options={\"timeout\": 180} # Increased timeout for potentially long summary task\n",
    "        )\n",
    "        \n",
    "        if response_summary.prompt_feedback and response_summary.prompt_feedback.block_reason:\n",
    "            print(f\"  WARNING: Global summary request was blocked. Reason: {response_summary.prompt_feedback.block_reason}\")\n",
    "            global_script_summary_for_lm = \"Global summary generation blocked. Using fallback creative brief for context.\"\n",
    "        elif not response_summary.candidates or not hasattr(response_summary.candidates[0], 'content') or not response_summary.candidates[0].content.parts:\n",
    "            print(\"  WARNING: Gemini returned no content or malformed response for global summary.\")\n",
    "            global_script_summary_for_lm = \"Global summary generation resulted in empty content. Using fallback.\"\n",
    "        else:\n",
    "            decoded_summary = response_summary.text\n",
    "            if decoded_summary and decoded_summary.strip():\n",
    "                global_script_summary_for_lm = decoded_summary.strip()\n",
    "                # Clean up potential leading phrases that echo the prompt\n",
    "                if global_script_summary_for_lm.lower().startswith(\"concise visual & thematic summary\"):\n",
    "                    # Find the first actual content after the echoed part\n",
    "                    match = re.search(r\"(?:concise visual & thematic summary(?: \\(100-150 words\\))?:)(.*)\", global_script_summary_for_lm, re.IGNORECASE | re.DOTALL)\n",
    "                    if match and match.group(1).strip():\n",
    "                        global_script_summary_for_lm = match.group(1).strip()\n",
    "                print(f\"  Successfully generated global script summary:\\n---\\n{global_script_summary_for_lm}\\n---\")\n",
    "            else:\n",
    "                print(\"  WARNING: LM returned an empty summary string. Using fallback.\")\n",
    "                global_script_summary_for_lm = \"Key themes from script: [visualize core ideas].\" # Generic fallback\n",
    "\n",
    "    except Exception as e_summarize_gemini:\n",
    "        print(f\"  ERROR during global script summarization with Gemini: {e_summarize_gemini}\")\n",
    "        traceback.print_exc()\n",
    "        global_script_summary_for_lm = \"Error generating summary. Focus on individual chunk and creative brief.\"\n",
    "elif gemini_model_obj and (not TEXT_TO_SPEAK or not TEXT_TO_SPEAK.strip()):\n",
    "    print(\"INFO: TEXT_TO_SPEAK is empty. Skipping global script summary generation.\")\n",
    "    global_script_summary_for_lm = \"Script was empty. General visual concepts.\"\n",
    "# If gemini_model_obj is None, the earlier message about skipping LM processing applies.\n",
    "\n",
    "# --- 3. Phase 4b: Loop Through Pacing Chunks and Generate Image Prompts ---\n",
    "# 'chunk_text_for_lm_input' should be available from Cell 5 (Phase 3b - STT)\n",
    "if gemini_model_obj and 'chunk_text_for_lm_input' in locals() and chunk_text_for_lm_input:\n",
    "    print(f\"\\nPhase 4b: Generating image prompts for {len(chunk_text_for_lm_input)} text segments using Gemini and global summary...\")\n",
    "\n",
    "    for i, item_data in enumerate(chunk_text_for_lm_input):\n",
    "        raw_chunk_text = item_data.get(\"raw_text\", \"\")\n",
    "        duration_ms = item_data.get(\"duration_ms\", 0)\n",
    "        original_chunk_idx = item_data.get(\"original_chunk_index\", i)\n",
    "\n",
    "        print(f\"  Processing segment {i+1}/{len(chunk_text_for_lm_input)} (Orig. Chunk Idx: {original_chunk_idx}) for LM prompt...\")\n",
    "        # print(f\"    Raw text for this segment: \\\"{raw_chunk_text[:70].replace(chr(10), ' ')}...\\\"\") # Already printed by STT cell\n",
    "\n",
    "        if not raw_chunk_text.strip() or \"[STT_\" in raw_chunk_text or \"[WHISPER_\" in raw_chunk_text:\n",
    "            print(\"    Skipping LM prompt generation due to STT failure or empty text for this chunk.\")\n",
    "            lm_generated_prompts_data.append({\n",
    "                \"image_prompt\": \"placeholder image, empty scene, neutral background, error in source text for LM\",\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"original_chunk_index\": original_chunk_idx,\n",
    "                \"raw_chunk_text_for_reference\": raw_chunk_text\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        lm_input_prompt_for_chunk = f\"\"\"\n",
    "You are an expert visual concept artist. Your task is to generate a concise visual prompt for a text-to-image AI, based on a 'Current Text Segment'.\n",
    "Use the 'Global Script Summary' for thematic consistency and strictly adhere to the 'Creative Brief' for style.\n",
    "\n",
    "**INPUTS:**\n",
    "\n",
    "1.  **Global Script Summary (Overall Theme):**\n",
    "    ---\n",
    "    {global_script_summary_for_lm}\n",
    "    ---\n",
    "\n",
    "2.  **Creative Brief (Artistic Style, Mood, Constraints - STRICTLY ADHERE):**\n",
    "    ---\n",
    "    {CREATIVE_BRIEF_FOR_LM}\n",
    "    ---\n",
    "\n",
    "3.  **Current Text Segment (PRIMARY FOCUS for visual content):**\n",
    "    ---\n",
    "    {raw_chunk_text}\n",
    "    ---\n",
    "\n",
    "**INSTRUCTIONS FOR THE VISUAL PROMPT:**\n",
    "A. **Interpret & Abstract:** Understand the 'Current Text Segment's' core meaning/emotion.\n",
    "B. **Visually Descriptive:** Use strong nouns, evocative adjectives, mood, lighting, composition.\n",
    "C. **Adhere to Brief & Summary:** Ensure style aligns with 'Creative Brief' and theme with 'Global Script Summary'.\n",
    "D. **Concise & Original:** Aim for 30-70 words for the visual prompt. Must be your original visual interpretation based on all inputs.\n",
    "E **DO NOT** repeat inputs or add explanations. Output only the prompt itself.\n",
    "\n",
    "**Generated Visual Prompt:**\n",
    "\"\"\"\n",
    "        generated_image_prompt = \"Error in LM, abstract design, vibrant patterns, based on text segment.\" # Default fallback\n",
    "        try:\n",
    "            image_prompt_generation_config = genai.types.GenerationConfig(\n",
    "                max_output_tokens=120,  # Max tokens for the image prompt output (approx 50-80 words)\n",
    "                temperature=0.7,      # Creative temperature\n",
    "                # top_p=0.9,          # Optional\n",
    "            )\n",
    "\n",
    "            print(f\"    Waiting {LLM_API_REQUEST_DELAY_S:.2f}s before API call (for chunk {i+1} image prompt)...\")\n",
    "            time.sleep(LLM_API_REQUEST_DELAY_S) # Apply delay BEFORE each call in the loop\n",
    "\n",
    "            response_chunk_prompt = gemini_model_obj.generate_content(\n",
    "                lm_input_prompt_for_chunk,\n",
    "                generation_config=image_prompt_generation_config,\n",
    "                request_options={\"timeout\": 120} # Timeout for individual prompt generation\n",
    "            )\n",
    "\n",
    "            if response_chunk_prompt.prompt_feedback and response_chunk_prompt.prompt_feedback.block_reason:\n",
    "                print(f\"    WARNING: Image prompt request for chunk {i+1} was blocked. Reason: {response_chunk_prompt.prompt_feedback.block_reason}\")\n",
    "                generated_image_prompt = f\"Visual prompt generation blocked. Abstract scene for: {raw_chunk_text[:50]}\"\n",
    "            elif not response_chunk_prompt.candidates or not hasattr(response_chunk_prompt.candidates[0], 'content') or not response_chunk_prompt.candidates[0].content.parts:\n",
    "                print(f\"    WARNING: Gemini returned no content or malformed response for image prompt (chunk {i+1}).\")\n",
    "                generated_image_prompt = f\"No prompt generated. Visual interpretation of: {raw_chunk_text[:50]}\"\n",
    "            else:\n",
    "                decoded_image_prompt = response_chunk_prompt.text\n",
    "                if decoded_image_prompt and decoded_image_prompt.strip():\n",
    "                    generated_image_prompt = decoded_image_prompt.strip()\n",
    "                    # Clean up potential leading phrases that echo the prompt\n",
    "                    if generated_image_prompt.lower().startswith(\"generated visual prompt:\"):\n",
    "                        generated_image_prompt = generated_image_prompt.split(\":\", 1)[-1].strip()\n",
    "                    if not generated_image_prompt: # If cleaning results in empty\n",
    "                        generated_image_prompt = f\"cinematic scene based on: {raw_chunk_text[:70]}\" # Fallback if cleaning empties it\n",
    "                    print(f\"    LM Generated SD Prompt: \\\"{generated_image_prompt}\\\"\")\n",
    "                else:\n",
    "                    print(f\"    WARNING: Gemini returned an empty image prompt string for chunk {i+1}. Using fallback.\")\n",
    "                    generated_image_prompt = f\"Detailed visual scene inspired by: {raw_chunk_text[:70]}\"\n",
    "        \n",
    "        except Exception as e_lm_gen_chunk_gemini:\n",
    "            print(f\"    ERROR during Gemini prompt generation for segment {i+1}: {e_lm_gen_chunk_gemini}\")\n",
    "            traceback.print_exc()\n",
    "            # Fallback 'generated_image_prompt' defined above will be used.\n",
    "        \n",
    "        lm_generated_prompts_data.append({\n",
    "            \"image_prompt\": generated_image_prompt,\n",
    "            \"duration_ms\": duration_ms,\n",
    "            \"original_chunk_index\": original_chunk_idx,\n",
    "            \"raw_chunk_text_for_reference\": raw_chunk_text\n",
    "        })\n",
    "        \n",
    "elif gemini_model_obj and (not 'chunk_text_for_lm_input' in locals() or not chunk_text_for_lm_input):\n",
    "    print(\"INFO: 'chunk_text_for_lm_input' is missing or empty. Skipping per-chunk prompt generation.\")\n",
    "# If gemini_model_obj is None, the earlier message about skipping LM processing applies.\n",
    "\n",
    "# --- 4. LM Processing Cleanup ---\n",
    "print(\"\\n--- LM (Gemini API) Processing Cleanup ---\")\n",
    "# The gemini_model_obj is a client instance and doesn't hold GPU VRAM like local PyTorch models.\n",
    "# Python's garbage collection will handle the client object when it's no longer referenced\n",
    "# or when the script/notebook session ends.\n",
    "gc.collect()\n",
    "print(\"Language Model (Gemini API) processing has concluded. Garbage collection called.\")\n",
    "\n",
    "# --- 5. Verify Output ---\n",
    "if lm_generated_prompts_data:\n",
    "    print(f\"\\nSUCCESS: Attempted to generate {len(lm_generated_prompts_data)} image prompts using the Gemini API.\")\n",
    "    print(f\"  Global script summary used as context:\\n---\\n{global_script_summary_for_lm}\\n---\")\n",
    "    if len(lm_generated_prompts_data) > 0 :\n",
    "        print(f\"  Sample LM-generated prompt (first item's prompt): \\\"{lm_generated_prompts_data[0]['image_prompt']}\\\"\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No image prompts were generated or added to 'lm_generated_prompts_data'.\")\n",
    "    if not gemini_model_obj:\n",
    "        print(\"  This was likely because the Gemini client could not be initialized (check API key and setup).\")\n",
    "    elif not ('chunk_text_for_lm_input' in locals() and chunk_text_for_lm_input):\n",
    "        print(\"  This was likely because no input text chunks were available from STT phase.\")\n",
    "\n",
    "print(\"\\n-------------------------------------------------------------------------------\")\n",
    "print(\"Phase 4 (LM - Gemini API): Image Prompt Generation Complete.\")\n",
    "print(\"-------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Phase 5 - Image Generation with Gemini API (Major Overhaul)\n",
    "# --------------------------------------------------------------------\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import traceback\n",
    "from io import BytesIO # For handling image data from API\n",
    "\n",
    "# Imports from Cell 1 (ensure they are loaded):\n",
    "# import google.generativeai as genai\n",
    "# from google.generativeai import types as genai_types\n",
    "# from PIL import Image as PILImageMod # Or just from PIL import Image\n",
    "\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Phase 5 (Image Generation with Gemini API): Starting Image Generation...\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "# --- 0. DEBUG: Clear/Re-initialize output list ---\n",
    "print(\"DEBUG: Clearing/Re-initializing 'image_generation_results_with_paths' for this run.\")\n",
    "image_generation_results_with_paths = []\n",
    "\n",
    "# --- 1. Retrieve Configurations from Cell 2 ---\n",
    "USE_GEMINI_FOR_IMAGE_GENERATION = globals().get('USE_GEMINI_FOR_IMAGE_GENERATION', False)\n",
    "GEMINI_API_KEY = globals().get('GEMINI_API_KEY') # Single API key for all Gemini services\n",
    "lm_generated_prompts_data = globals().get('lm_generated_prompts_data', []) # From Cell 6N (LM prompts)\n",
    "\n",
    "if USE_GEMINI_FOR_IMAGE_GENERATION:\n",
    "    GEMINI_IMAGE_GEN_MODEL_NAME = globals().get('GEMINI_IMAGE_GEN_MODEL_NAME')\n",
    "    GEMINI_IMAGE_GEN_REQUEST_DELAY_S = globals().get('GEMINI_IMAGE_GEN_REQUEST_DELAY_S', 6.0) # Default 6s if not set\n",
    "    GEMINI_IMAGE_GEN_API_TIMEOUT_S = globals().get('GEMINI_IMAGE_GEN_API_TIMEOUT_S', 180)\n",
    "    TARGET_IMAGE_WIDTH = globals().get('TARGET_IMAGE_WIDTH')\n",
    "    TARGET_IMAGE_HEIGHT = globals().get('TARGET_IMAGE_HEIGHT')\n",
    "    IMAGE_OUTPUT_DIR = globals().get('IMAGE_OUTPUT_DIR')\n",
    "    GEMINI_PROMPT_UNIVERSAL_SUFFIX = globals().get('GEMINI_PROMPT_UNIVERSAL_SUFFIX', \"\")\n",
    "    NEGATIVE_PROMPT_TERMS = globals().get('NEGATIVE_PROMPT_TERMS', \"\")\n",
    "    # Ensure essential configs are present\n",
    "    if not all([GEMINI_API_KEY, GEMINI_IMAGE_GEN_MODEL_NAME, TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT, IMAGE_OUTPUT_DIR]):\n",
    "        print(\"ERROR: Critical Gemini Image Generation configurations (API Key, Model Name, Target Dims, Output Dir) are missing from Cell 2.\")\n",
    "        USE_GEMINI_FOR_IMAGE_GENERATION = False # Disable to prevent further errors\n",
    "\n",
    "# --- Helper Function for Image Resizing/Scaling (adapted from original Cell 7 / Phase A) ---\n",
    "def resize_and_prepare_gemini_image(pil_img_from_api, target_width, target_height):\n",
    "    \"\"\"\n",
    "    Resizes PIL image from Gemini to fill target_width x target_height,\n",
    "    cropping if necessary, maintaining aspect ratio. Converts to RGB.\n",
    "    \"\"\"\n",
    "    if pil_img_from_api.mode != 'RGB':\n",
    "        pil_img_rgb = pil_img_from_api.convert('RGB')\n",
    "    else:\n",
    "        pil_img_rgb = pil_img_from_api.copy()\n",
    "\n",
    "    original_width, original_height = pil_img_rgb.size\n",
    "    target_aspect = target_width / target_height\n",
    "    original_aspect = original_width / original_height\n",
    "\n",
    "    if original_aspect > target_aspect: # Original is wider\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * original_aspect)\n",
    "    else: # Original is taller or same aspect\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / original_aspect)\n",
    "\n",
    "    # Ensure new dimensions are at least target dimensions before crop\n",
    "    if new_width < target_width: new_width = target_width\n",
    "    if new_height < target_height: new_height = target_height\n",
    "    \n",
    "    try: resampling_filter = PILImageMod.Resampling.LANCZOS\n",
    "    except AttributeError: resampling_filter = PILImageMod.LANCZOS # Fallback for older Pillow\n",
    "\n",
    "    resized_img = pil_img_rgb.resize((new_width, new_height), resampling_filter)\n",
    "\n",
    "    left = (new_width - target_width) / 2\n",
    "    top = (new_height - target_height) / 2\n",
    "    right = (new_width + target_width) / 2\n",
    "    bottom = (new_height + target_height) / 2\n",
    "    \n",
    "    cropped_img = resized_img.crop((int(left), int(top), int(right), int(bottom)))\n",
    "    return cropped_img\n",
    "\n",
    "# --- 2. Generate Images using Gemini API ---\n",
    "gemini_client_for_images = None # For genai.Client() approach\n",
    "gemini_model_for_images = None # For genai.GenerativeModel() approach\n",
    "client_initialization_method = None # To track which method was used/attempted\n",
    "\n",
    "if not USE_GEMINI_FOR_IMAGE_GENERATION:\n",
    "    print(\"INFO: Image generation with Gemini API is disabled (USE_GEMINI_FOR_IMAGE_GENERATION=False).\")\n",
    "elif not lm_generated_prompts_data:\n",
    "    print(\"WARNING: No LM-generated prompt data found ('lm_generated_prompts_data' is empty). Skipping image generation.\")\n",
    "else:\n",
    "    print(f\"Attempting to initialize Gemini client/model for Image Generation ({GEMINI_IMAGE_GEN_MODEL_NAME})...\")\n",
    "    # --- Attempt 1: Using genai.Client (as per your isolated image gen snippet) ---\n",
    "    try:\n",
    "        print(\"  Trying to initialize with 'genai.Client(api_key=...)'.\")\n",
    "        gemini_client_for_images = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        client_initialization_method = \"Client\"\n",
    "        print(f\"    Successfully initialized 'genai.Client' for image generation.\")\n",
    "    except AttributeError:\n",
    "        print(\"    'genai.Client' not found. Falling back to 'genai.GenerativeModel'.\")\n",
    "        # --- Attempt 2: Using genai.GenerativeModel (fallback) ---\n",
    "        try:\n",
    "            genai.configure(api_key=GEMINI_API_KEY)\n",
    "            # Safety settings can be more permissive for image generation if prompts are well-controlled\n",
    "            safety_settings_img = [\n",
    "                {\"category\": c, \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"} # Or BLOCK_ONLY_HIGH / BLOCK_NONE\n",
    "                for c in [\"HARM_CATEGORY_HARASSMENT\", \"HARM_CATEGORY_HATE_SPEECH\", \n",
    "                          \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"HARM_CATEGORY_DANGEROUS_CONTENT\"]\n",
    "            ]\n",
    "            gemini_model_for_images = genai.GenerativeModel(\n",
    "                GEMINI_IMAGE_GEN_MODEL_NAME,\n",
    "                safety_settings=safety_settings_img\n",
    "            )\n",
    "            client_initialization_method = \"GenerativeModel\"\n",
    "            print(f\"    Successfully initialized 'genai.GenerativeModel' for image generation.\")\n",
    "        except Exception as e_gm_init:\n",
    "            print(f\"    ERROR: Failed to initialize 'genai.GenerativeModel' as fallback: {e_gm_init}\")\n",
    "            traceback.print_exc()\n",
    "    except Exception as e_client_init:\n",
    "        print(f\"  ERROR: Unexpected error during 'genai.Client' initialization: {e_client_init}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Proceed if either client or model was initialized\n",
    "    if gemini_client_for_images or gemini_model_for_images:\n",
    "        print(f\"\\\\nStarting image generation for {len(lm_generated_prompts_data)} prompts using Gemini ({client_initialization_method} method).\")\n",
    "        print(f\"  Rate Limit: {GEMINI_IMAGE_GEN_RPM_LIMIT} RPM. Min delay between requests: {GEMINI_IMAGE_GEN_REQUEST_DELAY_S:.2f}s.\")\n",
    "\n",
    "        for i, item_data in enumerate(lm_generated_prompts_data):\n",
    "            lm_image_prompt = item_data.get(\"image_prompt\", \"\")\n",
    "            duration_ms = item_data.get(\"duration_ms\", 0)\n",
    "            original_chunk_idx = item_data.get(\"original_chunk_index\", i)\n",
    "            raw_chunk_text_ref = item_data.get(\"raw_chunk_text_for_reference\", \"N/A\")\n",
    "\n",
    "            print(f\"\\\\n  Processing image {i+1}/{len(lm_generated_prompts_data)} (Orig. Chunk Idx: {original_chunk_idx})...\")\n",
    "            # print(f\"    Ref Text: \\\\\\\"{raw_chunk_text_ref[:60].replace(chr(10), ' ')}...\\\\\\\"\") # Can be verbose\n",
    "\n",
    "            if not lm_image_prompt.strip() or \"placeholder image\" in lm_image_prompt.lower() or \"error in lm\" in lm_image_prompt.lower():\n",
    "                print(f\"    Skipping image generation due to fallback/error LM prompt: \\\\\\\"{lm_image_prompt}\\\\\\\"\")\n",
    "                image_generation_results_with_paths.append({**item_data, \"image_path\": None, \"error\": \"Skipped due to bad LM prompt\"})\n",
    "                continue\n",
    "\n",
    "            # Construct final prompt for Gemini Image Gen\n",
    "            final_image_prompt = lm_image_prompt.strip()\n",
    "            if GEMINI_PROMPT_UNIVERSAL_SUFFIX and GEMINI_PROMPT_UNIVERSAL_SUFFIX.strip():\n",
    "                final_image_prompt += f\", {GEMINI_PROMPT_UNIVERSAL_SUFFIX.strip()}\"\n",
    "            if NEGATIVE_PROMPT_TERMS and NEGATIVE_PROMPT_TERMS.strip():\n",
    "                final_image_prompt += f\". AVOID: {NEGATIVE_PROMPT_TERMS.strip()}\" # Incorporate negatives\n",
    "\n",
    "            print(f\"    Final Gemini Img Prompt: \\\\\\\"{final_image_prompt[:150]}...\\\\\\\"\")\n",
    "            \n",
    "            # --- Rate Limiting ---\n",
    "            if i > 0 : # No delay before the very first call\n",
    "                print(f\"    Waiting {GEMINI_IMAGE_GEN_REQUEST_DELAY_S:.2f}s (rate limit)...\")\n",
    "                time.sleep(GEMINI_IMAGE_GEN_REQUEST_DELAY_S)\n",
    "\n",
    "            generated_image_path_for_item = None\n",
    "            api_error_message = None\n",
    "            try:\n",
    "                response = None\n",
    "                if client_initialization_method == \"Client\" and gemini_client_for_images:\n",
    "                    response = gemini_client_for_images.models.generate_content(\n",
    "                        model=GEMINI_IMAGE_GEN_MODEL_NAME,\n",
    "                        contents=final_image_prompt,\n",
    "                        # Using 'config' as per your working image gen snippet\n",
    "                        config=genai_types.GenerateContentConfig(\n",
    "                            response_modalities=['TEXT', 'IMAGE'] # Or just ['IMAGE'] if text isn't useful\n",
    "                            # Add other specific image gen configs here if discovered from API docs\n",
    "                            # e.g., number_of_images_to_generate: 1 (usually default)\n",
    "                        ),\n",
    "                        request_options={\"timeout\": GEMINI_IMAGE_GEN_API_TIMEOUT_S}\n",
    "                    )\n",
    "                elif client_initialization_method == \"GenerativeModel\" and gemini_model_for_images:\n",
    "                    # For GenerativeModel, generation_config usually takes a dict or a GenerationConfig object\n",
    "                    # Let's try with a dict first, as that solved the TTS TypeError\n",
    "                    generation_config_dict_img = {\n",
    "                        \"response_modalities\": ['TEXT', 'IMAGE'], # Or just ['IMAGE']\n",
    "                        \"candidate_count\": 1 # Typically 1 for image generation unless you want variations\n",
    "                    }\n",
    "                    response = gemini_model_for_images.generate_content(\n",
    "                        contents=final_image_prompt,\n",
    "                        generation_config=generation_config_dict_img,\n",
    "                        request_options={\"timeout\": GEMINI_IMAGE_GEN_API_TIMEOUT_S}\n",
    "                    )\n",
    "                \n",
    "                if not response:\n",
    "                    raise ValueError(\"API call was not made (client/model not properly initialized or method mismatch).\")\n",
    "\n",
    "                # Process response (adapted from your snippet)\n",
    "                processed_image_from_api = False\n",
    "                if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n",
    "                    for part_idx, part in enumerate(response.candidates[0].content.parts):\n",
    "                        if hasattr(part, 'text') and part.text is not None and part.text.strip():\n",
    "                            print(f\"      Gemini Img Gen API - Text part [{part_idx}]: {part.text.strip()[:100]}...\")\n",
    "                        \n",
    "                        if hasattr(part, 'inline_data') and part.inline_data is not None and hasattr(part.inline_data, 'data'):\n",
    "                            print(f\"      Found image data in part [{part_idx}] (MIME type: {part.inline_data.mime_type}, Size: {len(part.inline_data.data)} bytes).\")\n",
    "                            try:\n",
    "                                pil_img_api = PILImageMod.open(BytesIO(part.inline_data.data))\n",
    "                                print(f\"        PIL Image from API: Mode={pil_img_api.mode}, Size={pil_img_api.size}\")\n",
    "\n",
    "                                # Resize and prepare\n",
    "                                prepared_pil_image = resize_and_prepare_gemini_image(\n",
    "                                    pil_img_api, TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT\n",
    "                                )\n",
    "                                \n",
    "                                image_filename = f\"gemini_image_idx{original_chunk_idx:03d}_item{i:03d}.png\"\n",
    "                                generated_image_path_for_item = os.path.join(IMAGE_OUTPUT_DIR, image_filename)\n",
    "                                prepared_pil_image.save(generated_image_path_for_item)\n",
    "                                print(f\"        Image successfully processed and saved to: {generated_image_path_for_item}\")\n",
    "                                processed_image_from_api = True\n",
    "                                break # Assuming one image per prompt is sufficient for now\n",
    "                            except Exception as e_img_proc:\n",
    "                                print(f\"        ERROR processing/saving image data from API: {e_img_proc}\")\n",
    "                                traceback.print_exc()\n",
    "                                api_error_message = f\"Image processing error: {e_img_proc}\"\n",
    "                \n",
    "                if not processed_image_from_api and not api_error_message: # No image data found in parts\n",
    "                    api_error_message = \"No image data found in API response parts.\"\n",
    "                    print(f\"      WARNING: {api_error_message}\")\n",
    "                    if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "                         api_error_message += f\" Block Reason: {response.prompt_feedback.block_reason}\"\n",
    "                         print(f\"        API Block Reason: {response.prompt_feedback.block_reason}\")\n",
    "\n",
    "\n",
    "            except Exception as e_gen_img_api:\n",
    "                print(f\"    âŒ ERROR during Gemini Image Gen API call for prompt of chunk {original_chunk_idx}: {e_gen_img_api}\")\n",
    "                traceback.print_exc()\n",
    "                api_error_message = str(e_gen_img_api)\n",
    "            \n",
    "            image_generation_results_with_paths.append({\n",
    "                **item_data,\n",
    "                \"image_path\": generated_image_path_for_item,\n",
    "                \"error\": api_error_message\n",
    "            })\n",
    "            \n",
    "            # Intermittent VRAM clear (less critical if not using local PyTorch models for image gen)\n",
    "            # if DEVICE == \"cuda\" and (i + 1) % 5 == 0:\n",
    "            #     print(\"    Clearing GPU cache intermittently (less critical for API-based generation).\")\n",
    "            #     clear_gpu_memory() # clear_gpu_memory() defined in Cell 1\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        print(\"\\\\nImage generation attempts with Gemini API complete.\")\n",
    "    else:\n",
    "        print(\"ERROR: Gemini client or model for image generation could not be initialized. Skipping image generation loop.\")\n",
    "\n",
    "# --- 3. Cleanup (Primarily Python objects, no heavy local models to clear for Gemini API) ---\n",
    "print(\"\\\\nCleaning up (Python object references for image generation phase)...\")\n",
    "if 'gemini_client_for_images' in locals(): del gemini_client_for_images\n",
    "if 'gemini_model_for_images' in locals(): del gemini_model_for_images\n",
    "# PIL images within the loop are handled by scope or explicit del if needed for very large batches (not done here)\n",
    "clear_gpu_memory() # Call standard clear_gpu_memory just in case, though impact is minimal for API calls\n",
    "print(\"Gemini image generation phase cleanup attempted.\")\n",
    "\n",
    "# --- 4. Verify Output ---\n",
    "if image_generation_results_with_paths:\n",
    "    successful_generations = [item for item in image_generation_results_with_paths if item.get(\"image_path\")]\n",
    "    failed_generations_count = len(image_generation_results_with_paths) - len(successful_generations)\n",
    "    print(f\"\\\\nSUCCESS/STATS: Generated and saved {len(successful_generations)} images out of {len(lm_generated_prompts_data)} prompts using Gemini API.\")\n",
    "    if failed_generations_count > 0:\n",
    "        print(f\"  {failed_generations_count} prompts failed or did not result in a saved image.\")\n",
    "        # Optionally print details of first few errors\n",
    "        # for item_idx, item_res in enumerate(image_generation_results_with_paths):\n",
    "        #     if not item_res.get(\"image_path\") and item_idx < 3: # Print first 3 errors\n",
    "        #         print(f\"    Error for prompt index {item_res.get('original_chunk_index', 'N/A')}: {item_res.get('error', 'Unknown error')}\")\n",
    "\n",
    "    if successful_generations:\n",
    "        print(f\"  Sample image path (first successful): {successful_generations[0]['image_path']}\")\n",
    "    elif len(lm_generated_prompts_data) > 0:\n",
    "        print(\"WARNING: No images were successfully generated despite having LM prompts.\")\n",
    "else:\n",
    "    if USE_GEMINI_FOR_IMAGE_GENERATION and lm_generated_prompts_data:\n",
    "        print(\"\\\\nERROR: No image generation results were produced, though attempts should have been made.\")\n",
    "    elif USE_GEMINI_FOR_IMAGE_GENERATION:\n",
    "         print(\"\\\\nINFO: No image generation attempts made (likely no LM prompts).\")\n",
    "\n",
    "print(\"\\\\n----------------------------------------------------------------------------\")\n",
    "if USE_GEMINI_FOR_IMAGE_GENERATION:\n",
    "    print(\"Phase 5 (Image Generation with Gemini API): Complete.\")\n",
    "else:\n",
    "    print(\"Phase 5 (Image Generation with Gemini API): Skipped as per configuration.\")\n",
    "print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7 (Refactored for Phase A): Prepare Visual Components (ImageClips for Main Content)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# This cell takes the output of image generation (e.g., 'image_generation_results_with_paths')\n",
    "# and prepares a list of ImageClip objects for the main content of the video.\n",
    "# It REPLACES the original Cell 7 that created SILENT_VIDEO_PATH.\n",
    "\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Cell 7 (Phase A): Preparing Visual Components (ImageClips for Main Content)...\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "# Ensure global variables from Cell 2 are accessible:\n",
    "# VIDEO_TARGET_WIDTH, VIDEO_TARGET_HEIGHT, VIDEO_FPS\n",
    "\n",
    "# Ensure MoviePy components, PIL, NumPy, os, gc are imported (typically in Cell 1)\n",
    "# from moviepy.editor import ImageClip\n",
    "# from PIL import Image as PILImageMod\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc\n",
    "# import traceback # Already in Cell 1\n",
    "\n",
    "# --- 1. Helper Function for Image Resizing/Scaling (from original Cell 7 logic) ---\n",
    "def resize_and_scale_pil_image_to_fill(pil_img, target_width, target_height):\n",
    "    \"\"\"\n",
    "    Resizes PIL image to fill target_width x target_height, cropping if necessary,\n",
    "    maintaining aspect ratio while ensuring the target dimensions are filled.\n",
    "    \"\"\"\n",
    "    original_width, original_height = pil_img.size\n",
    "    target_aspect = target_width / target_height\n",
    "    original_aspect = original_width / original_height\n",
    "\n",
    "    if original_aspect > target_aspect: # Original is wider than target: fit to height, then crop width\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * original_aspect)\n",
    "        if new_width < target_width: # Should not happen if logic is correct but as a safeguard\n",
    "            # print(f\"  DEBUG: resize_and_scale_pil_image_to_fill - Wider image new_width ({new_width}) < target_width ({target_width}). Adjusting.\")\n",
    "            new_width = target_width\n",
    "            new_height = int(new_width / original_aspect)\n",
    "    else: # Original is taller or same aspect as target: fit to width, then crop height\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / original_aspect)\n",
    "        if new_height < target_height: # Safeguard\n",
    "            # print(f\"  DEBUG: resize_and_scale_pil_image_to_fill - Taller image new_height ({new_height}) < target_height ({target_height}). Adjusting.\")\n",
    "            new_height = target_height\n",
    "            new_width = int(new_height * original_aspect)\n",
    "    \n",
    "    try: # Handle Pillow version differences for resampling filter\n",
    "        resampling_filter = PILImageMod.Resampling.LANCZOS\n",
    "    except AttributeError:\n",
    "        resampling_filter = PILImageMod.LANCZOS\n",
    "\n",
    "    resized_img = pil_img.resize((new_width, new_height), resampling_filter)\n",
    "\n",
    "    left = (new_width - target_width) / 2\n",
    "    top = (new_height - target_height) / 2\n",
    "    right = (new_width + target_width) / 2\n",
    "    bottom = (new_height + target_height) / 2\n",
    "    \n",
    "    cropped_img = resized_img.crop((int(left), int(top), int(right), int(bottom)))\n",
    "    return cropped_img\n",
    "\n",
    "# --- 2. Create ImageClips for the Main Content ---\n",
    "# This list will be the output of this phase, used in Phase C\n",
    "main_content_image_clips = [] \n",
    "\n",
    "# This variable should be populated by your image generation cell (original Cell 6 / Phase 5)\n",
    "# It's a list of dictionaries, each with \"image_path\", \"duration_ms\", \"original_chunk_index\"\n",
    "# Example: image_generation_results_with_paths = [\n",
    "#    {\"image_path\": \"/path/to/img1.png\", \"duration_ms\": 5000, \"original_chunk_index\": 0, ...},\n",
    "#    ...\n",
    "# ]\n",
    "\n",
    "# Clear any previous content if re-running\n",
    "if 'main_content_image_clips' in globals():\n",
    "    for old_clip in main_content_image_clips:\n",
    "        if hasattr(old_clip, 'close') and callable(old_clip.close):\n",
    "            try:\n",
    "                old_clip.close()\n",
    "            except Exception as e_close:\n",
    "                print(f\"DEBUG: Error closing old clip: {e_close}\")\n",
    "    del main_content_image_clips\n",
    "    gc.collect()\n",
    "main_content_image_clips = []\n",
    "\n",
    "\n",
    "print(f\"Accessing 'image_generation_results_with_paths' for creating ImageClips.\")\n",
    "if 'image_generation_results_with_paths' in globals() and image_generation_results_with_paths:\n",
    "    print(f\"Found {len(image_generation_results_with_paths)} items in 'image_generation_results_with_paths'.\")\n",
    "    \n",
    "    # Filter for segments that have a valid image path and a minimum duration\n",
    "    valid_segments_for_clips = []\n",
    "    for item in image_generation_results_with_paths:\n",
    "        img_path = item.get(\"image_path\")\n",
    "        duration_val = item.get(\"duration_ms\", 0)\n",
    "        if img_path and os.path.exists(img_path) and (duration_val / 1000.0 > 0.05):\n",
    "            valid_segments_for_clips.append(item)\n",
    "        # else:\n",
    "        #     print(f\"  DEBUG: Skipping item for ImageClip: Path='{img_path}', Exists={os.path.exists(img_path) if img_path else 'N/A'}, DurationOK={(duration_val / 1000.0 > 0.05) if duration_val else 'N/A'}\")\n",
    "\n",
    "    if not valid_segments_for_clips:\n",
    "        print(\"WARNING: No valid image segments (with existing image and sufficient duration) found to create ImageClips.\")\n",
    "    else:\n",
    "        print(f\"Creating ImageClips for {len(valid_segments_for_clips)} valid image segments...\")\n",
    "        for i, item_data in enumerate(valid_segments_for_clips):\n",
    "            image_path = item_data[\"image_path\"]\n",
    "            duration_s = item_data[\"duration_ms\"] / 1000.0\n",
    "            original_chunk_index = item_data[\"original_chunk_index\"]\n",
    "\n",
    "            print(f\"  Preparing ImageClip {i+1}/{len(valid_segments_for_clips)} \"\n",
    "                  f\"(Orig. Chunk Idx: {original_chunk_index}, Img: {os.path.basename(image_path)}, Dur: {duration_s:.2f}s)\")\n",
    "\n",
    "            try:\n",
    "                with PILImageMod.open(image_path) as pil_img_source: # Use 'with' to ensure file is closed\n",
    "                    if pil_img_source.mode != 'RGB':\n",
    "                        pil_img_rgb = pil_img_source.convert('RGB')\n",
    "                    else:\n",
    "                        pil_img_rgb = pil_img_source.copy() # Make a copy if already RGB to avoid issues if original is used elsewhere\n",
    "\n",
    "                prepared_pil_frame = resize_and_scale_pil_image_to_fill(\n",
    "                    pil_img_rgb,\n",
    "                    VIDEO_TARGET_WIDTH,  # From Cell 2\n",
    "                    VIDEO_TARGET_HEIGHT  # From Cell 2\n",
    "                )\n",
    "                \n",
    "                # Convert PIL image to NumPy array for MoviePy\n",
    "                frame_np_array = np.array(prepared_pil_frame)\n",
    "\n",
    "                image_clip_obj = ImageClip(frame_np_array).set_duration(duration_s)\n",
    "                # Set FPS for the clip; important for smooth concatenation later\n",
    "                image_clip_obj = image_clip_obj.set_fps(VIDEO_FPS) # From Cell 2\n",
    "\n",
    "                main_content_image_clips.append(image_clip_obj)\n",
    "                \n",
    "                # PIL image (pil_img_rgb, prepared_pil_frame) and numpy array (frame_np_array) \n",
    "                # go out of scope or are overwritten in next iteration.\n",
    "                # Explicit deletion might be needed if memory becomes an issue with very many images.\n",
    "                del pil_img_rgb\n",
    "                del prepared_pil_frame\n",
    "                del frame_np_array\n",
    "                if (i + 1) % 20 == 0: # Collect garbage every 20 images\n",
    "                    gc.collect()\n",
    "\n",
    "\n",
    "            except Exception as e_clip_creation:\n",
    "                print(f\"    ERROR creating ImageClip for {image_path}: {e_clip_creation}\")\n",
    "                traceback.print_exc()\n",
    "                # Attempt to clean up potentially problematic objects\n",
    "                if 'pil_img_rgb' in locals(): del pil_img_rgb\n",
    "                if 'prepared_pil_frame' in locals(): del prepared_pil_frame\n",
    "                if 'frame_np_array' in locals(): del frame_np_array\n",
    "                gc.collect()\n",
    "                continue \n",
    "        \n",
    "        print(f\"\\nSuccessfully created {len(main_content_image_clips)} ImageClips for the main video content.\")\n",
    "else:\n",
    "    print(\"ERROR: 'image_generation_results_with_paths' not found in global scope or is empty. Cannot prepare visual components.\")\n",
    "    # Ensure main_content_image_clips is defined as an empty list if we hit this branch\n",
    "    if 'main_content_image_clips' not in globals(): # Should have been defined above\n",
    "        main_content_image_clips = []\n",
    "\n",
    "# --- Phase A Final Verification ---\n",
    "if main_content_image_clips:\n",
    "    total_duration_visual_clips = sum(clip.duration for clip in main_content_image_clips)\n",
    "    print(f\"Phase A (Visual Components) Complete: Prepared {len(main_content_image_clips)} ImageClip objects for main content.\")\n",
    "    print(f\"  Total estimated duration of these main content clips: {total_duration_visual_clips:.2f}s\")\n",
    "    if main_content_image_clips[0].duration:\n",
    "         print(f\"  The first clip has a duration of: {main_content_image_clips[0].duration:.2f}s\")\n",
    "    # print(f\"  Memory usage of main_content_image_clips list (approx): {sys.getsizeof(main_content_image_clips) / (1024*1024):.2f} MB\") # Rough estimate\n",
    "else:\n",
    "    print(\"Phase A (Visual Components) Complete: No ImageClips were prepared (list is empty). Review logs for errors.\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print(\"Output of this cell: `main_content_image_clips` (list of MoviePy ImageClip objects)\")\n",
    "print(\"---------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell PhaseC: Prepare Visual Sequence Data & Pre-render Intro/Outro with FFmpeg\n",
    "# ------------------------------------------------------------------------------------\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Phase C: Preparing Visual Sequence Data & Pre-rendering Intro/Outro with FFmpeg...\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import subprocess # For calling FFmpeg\n",
    "import traceback\n",
    "import json # For potentially structuring data if needed\n",
    "# MoviePy imports might still be needed if we create placeholder clips or load pre-rendered ones\n",
    "from moviepy.editor import VideoFileClip, ColorClip \n",
    "\n",
    "# --- Configuration Values (Expected from Cell 2) ---\n",
    "KAGGLE_WORKING_DIR = globals().get('KAGGLE_WORKING_DIR', '/kaggle/working/')\n",
    "timestamp_final = globals().get('timestamp_final', str(int(time.time()))) # For unique filenames\n",
    "\n",
    "# Intro/Outro General Configs\n",
    "ADD_INTRO_OUTRO_SCREENS = globals().get('ADD_INTRO_OUTRO_SCREENS', False)\n",
    "INTRO_SCREEN_DURATION_S = globals().get('INTRO_SCREEN_DURATION_S', 3.0)\n",
    "OUTRO_SCREEN_DURATION_S = globals().get('OUTRO_SCREEN_DURATION_S', 3.0)\n",
    "\n",
    "# Intro Text Configs (for FFmpeg drawtext)\n",
    "ENABLE_INTRO_TEXT = globals().get('ENABLE_INTRO_TEXT', False)\n",
    "INTRO_TEXT_CONTENT = globals().get('INTRO_TEXT_CONTENT', \"Intro Text\")\n",
    "INTRO_TEXT_FONTSIZE = globals().get('INTRO_TEXT_FONTSIZE', 70)\n",
    "# Note: INTRO_TEXT_FONT (e.g., \"Arial-Bold\") is less directly useful than FFMPEG_DRAWTEXT_FONTFILE\n",
    "FFMPEG_DRAWTEXT_FONTFILE = globals().get('FFMPEG_DRAWTEXT_FONTFILE', '/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf')\n",
    "FFMPEG_DRAWTEXT_INTRO_COLOR = globals().get('FFMPEG_DRAWTEXT_INTRO_COLOR', 'white')\n",
    "# INTRO_TEXT_ALIGNMENT and INTRO_TEXT_POSITION will need to be translated to x,y for FFmpeg\n",
    "\n",
    "# Outro Text Configs (for FFmpeg drawtext)\n",
    "ENABLE_OUTRO_TEXT = globals().get('ENABLE_OUTRO_TEXT', False)\n",
    "OUTRO_TEXT_CONTENT = globals().get('OUTRO_TEXT_CONTENT', \"Outro Text\")\n",
    "OUTRO_TEXT_FONTSIZE = globals().get('OUTRO_TEXT_FONTSIZE', 70)\n",
    "FFMPEG_DRAWTEXT_OUTRO_COLOR = globals().get('FFMPEG_DRAWTEXT_OUTRO_COLOR', 'yellow')\n",
    "# OUTRO_TEXT_ALIGNMENT and OUTRO_TEXT_POSITION will need to be translated for FFmpeg\n",
    "\n",
    "# Video Target Dimensions (for intro/outro rendering)\n",
    "VIDEO_TARGET_WIDTH = globals().get('VIDEO_TARGET_WIDTH', 1920)\n",
    "VIDEO_TARGET_HEIGHT = globals().get('VIDEO_TARGET_HEIGHT', 1080)\n",
    "VIDEO_FPS = globals().get('VIDEO_FPS', 24)\n",
    "\n",
    "# Main Content Data (from image generation phase)\n",
    "# This list is expected to be populated by Cell 7 (Phase 5 - Image Generation)\n",
    "# It contains dicts like: {\"image_path\": \"/path/img.png\", \"duration_ms\": 5000, ...}\n",
    "image_generation_results_with_paths = globals().get('image_generation_results_with_paths', [])\n",
    "\n",
    "# --- Output variables for this phase ---\n",
    "# `final_video_visual_structure` will now be a dictionary holding paths and data\n",
    "# It will NOT be a renderable MoviePy VideoClip of the whole sequence.\n",
    "final_video_visual_structure_data = {\n",
    "    \"intro_clip_path\": None,\n",
    "    \"outro_clip_path\": None,\n",
    "    \"main_content_image_sequence\": [], # List of {\"path\": \"...\", \"duration_s\": ...}\n",
    "    \"total_main_content_duration_s\": 0.0,\n",
    "    \"overall_estimated_duration_s\": 0.0 # intro + main + outro\n",
    "}\n",
    "\n",
    "# --- Helper Function to Create Video with Text using FFmpeg ---\n",
    "def create_text_video_with_ffmpeg(output_path, text_content, duration_s,\n",
    "                                  width, height, fps,\n",
    "                                  fontfile, fontsize, fontcolor,\n",
    "                                  bg_color=\"black\"):\n",
    "    \"\"\"\n",
    "    Creates a short video with centered text using FFmpeg.\n",
    "    Text content should handle its own newlines with \\\\n if needed for FFmpeg drawtext.\n",
    "    \"\"\"\n",
    "    print(f\"  Attempting to create FFmpeg text video: {os.path.basename(output_path)}\")\n",
    "    if not text_content or not text_content.strip():\n",
    "        print(\"    No text content provided. Creating blank video.\")\n",
    "        ffmpeg_drawtext_filter = \"\"\n",
    "    else:\n",
    "        # Basic centering for FFmpeg drawtext. More complex alignment needs more complex x,y expressions.\n",
    "        # FFmpeg's drawtext is powerful but syntax can be tricky for precise alignment.\n",
    "        # For multi-line text, ensure `text_content` has literal '\\n' or use `textfile` option.\n",
    "        # Escaping text for FFmpeg: ' -> '\\''\n",
    "        escaped_text = text_content.replace(\":\", \"\\\\:\").replace(\"'\", \"'\\\\\\\\\\\\''\") # Basic escaping\n",
    "\n",
    "        # A very basic attempt at multi-line centering.\n",
    "        # For robust centering, you'd often measure text width/height first, which is hard with drawtext alone.\n",
    "        # Or use fixed y positions for lines.\n",
    "        # This example centers the block, assuming newlines are in the text.\n",
    "        drawtext_params = f\"fontfile='{fontfile}':text='{escaped_text}':fontsize={fontsize}:fontcolor='{fontcolor}':x=(w-text_w)/2:y=(h-text_h)/2\"\n",
    "        ffmpeg_drawtext_filter = f\"drawtext={drawtext_params}\"\n",
    "\n",
    "    # Command to create a color background and overlay text\n",
    "    # Using lavfi for color source, then drawtext, then format for encoder\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg', '-y', # Overwrite output\n",
    "        '-f', 'lavfi', '-i', f\"color=c={bg_color}:s={width}x{height}:d={duration_s}:r={fps}\",\n",
    "    ]\n",
    "    if ffmpeg_drawtext_filter: # Add drawtext filter only if there's text\n",
    "         ffmpeg_cmd.extend(['-vf', ffmpeg_drawtext_filter])\n",
    "    ffmpeg_cmd.extend([\n",
    "        '-c:v', 'libx264', # Using a common CPU encoder for these short clips\n",
    "        '-preset', 'ultrafast',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        '-r', str(fps), # Ensure output FPS matches\n",
    "        output_path\n",
    "    ])\n",
    "\n",
    "    print(f\"    Executing FFmpeg: {' '.join(ffmpeg_cmd)}\")\n",
    "    try:\n",
    "        process = subprocess.run(ffmpeg_cmd, check=True, capture_output=True, text=True, encoding='utf-8')\n",
    "        print(f\"    Successfully created: {os.path.basename(output_path)}\")\n",
    "        return output_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"    ERROR creating FFmpeg text video for {os.path.basename(output_path)}:\")\n",
    "        print(f\"      FFmpeg stderr: {e.stderr}\")\n",
    "        # print(f\"      FFmpeg stdout: {e.stdout}\") # Usually less useful for errors\n",
    "        return None\n",
    "    except Exception as e_gen:\n",
    "        print(f\"    UNEXPECTED ERROR creating FFmpeg text video for {os.path.basename(output_path)}: {e_gen}\")\n",
    "        return None\n",
    "\n",
    "# --- 1. Pre-render Intro Screen (if enabled) using FFmpeg ---\n",
    "if ADD_INTRO_OUTRO_SCREENS and INTRO_SCREEN_DURATION_S > 0:\n",
    "    print(f\"\\nCreating intro screen video ({INTRO_SCREEN_DURATION_S}s) using FFmpeg...\")\n",
    "    intro_output_filename = f\"temp_intro_ffmpeg_{timestamp_final}.mp4\"\n",
    "    intro_video_path = os.path.join(KAGGLE_WORKING_DIR, intro_output_filename)\n",
    "    \n",
    "    text_to_render_intro = INTRO_TEXT_CONTENT if ENABLE_INTRO_TEXT else \"\"\n",
    "    \n",
    "    created_intro_path = create_text_video_with_ffmpeg(\n",
    "        output_path=intro_video_path,\n",
    "        text_content=text_to_render_intro,\n",
    "        duration_s=INTRO_SCREEN_DURATION_S,\n",
    "        width=VIDEO_TARGET_WIDTH,\n",
    "        height=VIDEO_TARGET_HEIGHT,\n",
    "        fps=VIDEO_FPS,\n",
    "        fontfile=FFMPEG_DRAWTEXT_FONTFILE,\n",
    "        fontsize=INTRO_TEXT_FONTSIZE,\n",
    "        fontcolor=FFMPEG_DRAWTEXT_INTRO_COLOR,\n",
    "        bg_color=\"black\" # Or a configured intro background color\n",
    "    )\n",
    "    if created_intro_path and os.path.exists(created_intro_path):\n",
    "        final_video_visual_structure_data[\"intro_clip_path\"] = created_intro_path\n",
    "        final_video_visual_structure_data[\"overall_estimated_duration_s\"] += INTRO_SCREEN_DURATION_S\n",
    "        print(f\"  Intro video pre-rendered to: {created_intro_path}\")\n",
    "    else:\n",
    "        print(\"  Failed to pre-render intro video. It will be skipped.\")\n",
    "else:\n",
    "    print(\"\\nSkipping intro screen generation (disabled or zero duration).\")\n",
    "\n",
    "\n",
    "# --- 2. Prepare Main Content Image Sequence Data ---\n",
    "# This step doesn't render anything. It just structures the data for Cell PhaseD.\n",
    "print(f\"\\nPreparing main content image sequence data...\")\n",
    "if image_generation_results_with_paths:\n",
    "    valid_main_content_images = []\n",
    "    current_main_total_duration_s = 0.0\n",
    "    for item_data in image_generation_results_with_paths:\n",
    "        img_path = item_data.get(\"image_path\")\n",
    "        duration_s = item_data.get(\"duration_ms\", 0) / 1000.0\n",
    "        if img_path and os.path.exists(img_path) and duration_s > 0.01:\n",
    "            valid_main_content_images.append({\n",
    "                \"path\": img_path,\n",
    "                \"duration_s\": duration_s,\n",
    "                \"original_chunk_index\": item_data.get(\"original_chunk_index\") # Keep for reference\n",
    "            })\n",
    "            current_main_total_duration_s += duration_s\n",
    "        else:\n",
    "            print(f\"  Skipping invalid image/duration for main content: Path='{img_path}', Dur={duration_s:.2f}s\")\n",
    "            \n",
    "    final_video_visual_structure_data[\"main_content_image_sequence\"] = valid_main_content_images\n",
    "    final_video_visual_structure_data[\"total_main_content_duration_s\"] = current_main_total_duration_s\n",
    "    final_video_visual_structure_data[\"overall_estimated_duration_s\"] += current_main_total_duration_s\n",
    "    print(f\"  Prepared data for {len(valid_main_content_images)} main content images.\")\n",
    "    print(f\"  Total duration of main content visuals: {current_main_total_duration_s:.2f}s\")\n",
    "else:\n",
    "    print(\"  WARNING: 'image_generation_results_with_paths' is empty. No main content visuals to prepare.\")\n",
    "\n",
    "\n",
    "# --- 3. Pre-render Outro Screen (if enabled) using FFmpeg ---\n",
    "if ADD_INTRO_OUTRO_SCREENS and OUTRO_SCREEN_DURATION_S > 0:\n",
    "    print(f\"\\nCreating outro screen video ({OUTRO_SCREEN_DURATION_S}s) using FFmpeg...\")\n",
    "    outro_output_filename = f\"temp_outro_ffmpeg_{timestamp_final}.mp4\"\n",
    "    outro_video_path = os.path.join(KAGGLE_WORKING_DIR, outro_output_filename)\n",
    "\n",
    "    text_to_render_outro = OUTRO_TEXT_CONTENT if ENABLE_OUTRO_TEXT else \"\"\n",
    "\n",
    "    created_outro_path = create_text_video_with_ffmpeg(\n",
    "        output_path=outro_video_path,\n",
    "        text_content=text_to_render_outro,\n",
    "        duration_s=OUTRO_SCREEN_DURATION_S,\n",
    "        width=VIDEO_TARGET_WIDTH,\n",
    "        height=VIDEO_TARGET_HEIGHT,\n",
    "        fps=VIDEO_FPS,\n",
    "        fontfile=FFMPEG_DRAWTEXT_FONTFILE, # Using same font file, can be different\n",
    "        fontsize=OUTRO_TEXT_FONTSIZE,\n",
    "        fontcolor=FFMPEG_DRAWTEXT_OUTRO_COLOR,\n",
    "        bg_color=\"black\" # Or a configured outro background color\n",
    "    )\n",
    "    if created_outro_path and os.path.exists(created_outro_path):\n",
    "        final_video_visual_structure_data[\"outro_clip_path\"] = created_outro_path\n",
    "        final_video_visual_structure_data[\"overall_estimated_duration_s\"] += OUTRO_SCREEN_DURATION_S\n",
    "        print(f\"  Outro video pre-rendered to: {created_outro_path}\")\n",
    "    else:\n",
    "        print(\"  Failed to pre-render outro video. It will be skipped.\")\n",
    "else:\n",
    "    print(\"\\nSkipping outro screen generation (disabled or zero duration).\")\n",
    "\n",
    "# --- 4. Define `final_video_visual_structure` (Placeholder for MoviePy based effects if any were kept) ---\n",
    "# For this \"all-in-FFmpeg\" plan, MoviePy's role in visual structure assembly is minimized.\n",
    "# If ALL visual effects (zoom, fades) are moved to FFmpeg in PhaseD,\n",
    "# then `final_video_visual_structure` as a MoviePy VideoClip might not be needed for visuals.\n",
    "# However, PhaseD expects *something* named `final_video_visual_structure` to get the .duration.\n",
    "# We'll create a simple placeholder MoviePy clip representing the *total duration*.\n",
    "# Or, Cell PhaseD could be modified to take `final_video_visual_structure_data` directly.\n",
    "\n",
    "# For now, let's create a placeholder MoviePy clip just for total duration.\n",
    "# This avoids major refactoring of Phase D's initial duration calculation for now.\n",
    "if final_video_visual_structure_data[\"overall_estimated_duration_s\"] > 0:\n",
    "    print(f\"\\nCreating a placeholder MoviePy ColorClip for total visual duration: {final_video_visual_structure_data['overall_estimated_duration_s']:.2f}s\")\n",
    "    final_video_visual_structure = ColorClip(\n",
    "        size=(VIDEO_TARGET_WIDTH, VIDEO_TARGET_HEIGHT), # Use target video dimensions\n",
    "        color=(5,5,5), # A dummy color, won't be rendered if all visuals are FFmpeg\n",
    "        duration=final_video_visual_structure_data[\"overall_estimated_duration_s\"]\n",
    "    ).set_fps(VIDEO_FPS)\n",
    "    print(\"  Placeholder MoviePy clip created.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Zero total estimated duration. Creating a minimal 1-second placeholder clip.\")\n",
    "    final_video_visual_structure = ColorClip(\n",
    "        size=(VIDEO_TARGET_WIDTH, VIDEO_TARGET_HEIGHT),\n",
    "        color=(5,5,5),\n",
    "        duration=1.0 # Avoid zero duration clip\n",
    "    ).set_fps(VIDEO_FPS)\n",
    "    final_video_visual_structure_data[\"overall_estimated_duration_s\"] = 1.0\n",
    "\n",
    "\n",
    "# --- Phase C Cleanup (Minimal for this version) ---\n",
    "# No complex MoviePy clips are assembled here for the full sequence.\n",
    "# The helper function for FFmpeg calls cleans up after itself if it were to create intermediate files.\n",
    "gc.collect()\n",
    "print(\"\\nPhase C: Data preparation and intro/outro pre-rendering complete.\")\n",
    "\n",
    "# --- Phase C Final Verification ---\n",
    "print(\"\\n--- Phase C Final Data Structure ---\")\n",
    "# Using pprint for cleaner dictionary output if you import it: import pprint; pprint.pprint(final_video_visual_structure_data)\n",
    "print(json.dumps(final_video_visual_structure_data, indent=2)) # Print the data structure\n",
    "\n",
    "if final_video_visual_structure:\n",
    "    print(f\"\\nINFO: A placeholder MoviePy 'final_video_visual_structure' VideoClip object has been created.\")\n",
    "    print(f\"  Its duration is: {final_video_visual_structure.duration:.2f}s (This should match 'overall_estimated_duration_s')\")\n",
    "    print(f\"  This placeholder is primarily for Phase D to correctly calculate total video length.\")\n",
    "    print(f\"  The actual visual content will be assembled by FFmpeg in Phase D using data from 'final_video_visual_structure_data'.\")\n",
    "else:\n",
    "    print(\"CRITICAL Phase C Error: Placeholder 'final_video_visual_structure' was not created.\")\n",
    "\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Output of this cell: `final_video_visual_structure_data` (dictionary) and a placeholder `final_video_visual_structure` (MoviePy Clip)\")\n",
    "print(\"#######################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-23T12:33:46.914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell PhaseD: Final FFmpeg-Centric Render - TEST 3.0 (Zoom + XFade + Adjusted Durations)\n",
    "# ------------------------------------------------------------------------------------\n",
    "print(\"#######################################################################################\")\n",
    "print(\"Phase D: Final FFmpeg-Centric Render - TEST 3.0 (Zoom + XFade + Adjusted Durations)...\")\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time\n",
    "import math\n",
    "\n",
    "from moviepy.editor import AudioClip, CompositeAudioClip, concatenate_audioclips\n",
    "import moviepy.video.fx.all as vfx_all\n",
    "\n",
    "phase_d_start_time = time.time()\n",
    "\n",
    "# --- Configuration (Expected from Cell 2) ---\n",
    "FFMPEG_VCODEC = globals().get('FFMPEG_VCODEC', 'h264_nvenc')\n",
    "FFMPEG_NVENC_PRESET = globals().get('FFMPEG_NVENC_PRESET', 'p6')\n",
    "FFMPEG_NVENC_TUNE = globals().get('FFMPEG_NVENC_TUNE', 'hq')\n",
    "FFMPEG_NVENC_RC = globals().get('FFMPEG_NVENC_RC', 'vbr')\n",
    "FFMPEG_NVENC_CQ = globals().get('FFMPEG_NVENC_CQ', '23')\n",
    "FFMPEG_NVENC_BITRATE = globals().get('FFMPEG_NVENC_BITRATE', '0')\n",
    "FFMPEG_PIX_FMT = globals().get('FFMPEG_PIX_FMT', 'yuv420p')\n",
    "FFMPEG_ACODEC_FINAL = globals().get('FFMPEG_ACODEC_FINAL', 'aac')\n",
    "FFMPEG_AUDIO_CHANNELS_FINAL = globals().get('FFMPEG_AUDIO_CHANNELS_FINAL', 2)\n",
    "VIDEO_FPS = globals().get('VIDEO_FPS', 24)\n",
    "VIDEO_TARGET_WIDTH = globals().get('VIDEO_TARGET_WIDTH', 1920)\n",
    "VIDEO_TARGET_HEIGHT = globals().get('VIDEO_TARGET_HEIGHT', 1080)\n",
    "KAGGLE_WORKING_DIR = globals().get('KAGGLE_WORKING_DIR', \"/kaggle/working/\")\n",
    "timestamp_final = globals().get('timestamp_final', str(int(time.time())))\n",
    "\n",
    "FFMPEG_XFADE_TRANSITION_TYPE = globals().get('FFMPEG_XFADE_TRANSITION_TYPE', \"fade\")\n",
    "FFMPEG_XFADE_DURATION_S = globals().get('FFMPEG_XFADE_DURATION_S', 1.5)\n",
    "\n",
    "# For Test 3.0, CALM_ZOOM_EFFECT_ENABLED should be True from Cell 2\n",
    "CALM_ZOOM_EFFECT_ENABLED = globals().get('CALM_ZOOM_EFFECT_ENABLED', True) \n",
    "FFMPEG_ZOOMPAN_MAX_SCALE = globals().get('FFMPEG_ZOOMPAN_MAX_SCALE', 1.10)\n",
    "FFMPEG_ZOOMPAN_CYCLES_PER_CLIP = globals().get('FFMPEG_ZOOMPAN_CYCLES_PER_CLIP', 20) # Your desired cycles\n",
    "FFMPEG_ZOOMPAN_TARGET_FPS = globals().get('FFMPEG_ZOOMPAN_TARGET_FPS', VIDEO_FPS)\n",
    "CALM_ZOOM_TYPE = globals().get('CALM_ZOOM_TYPE', \"multi_cycle_in_out\")\n",
    "\n",
    "ASS_SUBTITLE_PATH = globals().get('ASS_SUBTITLE_PATH')\n",
    "ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD = globals().get('ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD')\n",
    "final_video_visual_structure_data = globals().get('final_video_visual_structure_data')\n",
    "narration_audioclip_from_array = globals().get('narration_audioclip_from_array')\n",
    "bg_music_audioclip_from_array = globals().get('bg_music_audioclip_from_array')\n",
    "ADD_BACKGROUND_MUSIC = globals().get('ADD_BACKGROUND_MUSIC', False)\n",
    "\n",
    "temp_final_mixed_audio_path = os.path.join(KAGGLE_WORKING_DIR, f\"temp_final_mixed_audio_{timestamp_final}.aac\")\n",
    "\n",
    "final_render_successful = False\n",
    "audio_for_ffmpeg = None; final_audio_mix_main_content = None; narration_final_for_mix = None\n",
    "bg_music_final_for_mix = None; intro_audio_dummy = None; outro_audio_dummy = None\n",
    "\n",
    "print(\"\\n--- Input Validation ---\")\n",
    "critical_data_missing = False\n",
    "if not final_video_visual_structure_data: critical_data_missing = True; print(\"ERROR: `final_video_visual_structure_data` missing.\")\n",
    "if not narration_audioclip_from_array: critical_data_missing = True; print(\"ERROR: `narration_audioclip_from_array` missing.\")\n",
    "if not ASS_SUBTITLE_PATH or not os.path.exists(ASS_SUBTITLE_PATH): critical_data_missing = True; print(f\"ERROR: Subtitle file '{ASS_SUBTITLE_PATH or 'Not Defined'}' missing.\")\n",
    "if not ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD: critical_data_missing = True; print(\"ERROR: `ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD` missing.\")\n",
    "if ADD_BACKGROUND_MUSIC and not bg_music_audioclip_from_array : critical_data_missing = True; print(\"ERROR: BGM missing despite ADD_BACKGROUND_MUSIC=True.\")\n",
    "\n",
    "if critical_data_missing: print(\"ERROR: Phase D skipped due to missing critical inputs/data.\")\n",
    "else:\n",
    "    try:\n",
    "        step_start_time = time.time()\n",
    "        print(\"\\n--- Step 1 (Phase D): Determine Target Durations & Adjust Image Durations ---\")\n",
    "        \n",
    "        intro_clip_path = final_video_visual_structure_data.get(\"intro_clip_path\")\n",
    "        outro_clip_path = final_video_visual_structure_data.get(\"outro_clip_path\")\n",
    "        main_content_sequence_original_durations = final_video_visual_structure_data.get(\"main_content_image_sequence\", [])\n",
    "        \n",
    "        intro_d, outro_d = 0.0, 0.0\n",
    "        if intro_clip_path and os.path.exists(intro_clip_path):\n",
    "            try: result = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', intro_clip_path], capture_output=True, text=True, check=True); intro_d = float(result.stdout.strip()); print(f\"  Intro clip: {os.path.basename(intro_clip_path)}, Duration: {intro_d:.3f}s\")\n",
    "            except: intro_d = globals().get('INTRO_SCREEN_DURATION_S', 0.0) if globals().get('ADD_INTRO_OUTRO_SCREENS', False) else 0.0; print(f\"  Intro ffprobe failed. Using configured: {intro_d:.3f}s\")\n",
    "        elif globals().get('ADD_INTRO_OUTRO_SCREENS', False) and globals().get('INTRO_SCREEN_DURATION_S', 0.0) > 0: intro_d = globals().get('INTRO_SCREEN_DURATION_S', 0.0); print(f\"  No intro clip path. Using configured: {intro_d:.3f}s\")\n",
    "        if outro_clip_path and os.path.exists(outro_clip_path):\n",
    "            try: result = subprocess.run(['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', outro_clip_path], capture_output=True, text=True, check=True); outro_d = float(result.stdout.strip()); print(f\"  Outro clip: {os.path.basename(outro_clip_path)}, Duration: {outro_d:.3f}s\")\n",
    "            except: outro_d = globals().get('OUTRO_SCREEN_DURATION_S', 0.0) if globals().get('ADD_INTRO_OUTRO_SCREENS', False) else 0.0; print(f\"  Outro ffprobe failed. Using configured: {outro_d:.3f}s\")\n",
    "        elif globals().get('ADD_INTRO_OUTRO_SCREENS', False) and globals().get('OUTRO_SCREEN_DURATION_S', 0.0) > 0: outro_d = globals().get('OUTRO_SCREEN_DURATION_S', 0.0); print(f\"  No outro clip path. Using configured: {outro_d:.3f}s\")\n",
    "\n",
    "        target_main_content_narration_duration_s = narration_audioclip_from_array.duration\n",
    "        print(f\"  Target Main Content Narration Duration (from audio clip): {target_main_content_narration_duration_s:.3f}s\")\n",
    "        main_content_sequence_adjusted_durations = []\n",
    "        num_main_content_images = len(main_content_sequence_original_durations)\n",
    "        current_xfade_duration_s = FFMPEG_XFADE_DURATION_S if FFMPEG_XFADE_DURATION_S > 0 else 0\n",
    "\n",
    "        if num_main_content_images > 0:\n",
    "            original_sum_image_durations = sum(item['duration_s'] for item in main_content_sequence_original_durations)\n",
    "            time_lost_to_fades = 0.0\n",
    "            if num_main_content_images > 1 and current_xfade_duration_s > 0: time_lost_to_fades = (num_main_content_images - 1) * current_xfade_duration_s\n",
    "            target_sum_adjusted_image_durations = target_main_content_narration_duration_s + time_lost_to_fades\n",
    "            if original_sum_image_durations > 0:\n",
    "                for item in main_content_sequence_original_durations:\n",
    "                    adjusted_duration = (item['duration_s'] / original_sum_image_durations) * target_sum_adjusted_image_durations\n",
    "                    main_content_sequence_adjusted_durations.append({**item, 'adjusted_duration_s': adjusted_duration})\n",
    "                    # print(f\"    Img {item.get('original_chunk_index', 'N/A')}: Orig Dur: {item['duration_s']:.3f}s -> Adj Dur: {adjusted_duration:.3f}s\") # Verbose\n",
    "            elif num_main_content_images > 0 and target_sum_adjusted_image_durations > 0: # Sum original is 0, distribute evenly\n",
    "                even_duration = target_sum_adjusted_image_durations / num_main_content_images\n",
    "                for item in main_content_sequence_original_durations: main_content_sequence_adjusted_durations.append({**item, 'adjusted_duration_s': even_duration})\n",
    "            else: main_content_sequence_adjusted_durations = [{**item, 'adjusted_duration_s': item['duration_s']} for item in main_content_sequence_original_durations]\n",
    "            main_content_visual_duration_s_effective = target_main_content_narration_duration_s\n",
    "        else: main_content_visual_duration_s_effective = 0.0\n",
    "        \n",
    "        final_total_video_duration_s = intro_d + main_content_visual_duration_s_effective + outro_d\n",
    "        if final_total_video_duration_s <= 0: final_total_video_duration_s = 0.1\n",
    "        print(f\"  Adjusted Main Content Visual Duration (Target for Faded Sequence): {main_content_visual_duration_s_effective:.3f}s\")\n",
    "        print(f\"  Final Total Video Duration (for audio & FFmpeg -t): {final_total_video_duration_s:.3f}s\")\n",
    "        print(f\"Time for Step 1 (Durations & Adjustments): {time.time() - step_start_time:.2f}s\")\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        # --- Audio Preparation (Steps 2-5) ---\n",
    "        # [This block is identical to the one in Test 2.9]\n",
    "        audio_fps_main = narration_audioclip_from_array.fps if hasattr(narration_audioclip_from_array, 'fps') and narration_audioclip_from_array.fps and narration_audioclip_from_array.fps > 0 else 44100\n",
    "        print(f\"\\n--- Step 2-5 (Phase D): Prepare Full Audio Track ---\"); print(f\"  Using audio FPS: {audio_fps_main}\")\n",
    "        if narration_audioclip_from_array.duration > 0 and target_main_content_narration_duration_s > 0: narration_final_for_mix = narration_audioclip_from_array.subclip(0, min(narration_audioclip_from_array.duration, target_main_content_narration_duration_s))\n",
    "        elif target_main_content_narration_duration_s > 0: narration_final_for_mix = AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=target_main_content_narration_duration_s, fps=audio_fps_main)\n",
    "        else: narration_final_for_mix = AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=0, fps=audio_fps_main)\n",
    "        if ADD_BACKGROUND_MUSIC and bg_music_audioclip_from_array and target_main_content_narration_duration_s > 0:\n",
    "            bgm_source = bg_music_audioclip_from_array; bgm_target_dur = target_main_content_narration_duration_s\n",
    "            if not (hasattr(bgm_source, 'fps') and bgm_source.fps and bgm_source.fps > 0): bgm_source.fps = audio_fps_main\n",
    "            if bgm_source.duration > bgm_target_dur: bg_music_final_for_mix = bgm_source.subclip(0, bgm_target_dur)\n",
    "            elif bgm_source.duration < bgm_target_dur and bgm_source.duration > 0: bg_music_final_for_mix = vfx_all.loop(bgm_source, duration=bgm_target_dur)\n",
    "            elif abs(bgm_source.duration - bgm_target_dur) < 0.01 : bg_music_final_for_mix = bgm_source\n",
    "            else: bg_music_final_for_mix = None\n",
    "        else: bg_music_final_for_mix = None\n",
    "        main_audio_components = [narration_final_for_mix];\n",
    "        if bg_music_final_for_mix: main_audio_components.append(bg_music_final_for_mix)\n",
    "        if len(main_audio_components) > 1: final_audio_mix_main_content = CompositeAudioClip(main_audio_components)\n",
    "        elif main_audio_components: final_audio_mix_main_content = main_audio_components[0]\n",
    "        else: final_audio_mix_main_content = AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=target_main_content_narration_duration_s, fps=audio_fps_main)\n",
    "        if abs(final_audio_mix_main_content.duration - target_main_content_narration_duration_s) > 0.1: final_audio_mix_main_content = final_audio_mix_main_content.set_duration(target_main_content_narration_duration_s)\n",
    "        full_audio_track_list = []\n",
    "        full_audio_fps = getattr(final_audio_mix_main_content, 'fps', None) \n",
    "        if not full_audio_fps or full_audio_fps <= 0: full_audio_fps = audio_fps_main \n",
    "        if intro_d > 0: intro_audio_dummy = AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=intro_d, fps=full_audio_fps); full_audio_track_list.append(intro_audio_dummy)\n",
    "        if final_audio_mix_main_content and final_audio_mix_main_content.duration > 0: full_audio_track_list.append(final_audio_mix_main_content)\n",
    "        elif target_main_content_narration_duration_s > 0 : full_audio_track_list.append(AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=target_main_content_narration_duration_s, fps=full_audio_fps))\n",
    "        if outro_d > 0: outro_audio_dummy = AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=outro_d, fps=full_audio_fps); full_audio_track_list.append(outro_audio_dummy)\n",
    "        if full_audio_track_list: audio_for_ffmpeg = concatenate_audioclips(full_audio_track_list)\n",
    "        else: audio_for_ffmpeg = AudioClip(lambda t: np.zeros((1, FFMPEG_AUDIO_CHANNELS_FINAL) if isinstance(t, (int, float)) else (len(t), FFMPEG_AUDIO_CHANNELS_FINAL)), duration=final_total_video_duration_s, fps=full_audio_fps)\n",
    "        if abs(audio_for_ffmpeg.duration - final_total_video_duration_s) > 0.1: audio_for_ffmpeg = audio_for_ffmpeg.set_duration(final_total_video_duration_s)\n",
    "        print(f\"  Full audio track prepared. Duration: {audio_for_ffmpeg.duration:.3f}s\")\n",
    "        print(f\"Time for Steps 2-5 (Audio Prep): {time.time() - step_start_time:.2f}s\")\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        print(\"\\n--- Step 6 (Phase D): Write Temporary Audio File ---\")\n",
    "        temp_audio_clip_fps = getattr(audio_for_ffmpeg, 'fps', None)\n",
    "        if temp_audio_clip_fps and temp_audio_clip_fps > 0: write_aac_fps = temp_audio_clip_fps\n",
    "        else: write_aac_fps = full_audio_fps        \n",
    "        audio_for_ffmpeg.write_audiofile(temp_final_mixed_audio_path, fps=write_aac_fps, codec='aac', ffmpeg_params=['-ac', str(FFMPEG_AUDIO_CHANNELS_FINAL)], logger=None )\n",
    "        print(\"  Temporary final mixed audio file written.\")\n",
    "        print(f\"Time for Step 6 (Write Temp Audio): {time.time() - step_start_time:.2f}s\")\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        print(\"\\n--- INFO: Step 7 (MoviePy visual rendering) is SKIPPED ---\")\n",
    "\n",
    "        print(\"\\n--- Step 8 (Phase D): Construct & Execute FFmpeg Command (Test 3.0 - Zoom + XFade + Adjusted Durations) ---\")\n",
    "        if not os.path.exists(ASS_SUBTITLE_PATH): raise FileNotFoundError(f\"ASS Subtitle file not found: {ASS_SUBTITLE_PATH}\")\n",
    "\n",
    "        ffmpeg_inputs = []\n",
    "        filter_complex_parts = []\n",
    "        input_map_idx = 0 \n",
    "        \n",
    "        intro_final_stream_label = None\n",
    "        if intro_clip_path and os.path.exists(intro_clip_path) and intro_d > 0:\n",
    "            ffmpeg_inputs.extend(['-i', intro_clip_path])\n",
    "            intro_input_label = f\"[{input_map_idx}:v]\"\n",
    "            intro_final_stream_label = f\"[v_intro_p{input_map_idx}]\"\n",
    "            filter_complex_parts.append(f\"{intro_input_label}fps={VIDEO_FPS},scale={VIDEO_TARGET_WIDTH}:{VIDEO_TARGET_HEIGHT}:force_original_aspect_ratio=decrease,pad={VIDEO_TARGET_WIDTH}:{VIDEO_TARGET_HEIGHT}:(ow-iw)/2:(oh-ih)/2:color=black,setpts=PTS-STARTPTS{intro_final_stream_label}\")\n",
    "            input_map_idx += 1\n",
    "\n",
    "        # This list will hold the final stream labels for each main content segment (after zoom, trim, setpts)\n",
    "        main_content_segments_for_xfade = [] \n",
    "        for idx, img_data in enumerate(main_content_sequence_adjusted_durations):\n",
    "            if not os.path.exists(img_data['path']): print(f\"WARNING: Image file missing, skipping: {img_data['path']}\"); continue\n",
    "            \n",
    "            ffmpeg_inputs.extend(['-loop', '1', '-framerate', str(VIDEO_FPS), \n",
    "                                  '-t', str(img_data['adjusted_duration_s']), # Input uses adjusted duration\n",
    "                                  '-i', img_data['path']])\n",
    "            current_img_input_label = f\"[{input_map_idx}:v]\"\n",
    "            \n",
    "            base_img_processed_label = f\"[base_img_p{idx}]\"\n",
    "            filter_complex_parts.append(\n",
    "                f\"{current_img_input_label}fps={VIDEO_FPS},\"\n",
    "                f\"scale={VIDEO_TARGET_WIDTH}:{VIDEO_TARGET_HEIGHT}:force_original_aspect_ratio=decrease:eval=frame,\"\n",
    "                f\"pad={VIDEO_TARGET_WIDTH}:{VIDEO_TARGET_HEIGHT}:(ow-iw)/2:(oh-ih)/2:color=black,\"\n",
    "                f\"setpts=PTS-STARTPTS{base_img_processed_label}\"\n",
    "            )\n",
    "            \n",
    "            segment_output_label_for_xfade = base_img_processed_label # Default if no zoom\n",
    "\n",
    "            if CALM_ZOOM_EFFECT_ENABLED:\n",
    "                # num_frames for zoompan 'd' is based on the adjusted_duration_s of this image segment\n",
    "                num_frames_this_clip = int(img_data['adjusted_duration_s'] * FFMPEG_ZOOMPAN_TARGET_FPS)\n",
    "                \n",
    "                if num_frames_this_clip <= 0:\n",
    "                    print(f\"    WARNING: Skipping zoom for image {idx} due to zero/negative frame count ({num_frames_this_clip}).\")\n",
    "                else:\n",
    "                    print(f\"  Applying ZoomPan to segment {idx} (image {img_data.get('original_chunk_index', 'N/A')}) for {num_frames_this_clip} frames using adjusted_duration {img_data['adjusted_duration_s']:.3f}s.\")\n",
    "                    zoom_expr = \"'1.0'\" \n",
    "                    if CALM_ZOOM_TYPE == \"continuous_in_center\":\n",
    "                        zoom_expr = f\"'min(max(1,zoom+(({FFMPEG_ZOOMPAN_MAX_SCALE}-1)/({num_frames_this_clip}))), {FFMPEG_ZOOMPAN_MAX_SCALE})'\"\n",
    "                    elif CALM_ZOOM_TYPE == \"multi_cycle_in_out\" and FFMPEG_ZOOMPAN_CYCLES_PER_CLIP > 0:\n",
    "                        zoom_expr = f\"'1+({FFMPEG_ZOOMPAN_MAX_SCALE}-1)*(0.5-0.5*cos({FFMPEG_ZOOMPAN_CYCLES_PER_CLIP}*2*PI*on/{num_frames_this_clip}))'\"\n",
    "                    \n",
    "                    temp_zoomed_label = f\"[temp_z_img{idx}]\"\n",
    "                    zoom_filter_str = (\n",
    "                        f\"{base_img_processed_label}zoompan=z={zoom_expr}:\"\n",
    "                        f\"x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':\"\n",
    "                        f\"d={num_frames_this_clip}:s={VIDEO_TARGET_WIDTH}x{VIDEO_TARGET_HEIGHT}:fps={FFMPEG_ZOOMPAN_TARGET_FPS}\"\n",
    "                        f\"{temp_zoomed_label}\"\n",
    "                    )\n",
    "                    filter_complex_parts.append(zoom_filter_str)\n",
    "\n",
    "                    final_zoomed_and_trimmed_label = f\"[img_p{idx}_zt]\" # zt for zoomed_trimmed\n",
    "                    trim_filter_str = (\n",
    "                        f\"{temp_zoomed_label}trim=duration={img_data['adjusted_duration_s']},\" # Trim to the adjusted duration\n",
    "                        f\"setpts=PTS-STARTPTS{final_zoomed_and_trimmed_label}\"\n",
    "                    )\n",
    "                    filter_complex_parts.append(trim_filter_str)\n",
    "                    segment_output_label_for_xfade = final_zoomed_and_trimmed_label\n",
    "            \n",
    "            main_content_segments_for_xfade.append(segment_output_label_for_xfade)\n",
    "            input_map_idx += 1\n",
    "        \n",
    "        final_main_content_stream_label = None \n",
    "        if main_content_segments_for_xfade:\n",
    "            if len(main_content_segments_for_xfade) > 1 and current_xfade_duration_s > 0:\n",
    "                print(f\"  Applying XFade between {len(main_content_segments_for_xfade)} main content segments (which may include zoom)...\")\n",
    "                current_stream_in_chain = main_content_segments_for_xfade[0]\n",
    "                duration_of_current_chain_for_offset = main_content_sequence_adjusted_durations[0]['adjusted_duration_s']\n",
    "                \n",
    "                for i in range(len(main_content_segments_for_xfade) - 1):\n",
    "                    stream_to_fade_with = main_content_segments_for_xfade[i+1]\n",
    "                    duration_of_stream_to_fade_with = main_content_sequence_adjusted_durations[i+1]['adjusted_duration_s']\n",
    "                    xfade_offset_val = duration_of_current_chain_for_offset - current_xfade_duration_s\n",
    "                    # print(f\"    XFADE {i}: Input1='{current_stream_in_chain}' (eff_dur={duration_of_current_chain_for_offset:.3f}s), Input2='{stream_to_fade_with}' (adj_img_dur={duration_of_stream_to_fade_with:.3f}s), fade_dur={current_xfade_duration_s:.3f}s, offset={xfade_offset_val:.3f}s\") # Verbose\n",
    "                    if xfade_offset_val < 0: xfade_offset_val = 0.0; # print(f\"      WARNING: xfade offset {i} clamped to 0.0.\")\n",
    "                    faded_output_label = f\"[v_main_faded{i}]\"\n",
    "                    filter_complex_parts.append(f\"{current_stream_in_chain}{stream_to_fade_with}xfade=transition={FFMPEG_XFADE_TRANSITION_TYPE}:duration={current_xfade_duration_s}:offset={xfade_offset_val}{faded_output_label}\")\n",
    "                    current_stream_in_chain = faded_output_label\n",
    "                    duration_of_current_chain_for_offset += (duration_of_stream_to_fade_with - current_xfade_duration_s)\n",
    "                    # print(f\"      Output of xfade {i} is '{faded_output_label}'. New effective chain duration = {duration_of_current_chain_for_offset:.3f}s\")\n",
    "                final_main_content_stream_label = current_stream_in_chain\n",
    "                # print(f\"  XFADE End: Final main content stream '{final_main_content_stream_label}' calculated effective duration: {duration_of_current_chain_for_offset:.3f}s\")\n",
    "                # if abs(duration_of_current_chain_for_offset - main_content_visual_duration_s_effective) > 0.5: \n",
    "                #      print(f\"    DURATION MISMATCH WARNING (after zoom+xfade): XFade chain eff_dur={duration_of_current_chain_for_offset:.3f}s vs Step1 calc_dur={main_content_visual_duration_s_effective:.3f}s\")\n",
    "            elif main_content_segments_for_xfade: \n",
    "                final_main_content_stream_label = main_content_segments_for_xfade[0]\n",
    "        \n",
    "        if not final_main_content_stream_label and main_content_visual_duration_s_effective > 0:\n",
    "             final_main_content_stream_label = \"[v_main_placeholder]\"\n",
    "             filter_complex_parts.append(f\"color=c=black:s={VIDEO_TARGET_WIDTH}x{VIDEO_TARGET_HEIGHT}:d={main_content_visual_duration_s_effective}:r={VIDEO_FPS},setpts=PTS-STARTPTS{final_main_content_stream_label}\")\n",
    "\n",
    "        outro_final_stream_label = None\n",
    "        if outro_clip_path and os.path.exists(outro_clip_path) and outro_d > 0:\n",
    "            ffmpeg_inputs.extend(['-i', outro_clip_path])\n",
    "            outro_input_label = f\"[{input_map_idx}:v]\"\n",
    "            outro_final_stream_label = f\"[v_outro_p{input_map_idx}]\"\n",
    "            filter_complex_parts.append(f\"{outro_input_label}fps={VIDEO_FPS},scale={VIDEO_TARGET_WIDTH}:{VIDEO_TARGET_HEIGHT}:force_original_aspect_ratio=decrease,pad={VIDEO_TARGET_WIDTH}:{VIDEO_TARGET_HEIGHT}:(ow-iw)/2:(oh-ih)/2:color=black,setpts=PTS-STARTPTS{outro_final_stream_label}\")\n",
    "            input_map_idx += 1\n",
    "            \n",
    "        streams_for_final_video_concat = []\n",
    "        if intro_final_stream_label: streams_for_final_video_concat.append(intro_final_stream_label)\n",
    "        if final_main_content_stream_label: streams_for_final_video_concat.append(final_main_content_stream_label)\n",
    "        if outro_final_stream_label: streams_for_final_video_concat.append(outro_final_stream_label)\n",
    "            \n",
    "        visual_stream_before_subs_label = \"[v_vis_concat_final]\"\n",
    "        if streams_for_final_video_concat:\n",
    "            if len(streams_for_final_video_concat) > 1:\n",
    "                filter_complex_parts.append(f\"{''.join(streams_for_final_video_concat)}concat=n={len(streams_for_final_video_concat)}:v=1:a=0{visual_stream_before_subs_label}\")\n",
    "            elif streams_for_final_video_concat:\n",
    "                 filter_complex_parts.append(f\"{streams_for_final_video_concat[0]}copy{visual_stream_before_subs_label}\")\n",
    "        else: \n",
    "            print(\"WARNING: No visual elements for final concat. Creating a black screen.\")\n",
    "            filter_complex_parts.append(f\"color=c=black:s={VIDEO_TARGET_WIDTH}x{VIDEO_TARGET_HEIGHT}:d={final_total_video_duration_s}:r={VIDEO_FPS},setpts=PTS-STARTPTS{visual_stream_before_subs_label}\")\n",
    "\n",
    "        escaped_ass_path_for_ffmpeg = ASS_SUBTITLE_PATH.replace(\"'\", \"'\\\\\\\\\\\\\\\\''\")\n",
    "        filter_complex_parts.append(f\"{visual_stream_before_subs_label}ass='{escaped_ass_path_for_ffmpeg}'[v_out]\")\n",
    "\n",
    "        ffmpeg_cmd_list = ['ffmpeg', '-y']; ffmpeg_cmd_list.extend(ffmpeg_inputs)\n",
    "        ffmpeg_cmd_list.extend(['-i', temp_final_mixed_audio_path])\n",
    "        final_filter_complex_string = \";\".join(filter_complex_parts)\n",
    "        ffmpeg_cmd_list.extend(['-filter_complex', final_filter_complex_string])\n",
    "        ffmpeg_cmd_list.extend(['-map', '[v_out]', '-map', f\"{input_map_idx}:a\"]) \n",
    "        ffmpeg_cmd_list.extend(['-c:v', FFMPEG_VCODEC, '-preset:v', FFMPEG_NVENC_PRESET])\n",
    "        if globals().get('FFMPEG_NVENC_TUNE') and globals().get('FFMPEG_NVENC_TUNE').lower()!='none' and globals().get('FFMPEG_NVENC_TUNE').strip()!=\"\": ffmpeg_cmd_list.extend(['-tune:v', globals().get('FFMPEG_NVENC_TUNE')])\n",
    "        ffmpeg_cmd_list.extend([\n",
    "            '-rc:v', FFMPEG_NVENC_RC, '-cq:v', FFMPEG_NVENC_CQ,\n",
    "            '-b:v', FFMPEG_NVENC_BITRATE, '-pix_fmt', FFMPEG_PIX_FMT,\n",
    "            '-c:a', FFMPEG_ACODEC_FINAL,\n",
    "            '-threads', str(max(1, os.cpu_count()//1)),\n",
    "            '-r', str(VIDEO_FPS),\n",
    "            '-t', str(final_total_video_duration_s), \n",
    "            ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD\n",
    "        ])\n",
    "        final_ffmpeg_command_str_list = [str(p) for p in ffmpeg_cmd_list if p is not None and str(p).strip() != \"\"]\n",
    "        \n",
    "        print(f\"Executing FFmpeg Command (Test 3.0 - Zoom + XFade + Adjusted Durations) (length: {len(final_ffmpeg_command_str_list)} parts):\")\n",
    "        debug_command_file = os.path.join(KAGGLE_WORKING_DIR, \"debug_ffmpeg_zoom_xfade_adj_dur_cmd.txt\")\n",
    "        with open(debug_command_file, \"w\") as f_cmd: f_cmd.write(\" \".join(final_ffmpeg_command_str_list))\n",
    "        print(f\"  FFmpeg command (Test 3.0) saved to {debug_command_file}\")\n",
    "        process = subprocess.Popen(final_ffmpeg_command_str_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')\n",
    "        stdout, stderr = process.communicate()\n",
    "        if process.returncode == 0: print(\"FFmpeg (Test 3.0) render process completed successfully.\"); final_render_successful = True\n",
    "        else: print(\"ERROR: FFmpeg (Test 3.0) render process failed.\"); print(\"  FFmpeg stdout:\\n\", stdout); print(\"  FFmpeg stderr:\\n\", stderr); raise subprocess.CalledProcessError(process.returncode, \" \".join(final_ffmpeg_command_str_list), output=stdout, stderr=stderr)\n",
    "        print(f\"Time for Step 8 (FFmpeg Render - Test 3.0): {time.time() - step_start_time:.2f}s\")\n",
    "\n",
    "    except Exception as e_phase_d_ffmpeg:\n",
    "        print(f\"ERROR during FFmpeg-centric Phase D (Test 3.0): {e_phase_d_ffmpeg}\")\n",
    "        traceback.print_exc()\n",
    "        final_render_successful = False\n",
    "\n",
    "if final_render_successful and os.path.exists(ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD) and os.path.getsize(ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD) > 0:\n",
    "    print(f\"\\nPhase D Complete: Final video successfully rendered to: {ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD}\\n  File size: {os.path.getsize(ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD) / (1024*1024):.2f} MB\")\n",
    "    try:\n",
    "        ffprobe_cmd_out = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD]\n",
    "        result_out = subprocess.run(ffprobe_cmd_out, capture_output=True, text=True, check=True)\n",
    "        actual_output_duration = float(result_out.stdout.strip())\n",
    "        print(f\"  Actual output video duration (from ffprobe): {actual_output_duration:.3f}s\")\n",
    "        if abs(actual_output_duration - final_total_video_duration_s) > 2.0: \n",
    "            print(f\"  WARNING: Actual output duration {actual_output_duration:.3f}s significantly different from target total duration {final_total_video_duration_s:.3f}s.\")\n",
    "    except Exception as e_ffprobe_final: print(f\"  Could not get actual output duration using ffprobe: {e_ffprobe_final}\")\n",
    "else:\n",
    "    path_to_check = ULTIMATE_FINAL_VIDEO_PATH_FOR_DOWNLOAD or 'Path_Not_Defined'\n",
    "    print(f\"\\nPhase D Failed or Incomplete. Final video may not be at: {path_to_check}\")\n",
    "    if os.path.exists(path_to_check): print(f\"  A file exists at the target path, but error occurred or it might be empty/corrupt.\")\n",
    "\n",
    "phase_d_total_time = time.time() - phase_d_start_time\n",
    "print(f\"\\nTotal time for Phase D execution: {phase_d_total_time:.2f}s\")\n",
    "print(\"#######################################################################################\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7756116,
     "sourceId": 12305059,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
